{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this is part of a larger Customer Analytics workshop)\n",
    "\n",
    "# Customer Segmentation Analytics\n",
    "\n",
    "Customers are not all the same.  If we can figure out what _classes_ of customers we have, we may be able to better meet their needs. \n",
    "\n",
    "## What is Segmentation\n",
    "\n",
    "* segmentation involves dividing the market (your prospects and customers) into homogenous subgroups\n",
    "* using these segments we can drive other business activities:\n",
    "  * different, targeted marketing campaigns\n",
    "  * unique pricing strategies and price points.  (Understanding basic economics/econometrics can help you here)\n",
    "\n",
    "## How do we do Segmentation\n",
    "\n",
    "3 basic methods:\n",
    "\n",
    "* **_a priori_ segmentation**\n",
    "  * we know the segments ahead of time.  Based on experience and history we know how we want to classify our customers.  \n",
    "  * example:  electric utilities divide their market into residential, commercial, and industry segments\n",
    "* **supervised (machine) learning** -- a model-based approach\n",
    "  * we know what _features_ we want to use as part of the segmentation and the _label_ is the segments\n",
    "  * we have to have _labeled_ historical segmentation data already\n",
    "  * When is this useful?  If we know the segments for existing customers but we want to classify new customers.  \n",
    "* **unsupervised learning** -- clustering \n",
    "  * when we don't know the segments _a priori_ but want the machine to teach us interesting things about our data.  \n",
    "  * this is a great way to facilitate _Design Thinking_ about our customers.  \n",
    "  * Examples\n",
    "    * hierarchical clustering \n",
    "      * agglomerative:  all rows start as their own cluster and the hierarchy is built bottom-up.  More compute-efficient.\n",
    "      * divisive:  all rows start in one big initial cluster (the root) and then we pull apart the differences (top-down approach).  More compute-intensive.    \n",
    "    * k-means clustering\n",
    "\n",
    "Let's do some clustering using a real world use case\n",
    "\n",
    "## Case Study\n",
    "\n",
    "You are the data analyst for a furniture wholesaler.  Your company manufactures and sells to locally-owned retailers throughout the US and you divide your marketing regions consistently with the [US Census regions](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf).  \n",
    "* midwest\n",
    "* northeast\n",
    "* south\n",
    "* west\n",
    "\n",
    "Each region has a salesforce and a VP of Sales and they have autonomy to set discounts and terms.  \n",
    "\n",
    "You have 43 products in 6 product lines, further divided into product classes (they aren't all listed here):\n",
    "\n",
    "|Product Line|Product Class|Product|\n",
    "|---|---|---|\n",
    "|Den|Chairs|Wingback chair|\n",
    "|Den|Tables|Side table|\n",
    "|Den|Sofas|Craftsman sofa|\n",
    "|Dining Room|Chairs|dining room armchair|\n",
    "|Dining Room|Tables|dining room table|\n",
    "|Dining Room|Tables|Baking Racks|\n",
    "|Kids Rooms|...|...|\n",
    "|Kitchen|...|...|\n",
    "|Living Room|...|...|\n",
    "|Bedroom|...|...|\n",
    "\n",
    "4 types of discounts are offered at the discretion of the regional sales teams.\n",
    "\n",
    "**The CMO wants customers segmented specifically for living room blinds so that we can develop a targeted marketing campaign.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup Template\n",
    "\n",
    "These are tools and scripts I always use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nrequire([\"codemirror/lib/codemirror\"]);\nfunction set(str) {\n    var obj = {}, words = str.split(\" \");\n    for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n    return obj;\n  }\nvar fugue_keywords = \"fill hash rand even presort persist broadcast params process output outtransform rowcount concurrency prepartition zip print title save append parquet csv json single checkpoint weak strong deterministic yield connect sample seed take sub callback dataframe file\";\nCodeMirror.defineMIME(\"text/x-fsql\", {\n    name: \"sql\",\n    keywords: set(fugue_keywords + \" add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n    builtin: set(\"date datetime tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n    atoms: set(\"false true null\"),\n    operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n    dateSQL: set(\"time\"),\n    support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n  });\n\nCodeMirror.modeInfo.push( {\n            name: \"Fugue SQL\",\n            mime: \"text/x-fsql\",\n            mode: \"sql\"\n          } );\n\nrequire(['notebook/js/codecell'], function(codecell) {\n    codecell.CodeCell.options_default.highlight_modes['magic_text/x-fsql'] = {'reg':[/%%fsql/]} ;\n    Jupyter.notebook.events.on('kernel_ready.Kernel', function(){\n    Jupyter.notebook.get_cells().map(function(cell){\n        if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n    });\n  });\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done running imports.py\n",
      "done running utils.py\n"
     ]
    }
   ],
   "source": [
    "## set various paths\n",
    "#datapath = '../Data/'\n",
    "datapath =     'https://davewdemodata.blob.core.windows.net/lake/CustomerAnalytics/orders.csv?sv=2020-10-02&st=2022-02-04T18%3A40%3A49Z&se=2030-02-05T18%3A40%3A00Z&sr=b&sp=r&sig=ROOMQEX14ZhvpPnPy9T%2BJ8kcllW3FxS8wADT5LwdcOE%3D'\n",
    "datapathCust = 'https://davewdemodata.blob.core.windows.net/lake/CustomerAnalytics/customers.csv?sp=r&st=2021-02-24T21:22:58Z&se=2032-02-25T05:22:58Z&spr=https&sv=2020-08-04&sr=b&sig=t0iAASRxFYRG%2BsORxbQV7b6eEyksPErSx6nBh8xD0sw%3D'\n",
    "# you might have to run this block first to install the packages\n",
    "# if using the devcontainer, this was already done for you\n",
    "#!pip install -r {scriptspath + 'requirements.txt'}\n",
    "\n",
    "# this will update the requirements.txt file later, if needed\n",
    "#!pip freeze > requirements.txt\n",
    "\n",
    "%run -i ./scripts/imports.py\n",
    "%run -i ./scripts/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension install --py --user fugue_notebook\n",
    "!jupyter nbextension enable fugue_notebook --py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analytics\n",
    "\n",
    "Now that we've done all that setup above, let's take a look at the data in our datalake using Synapse SQL Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Data Dictionary\n",
    "\n",
    "This is the data we are going to use.  You can use this as a reference.\n",
    "\n",
    "| Variable                  | Values                                 | Source       | Column Name |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | INT                     | Order Sys    | Onum         |\n",
    "| Customer ID               | INT                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                             | Order Sys    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "| Total Discount            | \\% discount                            | Calculated: Sum of discounts | Tdisc         |\n",
    "| Pocket Price              | \\$US                                   | Calculated: LPrice $\\times$ (1 - TDisc) | Pprice  | \n",
    "| Log of Unit Sales         | Log sales                              | Calculated: log(Usales)  | log_Usales  |\n",
    "| Log of Pocket Price       | \\$US                                   | Calculated: log(Pprice)  | log_Pprice  |\n",
    "| Revenue                   | \\$US                                   | Calculated: Usales $\\times$ Pprice | Rev          |\n",
    "| Contribution              | \\$US                                   | Calculated: Rev - Mcost | Con  |\n",
    "| Contribution Margin       | \\%                                     | Calculated: Con/Rev | CM |\n",
    "| Net Revenue               | \\$US                                   | Calculated: (Usales - returnAmount) $\\times$  Pprice  | netRev  |\n",
    "| Lost Revenue         |  \\$US   | Calculated: Rev - netRev  | lostRev  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's just look at the `Orders` data we were given and try to make some sense out of it.  \n",
    "\n",
    "In the next few cells I show you some tricks I use to _learn_ about a new dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Onum</th>\n",
       "      <th>CID</th>\n",
       "      <th>Tdate</th>\n",
       "      <th>Pline</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Usales</th>\n",
       "      <th>Return</th>\n",
       "      <th>returnAmount</th>\n",
       "      <th>Mcost</th>\n",
       "      <th>Lprice</th>\n",
       "      <th>Ddisc</th>\n",
       "      <th>Cdisc</th>\n",
       "      <th>Odisc</th>\n",
       "      <th>Pdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>36</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>586</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>57</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>587</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>588</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>21</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>589</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>56</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Onum   CID      Tdate        Pline                    Pclass  Usales  \\\n",
       "0   585  1015 2004-01-25  Living Room  Window Treatment: Blinds      36   \n",
       "1   586  1015 2004-01-25  Living Room  Window Treatment: Blinds      57   \n",
       "2   587  1015 2004-01-25  Living Room  Window Treatment: Blinds      27   \n",
       "3   588  1015 2004-01-25  Living Room  Window Treatment: Blinds      21   \n",
       "4   589  1015 2004-01-25  Living Room  Window Treatment: Blinds      56   \n",
       "\n",
       "  Return  returnAmount  Mcost  Lprice  Ddisc  Cdisc  Odisc  Pdisc  \n",
       "0     No             0   0.95     5.4    NaN    NaN  0.043  0.042  \n",
       "1     No             0   0.95     5.4  0.157  0.075  0.041  0.031  \n",
       "2     No             0   0.95     5.4    NaN  0.048  0.053  0.021  \n",
       "3     No             0   0.95     5.4    NaN  0.072    NaN  0.033  \n",
       "4     No             0   0.95     5.4  0.140  0.056  0.041  0.055  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "dfOrders = pd.read_csv( datapath, parse_dates = [ 'Tdate' ] )\n",
    "dfOrders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Onum</th>\n",
       "      <th>CID</th>\n",
       "      <th>Usales</th>\n",
       "      <th>returnAmount</th>\n",
       "      <th>Mcost</th>\n",
       "      <th>Lprice</th>\n",
       "      <th>Ddisc</th>\n",
       "      <th>Cdisc</th>\n",
       "      <th>Odisc</th>\n",
       "      <th>Pdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.027000e+04</td>\n",
       "      <td>70270.000000</td>\n",
       "      <td>70270.00000</td>\n",
       "      <td>70270.000000</td>\n",
       "      <td>70270.000000</td>\n",
       "      <td>70270.000000</td>\n",
       "      <td>70262.000000</td>\n",
       "      <td>70261.000000</td>\n",
       "      <td>70266.000000</td>\n",
       "      <td>70268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.759090e+05</td>\n",
       "      <td>1311.250021</td>\n",
       "      <td>30.99731</td>\n",
       "      <td>3.151188</td>\n",
       "      <td>1.246712</td>\n",
       "      <td>7.124275</td>\n",
       "      <td>0.122091</td>\n",
       "      <td>0.069948</td>\n",
       "      <td>0.050036</td>\n",
       "      <td>0.039978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.380221e+05</td>\n",
       "      <td>761.513895</td>\n",
       "      <td>18.95348</td>\n",
       "      <td>8.908806</td>\n",
       "      <td>0.182570</td>\n",
       "      <td>1.043404</td>\n",
       "      <td>0.040884</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.011547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.850000e+02</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.884252e+05</td>\n",
       "      <td>636.000000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>6.180000</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.608555e+05</td>\n",
       "      <td>1279.500000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.793148e+05</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.173642e+06</td>\n",
       "      <td>2626.000000</td>\n",
       "      <td>338.00000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>8.760000</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Onum           CID       Usales  returnAmount         Mcost  \\\n",
       "count  7.027000e+04  70270.000000  70270.00000  70270.000000  70270.000000   \n",
       "mean   5.759090e+05   1311.250021     30.99731      3.151188      1.246712   \n",
       "std    3.380221e+05    761.513895     18.95348      8.908806      0.182570   \n",
       "min    5.850000e+02     14.000000      2.00000      0.000000      0.950000   \n",
       "25%    2.884252e+05    636.000000     18.00000      0.000000      1.080000   \n",
       "50%    5.608555e+05   1279.500000     26.00000      0.000000      1.270000   \n",
       "75%    8.793148e+05   2011.000000     39.00000      0.000000      1.330000   \n",
       "max    1.173642e+06   2626.000000    338.00000    158.000000      1.530000   \n",
       "\n",
       "             Lprice         Ddisc         Cdisc         Odisc         Pdisc  \n",
       "count  70270.000000  70262.000000  70261.000000  70266.000000  70268.000000  \n",
       "mean       7.124275      0.122091      0.069948      0.050036      0.039978  \n",
       "std        1.043404      0.040884      0.020182      0.014451      0.011547  \n",
       "min        5.400000      0.045000      0.035000      0.025000      0.020000  \n",
       "25%        6.180000      0.086000      0.053000      0.038000      0.030000  \n",
       "50%        7.230000      0.116000      0.070000      0.050000      0.040000  \n",
       "75%        7.600000      0.157000      0.087000      0.063000      0.050000  \n",
       "max        8.760000      0.215000      0.105000      0.075000      0.060000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's always best to do some basic data profiling.  This is really simple in python\n",
    "# this will only show the numeric columns\n",
    "\n",
    "dfOrders.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fec10_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fec10_\">\n",
       "  <caption>DataFrame Dimensions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fec10_level0_row0\" class=\"row_heading level0 row0\" >Number of Rows</th>\n",
       "      <td id=\"T_fec10_row0_col0\" class=\"data row0 col0\" >70,270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fec10_level0_row1\" class=\"row_heading level0 row1\" >Number of Columns</th>\n",
       "      <td id=\"T_fec10_row1_col0\" class=\"data row1 col0\" >14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a095090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a2cb8_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "#T_a2cb8_row0_col0, #T_a2cb8_row0_col1, #T_a2cb8_row1_col0, #T_a2cb8_row1_col1, #T_a2cb8_row2_col0, #T_a2cb8_row2_col1, #T_a2cb8_row3_col0, #T_a2cb8_row3_col1, #T_a2cb8_row4_col0, #T_a2cb8_row4_col1, #T_a2cb8_row5_col0, #T_a2cb8_row5_col1, #T_a2cb8_row6_col0, #T_a2cb8_row6_col1, #T_a2cb8_row7_col0, #T_a2cb8_row7_col1, #T_a2cb8_row8_col0, #T_a2cb8_row8_col1, #T_a2cb8_row9_col0, #T_a2cb8_row9_col1, #T_a2cb8_row10_col0, #T_a2cb8_row10_col1, #T_a2cb8_row11_col0, #T_a2cb8_row11_col1, #T_a2cb8_row12_col0, #T_a2cb8_row12_col1, #T_a2cb8_row13_col0, #T_a2cb8_row13_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a2cb8_\">\n",
       "  <caption>DataFrame Column Check for DataFrame dfOrders</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Leading White Spaces</th>\n",
       "      <th class=\"col_heading level0 col1\" >Trailing White Spaces</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Column Name</th>\n",
       "      <th class=\"index_name level1\" >#Characters</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row0\" class=\"row_heading level0 row0\" >Onum</th>\n",
       "      <th id=\"T_a2cb8_level1_row0\" class=\"row_heading level1 row0\" >4</th>\n",
       "      <td id=\"T_a2cb8_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row1\" class=\"row_heading level0 row1\" >CID</th>\n",
       "      <th id=\"T_a2cb8_level1_row1\" class=\"row_heading level1 row1\" >3</th>\n",
       "      <td id=\"T_a2cb8_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row2\" class=\"row_heading level0 row2\" >Tdate</th>\n",
       "      <th id=\"T_a2cb8_level1_row2\" class=\"row_heading level1 row2\" >5</th>\n",
       "      <td id=\"T_a2cb8_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row3\" class=\"row_heading level0 row3\" >Pline</th>\n",
       "      <th id=\"T_a2cb8_level1_row3\" class=\"row_heading level1 row3\" >5</th>\n",
       "      <td id=\"T_a2cb8_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row4\" class=\"row_heading level0 row4\" >Pclass</th>\n",
       "      <th id=\"T_a2cb8_level1_row4\" class=\"row_heading level1 row4\" >6</th>\n",
       "      <td id=\"T_a2cb8_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row5\" class=\"row_heading level0 row5\" >Usales</th>\n",
       "      <th id=\"T_a2cb8_level1_row5\" class=\"row_heading level1 row5\" >6</th>\n",
       "      <td id=\"T_a2cb8_row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row6\" class=\"row_heading level0 row6\" >Return</th>\n",
       "      <th id=\"T_a2cb8_level1_row6\" class=\"row_heading level1 row6\" >6</th>\n",
       "      <td id=\"T_a2cb8_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row7\" class=\"row_heading level0 row7\" >returnAmount</th>\n",
       "      <th id=\"T_a2cb8_level1_row7\" class=\"row_heading level1 row7\" >12</th>\n",
       "      <td id=\"T_a2cb8_row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row8\" class=\"row_heading level0 row8\" >Mcost</th>\n",
       "      <th id=\"T_a2cb8_level1_row8\" class=\"row_heading level1 row8\" >5</th>\n",
       "      <td id=\"T_a2cb8_row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row9\" class=\"row_heading level0 row9\" >Lprice</th>\n",
       "      <th id=\"T_a2cb8_level1_row9\" class=\"row_heading level1 row9\" >6</th>\n",
       "      <td id=\"T_a2cb8_row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row10\" class=\"row_heading level0 row10\" >Ddisc</th>\n",
       "      <th id=\"T_a2cb8_level1_row10\" class=\"row_heading level1 row10\" >5</th>\n",
       "      <td id=\"T_a2cb8_row10_col0\" class=\"data row10 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row11\" class=\"row_heading level0 row11\" >Cdisc</th>\n",
       "      <th id=\"T_a2cb8_level1_row11\" class=\"row_heading level1 row11\" >5</th>\n",
       "      <td id=\"T_a2cb8_row11_col0\" class=\"data row11 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row12\" class=\"row_heading level0 row12\" >Odisc</th>\n",
       "      <th id=\"T_a2cb8_level1_row12\" class=\"row_heading level1 row12\" >5</th>\n",
       "      <td id=\"T_a2cb8_row12_col0\" class=\"data row12 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2cb8_level0_row13\" class=\"row_heading level0 row13\" >Pdisc</th>\n",
       "      <th id=\"T_a2cb8_level1_row13\" class=\"row_heading level1 row13\" >5</th>\n",
       "      <td id=\"T_a2cb8_row13_col0\" class=\"data row13 col0\" >0</td>\n",
       "      <td id=\"T_a2cb8_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a05eb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: n = 14 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_73efd_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_73efd_\">\n",
       "  <caption>Missing Value Report</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Total</th>\n",
       "      <th class=\"col_heading level0 col1\" >Missing</th>\n",
       "      <th class=\"col_heading level0 col2\" >Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row0\" class=\"row_heading level0 row0\" >Cdisc</th>\n",
       "      <td id=\"T_73efd_row0_col0\" class=\"data row0 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row0_col1\" class=\"data row0 col1\" >9</td>\n",
       "      <td id=\"T_73efd_row0_col2\" class=\"data row0 col2\" >0.0128%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row1\" class=\"row_heading level0 row1\" >Ddisc</th>\n",
       "      <td id=\"T_73efd_row1_col0\" class=\"data row1 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row1_col1\" class=\"data row1 col1\" >8</td>\n",
       "      <td id=\"T_73efd_row1_col2\" class=\"data row1 col2\" >0.0114%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row2\" class=\"row_heading level0 row2\" >Odisc</th>\n",
       "      <td id=\"T_73efd_row2_col0\" class=\"data row2 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row2_col1\" class=\"data row2 col1\" >4</td>\n",
       "      <td id=\"T_73efd_row2_col2\" class=\"data row2 col2\" >0.00569%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row3\" class=\"row_heading level0 row3\" >Pdisc</th>\n",
       "      <td id=\"T_73efd_row3_col0\" class=\"data row3 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row3_col1\" class=\"data row3 col1\" >2</td>\n",
       "      <td id=\"T_73efd_row3_col2\" class=\"data row3 col2\" >0.00285%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row4\" class=\"row_heading level0 row4\" >Onum</th>\n",
       "      <td id=\"T_73efd_row4_col0\" class=\"data row4 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row4_col2\" class=\"data row4 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row5\" class=\"row_heading level0 row5\" >CID</th>\n",
       "      <td id=\"T_73efd_row5_col0\" class=\"data row5 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row5_col2\" class=\"data row5 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row6\" class=\"row_heading level0 row6\" >Tdate</th>\n",
       "      <td id=\"T_73efd_row6_col0\" class=\"data row6 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row6_col2\" class=\"data row6 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row7\" class=\"row_heading level0 row7\" >Pline</th>\n",
       "      <td id=\"T_73efd_row7_col0\" class=\"data row7 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row7_col2\" class=\"data row7 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row8\" class=\"row_heading level0 row8\" >Pclass</th>\n",
       "      <td id=\"T_73efd_row8_col0\" class=\"data row8 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row8_col2\" class=\"data row8 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row9\" class=\"row_heading level0 row9\" >Usales</th>\n",
       "      <td id=\"T_73efd_row9_col0\" class=\"data row9 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row9_col2\" class=\"data row9 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row10\" class=\"row_heading level0 row10\" >Return</th>\n",
       "      <td id=\"T_73efd_row10_col0\" class=\"data row10 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row10_col2\" class=\"data row10 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row11\" class=\"row_heading level0 row11\" >returnAmount</th>\n",
       "      <td id=\"T_73efd_row11_col0\" class=\"data row11 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row11_col2\" class=\"data row11 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row12\" class=\"row_heading level0 row12\" >Mcost</th>\n",
       "      <td id=\"T_73efd_row12_col0\" class=\"data row12 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row12_col2\" class=\"data row12 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73efd_level0_row13\" class=\"row_heading level0 row13\" >Lprice</th>\n",
       "      <td id=\"T_73efd_row13_col0\" class=\"data row13 col0\" >70,270</td>\n",
       "      <td id=\"T_73efd_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "      <td id=\"T_73efd_row13_col2\" class=\"data row13 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7389e6a910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: n = 70270\n"
     ]
    }
   ],
   "source": [
    "df_size( dfOrders )\n",
    "column_check( dfOrders )\n",
    "mvReport ( dfOrders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "This is telling us:\n",
    "\n",
    "* there are 70270 rows in the dataset (by looking at count of `Onum`)\n",
    "* there are a bunch of cols with `null` (by looking at count for the other cols in `describe` or via `mvReport`)\n",
    "\n",
    "> NULLs are the bane of the data analyst, we may need to fix these later.  \n",
    "\n",
    "We can also look at individual column stats using this notation:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will tell us unique values\n",
    "dfOrders.Return.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "We generally have to clean our raw data, we do that in this block.  \n",
    "\n",
    "Note:  We merely build a \"pipeline\" of dataframes, almost like a temp table, but much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## handle the NULLs (NaNs)\n",
    "dfOrders10 = dfOrders.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python and `pandas dataframes` can be a bit daunting for business analysts.  \n",
    "\n",
    "Luckily we can put a SQL abstraction over a pandas data frame and use SQL and python interchangeably.   \n",
    "\n",
    "### Calculations\n",
    "\n",
    "We have the raw data, let's build some useful business calculations.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ddisc</th>\n",
       "      <th>Odisc</th>\n",
       "      <th>Pdisc</th>\n",
       "      <th>Cdisc</th>\n",
       "      <th>Tdisc</th>\n",
       "      <th>Pprice</th>\n",
       "      <th>Rev</th>\n",
       "      <th>NetRev</th>\n",
       "      <th>LostRev</th>\n",
       "      <th>Profit</th>\n",
       "      <th>ProfitMargin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>4.9410</td>\n",
       "      <td>177.8760</td>\n",
       "      <td>177.8760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.9260</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.157</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.304</td>\n",
       "      <td>3.7584</td>\n",
       "      <td>214.2288</td>\n",
       "      <td>214.2288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213.2788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.122</td>\n",
       "      <td>4.7412</td>\n",
       "      <td>128.0124</td>\n",
       "      <td>128.0124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0624</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.105</td>\n",
       "      <td>4.8330</td>\n",
       "      <td>101.4930</td>\n",
       "      <td>101.4930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.5430</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.292</td>\n",
       "      <td>3.8232</td>\n",
       "      <td>214.0992</td>\n",
       "      <td>214.0992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213.1492</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ddisc  Odisc  Pdisc  Cdisc  Tdisc  Pprice       Rev    NetRev  LostRev  \\\n",
       "0  0.000  0.043  0.042  0.000  0.085  4.9410  177.8760  177.8760      0.0   \n",
       "1  0.157  0.041  0.031  0.075  0.304  3.7584  214.2288  214.2288      0.0   \n",
       "2  0.000  0.053  0.021  0.048  0.122  4.7412  128.0124  128.0124      0.0   \n",
       "3  0.000  0.000  0.033  0.072  0.105  4.8330  101.4930  101.4930      0.0   \n",
       "4  0.140  0.041  0.055  0.056  0.292  3.8232  214.0992  214.0992      0.0   \n",
       "\n",
       "     Profit  ProfitMargin  \n",
       "0  176.9260           1.0  \n",
       "1  213.2788           1.0  \n",
       "2  127.0624           1.0  \n",
       "3  100.5430           1.0  \n",
       "4  213.1492           1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: Ddisc:double,Odisc:double,Pdisc:double,Cdisc:double,Tdisc:double,Pprice:double,Rev:double,NetRev:double,LostRev:double,Profit:double,ProfitMargin:double</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql \n",
    "\n",
    "SELECT \n",
    "    Ddisc, Odisc, Pdisc, Cdisc, \n",
    "    --calculations\n",
    "    --\"total discount\"\n",
    "    (Ddisc + Odisc + Pdisc + Cdisc) AS Tdisc,\n",
    "    --\"Pocket Price\":  what the wholesaler receives as revenue on the item (list - discounts)\n",
    "    Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)) AS Pprice,\n",
    "    --Gross Revenue (unit sales * pocket price)\n",
    "    Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc))) AS Rev,\n",
    "    --Net Revenue (subtract returns from Gross Rev)\n",
    "    (Usales - returnAmount) * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)) ) AS NetRev,\n",
    "    --Lost Revenue (...due to returns)\n",
    "    (Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)))) - ((Usales - returnAmount) * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)) )) AS LostRev,\n",
    "    --Profit (Revenue - Cost)\n",
    "    (Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc))) ) - Mcost AS Profit,\n",
    "    --Profit Margin (Profit/Rev)\n",
    "    ((Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)))))/(Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)))) AS ProfitMargin\n",
    "FROM  dfOrders10\n",
    "LIMIT 5\n",
    "PRINT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "In the code above we are merely building the calculations.  Let's change the code a bit to `YIELD` a dataframe we can use in subsequent cells.  This is basically a temp table.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql \n",
    "\n",
    "SELECT \n",
    "    *,\n",
    "    --calculations\n",
    "    --\"total discount\"\n",
    "    (Ddisc + Odisc + Pdisc + Cdisc) AS Tdisc,\n",
    "    --\"Pocket Price\":  what the wholesaler receives as revenue on the item (list - discounts)\n",
    "    Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)) AS Pprice,\n",
    "    --Gross Revenue (unit sales * pocket price)\n",
    "    Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc))) AS Rev,\n",
    "    --Net Revenue (subtract returns from Gross Rev)\n",
    "    (Usales - returnAmount) * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)) ) AS NetRev,\n",
    "    --Lost Revenue (...due to returns)\n",
    "    (Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)))) - ((Usales - returnAmount) * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)) )) AS LostRev,\n",
    "    --Profit (Revenue - Cost)\n",
    "    (Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc))) ) - Mcost AS Profit,\n",
    "    --Profit Margin (Profit/Rev)\n",
    "    ((Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)))))/(Usales * (Lprice * (1-(Ddisc + Odisc + Pdisc + Cdisc)))) AS ProfitMargin\n",
    "FROM  dfOrders10\n",
    "--remove these 2 lines and replace with YIELD\n",
    "--LIMIT 5\n",
    "--PRINT\n",
    "YIELD DATAFRAME AS dfOrders20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checked that we actually built a \"temp table\"\n",
    "\n",
    "> This is basically the CETAS/CTAS syntax and it is a great way to keep SQL queries manageable and easy to understand.  There is no perf penalty to do this since everything is in-memory.  This also allows us to do diffs between dataframes/tables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Onum</th>\n",
       "      <th>CID</th>\n",
       "      <th>Tdate</th>\n",
       "      <th>Pline</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Usales</th>\n",
       "      <th>Return</th>\n",
       "      <th>returnAmount</th>\n",
       "      <th>Mcost</th>\n",
       "      <th>Lprice</th>\n",
       "      <th>...</th>\n",
       "      <th>Cdisc</th>\n",
       "      <th>Odisc</th>\n",
       "      <th>Pdisc</th>\n",
       "      <th>Tdisc</th>\n",
       "      <th>Pprice</th>\n",
       "      <th>Rev</th>\n",
       "      <th>NetRev</th>\n",
       "      <th>LostRev</th>\n",
       "      <th>Profit</th>\n",
       "      <th>ProfitMargin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>36</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.085</td>\n",
       "      <td>4.9410</td>\n",
       "      <td>177.8760</td>\n",
       "      <td>177.8760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.9260</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>586</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>57</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.304</td>\n",
       "      <td>3.7584</td>\n",
       "      <td>214.2288</td>\n",
       "      <td>214.2288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213.2788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>587</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.122</td>\n",
       "      <td>4.7412</td>\n",
       "      <td>128.0124</td>\n",
       "      <td>128.0124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0624</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>588</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>21</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.105</td>\n",
       "      <td>4.8330</td>\n",
       "      <td>101.4930</td>\n",
       "      <td>101.4930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.5430</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>589</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>Window Treatment: Blinds</td>\n",
       "      <td>56</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.292</td>\n",
       "      <td>3.8232</td>\n",
       "      <td>214.0992</td>\n",
       "      <td>214.0992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213.1492</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Onum   CID      Tdate        Pline                    Pclass  Usales  \\\n",
       "0   585  1015 2004-01-25  Living Room  Window Treatment: Blinds      36   \n",
       "1   586  1015 2004-01-25  Living Room  Window Treatment: Blinds      57   \n",
       "2   587  1015 2004-01-25  Living Room  Window Treatment: Blinds      27   \n",
       "3   588  1015 2004-01-25  Living Room  Window Treatment: Blinds      21   \n",
       "4   589  1015 2004-01-25  Living Room  Window Treatment: Blinds      56   \n",
       "\n",
       "  Return  returnAmount  Mcost  Lprice  ...  Cdisc  Odisc  Pdisc  Tdisc  \\\n",
       "0     No             0   0.95     5.4  ...  0.000  0.043  0.042  0.085   \n",
       "1     No             0   0.95     5.4  ...  0.075  0.041  0.031  0.304   \n",
       "2     No             0   0.95     5.4  ...  0.048  0.053  0.021  0.122   \n",
       "3     No             0   0.95     5.4  ...  0.072  0.000  0.033  0.105   \n",
       "4     No             0   0.95     5.4  ...  0.056  0.041  0.055  0.292   \n",
       "\n",
       "   Pprice       Rev    NetRev  LostRev    Profit  ProfitMargin  \n",
       "0  4.9410  177.8760  177.8760      0.0  176.9260           1.0  \n",
       "1  3.7584  214.2288  214.2288      0.0  213.2788           1.0  \n",
       "2  4.7412  128.0124  128.0124      0.0  127.0624           1.0  \n",
       "3  4.8330  101.4930  101.4930      0.0  100.5430           1.0  \n",
       "4  3.8232  214.0992  214.0992      0.0  213.1492           1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: Onum:long,CID:long,Tdate:datetime,Pline:str,Pclass:str,Usales:long,Return:str,returnAmount:long,Mcost:double,Lprice:double,Ddisc:double,Cdisc:double,Odisc:double,Pdisc:double,Tdisc:double,Pprice:double,Rev:double,NetRev:double,LostRev:double,Profit:double,ProfitMargin:double</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM dfOrders20 \n",
    "LIMIT 5\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assemble the customer attribute data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1700</td>\n",
       "      <td>MT</td>\n",
       "      <td>59821</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>850</td>\n",
       "      <td>ND</td>\n",
       "      <td>58068</td>\n",
       "      <td>Midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>280</td>\n",
       "      <td>NY</td>\n",
       "      <td>10007</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1574</td>\n",
       "      <td>WY</td>\n",
       "      <td>83120</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>CO</td>\n",
       "      <td>80403</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CID State    ZIP     Region\n",
       "0  1700    MT  59821       West\n",
       "1   850    ND  58068    Midwest\n",
       "2   280    NY  10007  Northeast\n",
       "3  1574    WY  83120       West\n",
       "4   110    CO  80403       West"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data and do some EDA\n",
    "dfCustomers = pd.read_csv( datapathCust)\n",
    "dfCustomers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_995c5_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_995c5_\">\n",
       "  <caption>DataFrame Dimensions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_995c5_level0_row0\" class=\"row_heading level0 row0\" >Number of Rows</th>\n",
       "      <td id=\"T_995c5_row0_col0\" class=\"data row0 col0\" >1,136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_995c5_level0_row1\" class=\"row_heading level0 row1\" >Number of Columns</th>\n",
       "      <td id=\"T_995c5_row1_col0\" class=\"data row1 col0\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7389e72b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ac779_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "#T_ac779_row0_col0, #T_ac779_row0_col1, #T_ac779_row1_col0, #T_ac779_row1_col1, #T_ac779_row2_col0, #T_ac779_row2_col1, #T_ac779_row3_col0, #T_ac779_row3_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ac779_\">\n",
       "  <caption>DataFrame Column Check for DataFrame dfCustomers</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Leading White Spaces</th>\n",
       "      <th class=\"col_heading level0 col1\" >Trailing White Spaces</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Column Name</th>\n",
       "      <th class=\"index_name level1\" >#Characters</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ac779_level0_row0\" class=\"row_heading level0 row0\" >CID</th>\n",
       "      <th id=\"T_ac779_level1_row0\" class=\"row_heading level1 row0\" >3</th>\n",
       "      <td id=\"T_ac779_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_ac779_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac779_level0_row1\" class=\"row_heading level0 row1\" >State</th>\n",
       "      <th id=\"T_ac779_level1_row1\" class=\"row_heading level1 row1\" >5</th>\n",
       "      <td id=\"T_ac779_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_ac779_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac779_level0_row2\" class=\"row_heading level0 row2\" >ZIP</th>\n",
       "      <th id=\"T_ac779_level1_row2\" class=\"row_heading level1 row2\" >3</th>\n",
       "      <td id=\"T_ac779_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_ac779_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac779_level0_row3\" class=\"row_heading level0 row3\" >Region</th>\n",
       "      <th id=\"T_ac779_level1_row3\" class=\"row_heading level1 row3\" >6</th>\n",
       "      <td id=\"T_ac779_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_ac779_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a05e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: n = 4 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2e5b9_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2e5b9_\">\n",
       "  <caption>Missing Value Report</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Total</th>\n",
       "      <th class=\"col_heading level0 col1\" >Missing</th>\n",
       "      <th class=\"col_heading level0 col2\" >Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2e5b9_level0_row0\" class=\"row_heading level0 row0\" >CID</th>\n",
       "      <td id=\"T_2e5b9_row0_col0\" class=\"data row0 col0\" >1,136</td>\n",
       "      <td id=\"T_2e5b9_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_2e5b9_row0_col2\" class=\"data row0 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e5b9_level0_row1\" class=\"row_heading level0 row1\" >State</th>\n",
       "      <td id=\"T_2e5b9_row1_col0\" class=\"data row1 col0\" >1,136</td>\n",
       "      <td id=\"T_2e5b9_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_2e5b9_row1_col2\" class=\"data row1 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e5b9_level0_row2\" class=\"row_heading level0 row2\" >ZIP</th>\n",
       "      <td id=\"T_2e5b9_row2_col0\" class=\"data row2 col0\" >1,136</td>\n",
       "      <td id=\"T_2e5b9_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_2e5b9_row2_col2\" class=\"data row2 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e5b9_level0_row3\" class=\"row_heading level0 row3\" >Region</th>\n",
       "      <td id=\"T_2e5b9_row3_col0\" class=\"data row3 col0\" >1,136</td>\n",
       "      <td id=\"T_2e5b9_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_2e5b9_row3_col2\" class=\"data row3 col2\" >0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a05e050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: n = 1136\n"
     ]
    }
   ],
   "source": [
    "dfCustomers.describe()\n",
    "df_size( dfCustomers )\n",
    "column_check ( dfCustomers )\n",
    "mvReport (dfCustomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "* no NULLS, data looks good\n",
    "\n",
    "Let's make sure we can `JOIN` the 2 dataframes without losing any data.  ie, the `keys` are valid in both tables.  We know we have 70270 in `dfOrders` and we don't want to lose any of those rows due to invalid Customer data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrderCount\n",
       "0       70270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: OrderCount:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "\n",
    "SELECT count(*) AS OrderCount\n",
    "FROM dfOrders20 o\n",
    "JOIN dfCustomers c \n",
    "    ON o.CID = c.CID\n",
    "PRINT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CID` keys look good, let's join the dataframes together and output the results to use later.\n",
    "\n",
    "Note:\n",
    "* instead of `YIELD` we are actually materializing the table.  \n",
    "* we'd probably want to do this in our datalake sandbox but I just use a local file for ease\n",
    "* `csv` is a horrible format (`PARQUET` is probably better) but let's keep everything simple for now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "\n",
    "SELECT \n",
    "    c.State, c.Region , o.* \n",
    "FROM dfOrders20 o\n",
    "JOIN dfCustomers c \n",
    "    ON o.CID = c.CID\n",
    "SAVE OVERWRITE \"/tmp/dfOrder30.csv\" (header=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reload the data from the file.  \n",
    "\n",
    "Why?\n",
    "\n",
    "Because I want to do everything in python now.  python is a little bit easier than SQL for certain tasks.  \n",
    "\n",
    "We should now have a dataset that is usable for further analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c9d47_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c9d47_\">\n",
       "  <caption>Base dfMerged DataFrame</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >State</th>\n",
       "      <th class=\"col_heading level0 col1\" >Region</th>\n",
       "      <th class=\"col_heading level0 col2\" >Onum</th>\n",
       "      <th class=\"col_heading level0 col3\" >CID</th>\n",
       "      <th class=\"col_heading level0 col4\" >Tdate</th>\n",
       "      <th class=\"col_heading level0 col5\" >Pline</th>\n",
       "      <th class=\"col_heading level0 col6\" >Pclass</th>\n",
       "      <th class=\"col_heading level0 col7\" >Usales</th>\n",
       "      <th class=\"col_heading level0 col8\" >Return</th>\n",
       "      <th class=\"col_heading level0 col9\" >returnAmount</th>\n",
       "      <th class=\"col_heading level0 col10\" >Mcost</th>\n",
       "      <th class=\"col_heading level0 col11\" >Lprice</th>\n",
       "      <th class=\"col_heading level0 col12\" >Ddisc</th>\n",
       "      <th class=\"col_heading level0 col13\" >Cdisc</th>\n",
       "      <th class=\"col_heading level0 col14\" >Odisc</th>\n",
       "      <th class=\"col_heading level0 col15\" >Pdisc</th>\n",
       "      <th class=\"col_heading level0 col16\" >Tdisc</th>\n",
       "      <th class=\"col_heading level0 col17\" >Pprice</th>\n",
       "      <th class=\"col_heading level0 col18\" >Rev</th>\n",
       "      <th class=\"col_heading level0 col19\" >NetRev</th>\n",
       "      <th class=\"col_heading level0 col20\" >LostRev</th>\n",
       "      <th class=\"col_heading level0 col21\" >Profit</th>\n",
       "      <th class=\"col_heading level0 col22\" >ProfitMargin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c9d47_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c9d47_row0_col0\" class=\"data row0 col0\" >MI</td>\n",
       "      <td id=\"T_c9d47_row0_col1\" class=\"data row0 col1\" >Midwest</td>\n",
       "      <td id=\"T_c9d47_row0_col2\" class=\"data row0 col2\" >585</td>\n",
       "      <td id=\"T_c9d47_row0_col3\" class=\"data row0 col3\" >1015</td>\n",
       "      <td id=\"T_c9d47_row0_col4\" class=\"data row0 col4\" >2004-01-25</td>\n",
       "      <td id=\"T_c9d47_row0_col5\" class=\"data row0 col5\" >Living Room</td>\n",
       "      <td id=\"T_c9d47_row0_col6\" class=\"data row0 col6\" >Window Treatment: Blinds</td>\n",
       "      <td id=\"T_c9d47_row0_col7\" class=\"data row0 col7\" >36</td>\n",
       "      <td id=\"T_c9d47_row0_col8\" class=\"data row0 col8\" >No</td>\n",
       "      <td id=\"T_c9d47_row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "      <td id=\"T_c9d47_row0_col10\" class=\"data row0 col10\" >0.950000</td>\n",
       "      <td id=\"T_c9d47_row0_col11\" class=\"data row0 col11\" >5.400000</td>\n",
       "      <td id=\"T_c9d47_row0_col12\" class=\"data row0 col12\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row0_col13\" class=\"data row0 col13\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row0_col14\" class=\"data row0 col14\" >0.043000</td>\n",
       "      <td id=\"T_c9d47_row0_col15\" class=\"data row0 col15\" >0.042000</td>\n",
       "      <td id=\"T_c9d47_row0_col16\" class=\"data row0 col16\" >0.085000</td>\n",
       "      <td id=\"T_c9d47_row0_col17\" class=\"data row0 col17\" >4.941000</td>\n",
       "      <td id=\"T_c9d47_row0_col18\" class=\"data row0 col18\" >177.876000</td>\n",
       "      <td id=\"T_c9d47_row0_col19\" class=\"data row0 col19\" >177.876000</td>\n",
       "      <td id=\"T_c9d47_row0_col20\" class=\"data row0 col20\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row0_col21\" class=\"data row0 col21\" >176.926000</td>\n",
       "      <td id=\"T_c9d47_row0_col22\" class=\"data row0 col22\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9d47_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c9d47_row1_col0\" class=\"data row1 col0\" >MI</td>\n",
       "      <td id=\"T_c9d47_row1_col1\" class=\"data row1 col1\" >Midwest</td>\n",
       "      <td id=\"T_c9d47_row1_col2\" class=\"data row1 col2\" >586</td>\n",
       "      <td id=\"T_c9d47_row1_col3\" class=\"data row1 col3\" >1015</td>\n",
       "      <td id=\"T_c9d47_row1_col4\" class=\"data row1 col4\" >2004-01-25</td>\n",
       "      <td id=\"T_c9d47_row1_col5\" class=\"data row1 col5\" >Living Room</td>\n",
       "      <td id=\"T_c9d47_row1_col6\" class=\"data row1 col6\" >Window Treatment: Blinds</td>\n",
       "      <td id=\"T_c9d47_row1_col7\" class=\"data row1 col7\" >57</td>\n",
       "      <td id=\"T_c9d47_row1_col8\" class=\"data row1 col8\" >No</td>\n",
       "      <td id=\"T_c9d47_row1_col9\" class=\"data row1 col9\" >0</td>\n",
       "      <td id=\"T_c9d47_row1_col10\" class=\"data row1 col10\" >0.950000</td>\n",
       "      <td id=\"T_c9d47_row1_col11\" class=\"data row1 col11\" >5.400000</td>\n",
       "      <td id=\"T_c9d47_row1_col12\" class=\"data row1 col12\" >0.157000</td>\n",
       "      <td id=\"T_c9d47_row1_col13\" class=\"data row1 col13\" >0.075000</td>\n",
       "      <td id=\"T_c9d47_row1_col14\" class=\"data row1 col14\" >0.041000</td>\n",
       "      <td id=\"T_c9d47_row1_col15\" class=\"data row1 col15\" >0.031000</td>\n",
       "      <td id=\"T_c9d47_row1_col16\" class=\"data row1 col16\" >0.304000</td>\n",
       "      <td id=\"T_c9d47_row1_col17\" class=\"data row1 col17\" >3.758400</td>\n",
       "      <td id=\"T_c9d47_row1_col18\" class=\"data row1 col18\" >214.228800</td>\n",
       "      <td id=\"T_c9d47_row1_col19\" class=\"data row1 col19\" >214.228800</td>\n",
       "      <td id=\"T_c9d47_row1_col20\" class=\"data row1 col20\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row1_col21\" class=\"data row1 col21\" >213.278800</td>\n",
       "      <td id=\"T_c9d47_row1_col22\" class=\"data row1 col22\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9d47_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c9d47_row2_col0\" class=\"data row2 col0\" >MI</td>\n",
       "      <td id=\"T_c9d47_row2_col1\" class=\"data row2 col1\" >Midwest</td>\n",
       "      <td id=\"T_c9d47_row2_col2\" class=\"data row2 col2\" >587</td>\n",
       "      <td id=\"T_c9d47_row2_col3\" class=\"data row2 col3\" >1015</td>\n",
       "      <td id=\"T_c9d47_row2_col4\" class=\"data row2 col4\" >2004-01-25</td>\n",
       "      <td id=\"T_c9d47_row2_col5\" class=\"data row2 col5\" >Living Room</td>\n",
       "      <td id=\"T_c9d47_row2_col6\" class=\"data row2 col6\" >Window Treatment: Blinds</td>\n",
       "      <td id=\"T_c9d47_row2_col7\" class=\"data row2 col7\" >27</td>\n",
       "      <td id=\"T_c9d47_row2_col8\" class=\"data row2 col8\" >No</td>\n",
       "      <td id=\"T_c9d47_row2_col9\" class=\"data row2 col9\" >0</td>\n",
       "      <td id=\"T_c9d47_row2_col10\" class=\"data row2 col10\" >0.950000</td>\n",
       "      <td id=\"T_c9d47_row2_col11\" class=\"data row2 col11\" >5.400000</td>\n",
       "      <td id=\"T_c9d47_row2_col12\" class=\"data row2 col12\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row2_col13\" class=\"data row2 col13\" >0.048000</td>\n",
       "      <td id=\"T_c9d47_row2_col14\" class=\"data row2 col14\" >0.053000</td>\n",
       "      <td id=\"T_c9d47_row2_col15\" class=\"data row2 col15\" >0.021000</td>\n",
       "      <td id=\"T_c9d47_row2_col16\" class=\"data row2 col16\" >0.122000</td>\n",
       "      <td id=\"T_c9d47_row2_col17\" class=\"data row2 col17\" >4.741200</td>\n",
       "      <td id=\"T_c9d47_row2_col18\" class=\"data row2 col18\" >128.012400</td>\n",
       "      <td id=\"T_c9d47_row2_col19\" class=\"data row2 col19\" >128.012400</td>\n",
       "      <td id=\"T_c9d47_row2_col20\" class=\"data row2 col20\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row2_col21\" class=\"data row2 col21\" >127.062400</td>\n",
       "      <td id=\"T_c9d47_row2_col22\" class=\"data row2 col22\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9d47_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c9d47_row3_col0\" class=\"data row3 col0\" >MI</td>\n",
       "      <td id=\"T_c9d47_row3_col1\" class=\"data row3 col1\" >Midwest</td>\n",
       "      <td id=\"T_c9d47_row3_col2\" class=\"data row3 col2\" >588</td>\n",
       "      <td id=\"T_c9d47_row3_col3\" class=\"data row3 col3\" >1015</td>\n",
       "      <td id=\"T_c9d47_row3_col4\" class=\"data row3 col4\" >2004-01-25</td>\n",
       "      <td id=\"T_c9d47_row3_col5\" class=\"data row3 col5\" >Living Room</td>\n",
       "      <td id=\"T_c9d47_row3_col6\" class=\"data row3 col6\" >Window Treatment: Blinds</td>\n",
       "      <td id=\"T_c9d47_row3_col7\" class=\"data row3 col7\" >21</td>\n",
       "      <td id=\"T_c9d47_row3_col8\" class=\"data row3 col8\" >No</td>\n",
       "      <td id=\"T_c9d47_row3_col9\" class=\"data row3 col9\" >0</td>\n",
       "      <td id=\"T_c9d47_row3_col10\" class=\"data row3 col10\" >0.950000</td>\n",
       "      <td id=\"T_c9d47_row3_col11\" class=\"data row3 col11\" >5.400000</td>\n",
       "      <td id=\"T_c9d47_row3_col12\" class=\"data row3 col12\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row3_col13\" class=\"data row3 col13\" >0.072000</td>\n",
       "      <td id=\"T_c9d47_row3_col14\" class=\"data row3 col14\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row3_col15\" class=\"data row3 col15\" >0.033000</td>\n",
       "      <td id=\"T_c9d47_row3_col16\" class=\"data row3 col16\" >0.105000</td>\n",
       "      <td id=\"T_c9d47_row3_col17\" class=\"data row3 col17\" >4.833000</td>\n",
       "      <td id=\"T_c9d47_row3_col18\" class=\"data row3 col18\" >101.493000</td>\n",
       "      <td id=\"T_c9d47_row3_col19\" class=\"data row3 col19\" >101.493000</td>\n",
       "      <td id=\"T_c9d47_row3_col20\" class=\"data row3 col20\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row3_col21\" class=\"data row3 col21\" >100.543000</td>\n",
       "      <td id=\"T_c9d47_row3_col22\" class=\"data row3 col22\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9d47_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c9d47_row4_col0\" class=\"data row4 col0\" >MI</td>\n",
       "      <td id=\"T_c9d47_row4_col1\" class=\"data row4 col1\" >Midwest</td>\n",
       "      <td id=\"T_c9d47_row4_col2\" class=\"data row4 col2\" >589</td>\n",
       "      <td id=\"T_c9d47_row4_col3\" class=\"data row4 col3\" >1015</td>\n",
       "      <td id=\"T_c9d47_row4_col4\" class=\"data row4 col4\" >2004-01-25</td>\n",
       "      <td id=\"T_c9d47_row4_col5\" class=\"data row4 col5\" >Living Room</td>\n",
       "      <td id=\"T_c9d47_row4_col6\" class=\"data row4 col6\" >Window Treatment: Blinds</td>\n",
       "      <td id=\"T_c9d47_row4_col7\" class=\"data row4 col7\" >56</td>\n",
       "      <td id=\"T_c9d47_row4_col8\" class=\"data row4 col8\" >No</td>\n",
       "      <td id=\"T_c9d47_row4_col9\" class=\"data row4 col9\" >0</td>\n",
       "      <td id=\"T_c9d47_row4_col10\" class=\"data row4 col10\" >0.950000</td>\n",
       "      <td id=\"T_c9d47_row4_col11\" class=\"data row4 col11\" >5.400000</td>\n",
       "      <td id=\"T_c9d47_row4_col12\" class=\"data row4 col12\" >0.140000</td>\n",
       "      <td id=\"T_c9d47_row4_col13\" class=\"data row4 col13\" >0.056000</td>\n",
       "      <td id=\"T_c9d47_row4_col14\" class=\"data row4 col14\" >0.041000</td>\n",
       "      <td id=\"T_c9d47_row4_col15\" class=\"data row4 col15\" >0.055000</td>\n",
       "      <td id=\"T_c9d47_row4_col16\" class=\"data row4 col16\" >0.292000</td>\n",
       "      <td id=\"T_c9d47_row4_col17\" class=\"data row4 col17\" >3.823200</td>\n",
       "      <td id=\"T_c9d47_row4_col18\" class=\"data row4 col18\" >214.099200</td>\n",
       "      <td id=\"T_c9d47_row4_col19\" class=\"data row4 col19\" >214.099200</td>\n",
       "      <td id=\"T_c9d47_row4_col20\" class=\"data row4 col20\" >0.000000</td>\n",
       "      <td id=\"T_c9d47_row4_col21\" class=\"data row4 col21\" >213.149200</td>\n",
       "      <td id=\"T_c9d47_row4_col22\" class=\"data row4 col22\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a090450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: All dfMerged data\n"
     ]
    }
   ],
   "source": [
    "dfMerged = pd.read_csv( \"/tmp/dfOrder30.csv\")\n",
    "\n",
    "display( dfMerged.head().style.set_caption( 'Base dfMerged DataFrame' ).\\\n",
    "    set_table_styles( tbl_styles ))\n",
    "print( 'Base: All dfMerged data' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, count the number of unique *CID*s.\n",
    "\n",
    "We have 779 unique customers in our 1136 row dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mNumber of Unique CIDs:\u001b[0m\n",
      "779\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## How many unique CIDs are available?\n",
    "##\n",
    "x = dfMerged.CID.nunique()\n",
    "printbold( 'Number of Unique CIDs:' )\n",
    "print( f'{x}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Since we are dealing with a _customer churn_ problem we need to aggregate the data to the `CID` level so we can model it.  \n",
    "\n",
    "We could do this with SQL but let's see how to do it in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d819f_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d819f_\">\n",
       "  <caption>dfAgg:  Aggregated Data</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CID</th>\n",
       "      <th class=\"col_heading level0 col1\" >Region</th>\n",
       "      <th class=\"col_heading level0 col2\" >totalUsales</th>\n",
       "      <th class=\"col_heading level0 col3\" >meanPprice</th>\n",
       "      <th class=\"col_heading level0 col4\" >meanDdisc</th>\n",
       "      <th class=\"col_heading level0 col5\" >meanOdisc</th>\n",
       "      <th class=\"col_heading level0 col6\" >meanCdisc</th>\n",
       "      <th class=\"col_heading level0 col7\" >meanPdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d819f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d819f_row0_col0\" class=\"data row0 col0\" >14</td>\n",
       "      <td id=\"T_d819f_row0_col1\" class=\"data row0 col1\" >Northeast</td>\n",
       "      <td id=\"T_d819f_row0_col2\" class=\"data row0 col2\" >3,461</td>\n",
       "      <td id=\"T_d819f_row0_col3\" class=\"data row0 col3\" >$5.40</td>\n",
       "      <td id=\"T_d819f_row0_col4\" class=\"data row0 col4\" >13.2%</td>\n",
       "      <td id=\"T_d819f_row0_col5\" class=\"data row0 col5\" >5.1%</td>\n",
       "      <td id=\"T_d819f_row0_col6\" class=\"data row0 col6\" >6.8%</td>\n",
       "      <td id=\"T_d819f_row0_col7\" class=\"data row0 col7\" >3.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d819f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d819f_row1_col0\" class=\"data row1 col0\" >17</td>\n",
       "      <td id=\"T_d819f_row1_col1\" class=\"data row1 col1\" >West</td>\n",
       "      <td id=\"T_d819f_row1_col2\" class=\"data row1 col2\" >1,001</td>\n",
       "      <td id=\"T_d819f_row1_col3\" class=\"data row1 col3\" >$5.80</td>\n",
       "      <td id=\"T_d819f_row1_col4\" class=\"data row1 col4\" >12.9%</td>\n",
       "      <td id=\"T_d819f_row1_col5\" class=\"data row1 col5\" >5.0%</td>\n",
       "      <td id=\"T_d819f_row1_col6\" class=\"data row1 col6\" >7.2%</td>\n",
       "      <td id=\"T_d819f_row1_col7\" class=\"data row1 col7\" >3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d819f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d819f_row2_col0\" class=\"data row2 col0\" >26</td>\n",
       "      <td id=\"T_d819f_row2_col1\" class=\"data row2 col1\" >West</td>\n",
       "      <td id=\"T_d819f_row2_col2\" class=\"data row2 col2\" >787</td>\n",
       "      <td id=\"T_d819f_row2_col3\" class=\"data row2 col3\" >$5.74</td>\n",
       "      <td id=\"T_d819f_row2_col4\" class=\"data row2 col4\" >13.6%</td>\n",
       "      <td id=\"T_d819f_row2_col5\" class=\"data row2 col5\" >4.7%</td>\n",
       "      <td id=\"T_d819f_row2_col6\" class=\"data row2 col6\" >7.1%</td>\n",
       "      <td id=\"T_d819f_row2_col7\" class=\"data row2 col7\" >4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d819f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d819f_row3_col0\" class=\"data row3 col0\" >28</td>\n",
       "      <td id=\"T_d819f_row3_col1\" class=\"data row3 col1\" >West</td>\n",
       "      <td id=\"T_d819f_row3_col2\" class=\"data row3 col2\" >1,873</td>\n",
       "      <td id=\"T_d819f_row3_col3\" class=\"data row3 col3\" >$5.70</td>\n",
       "      <td id=\"T_d819f_row3_col4\" class=\"data row3 col4\" >13.7%</td>\n",
       "      <td id=\"T_d819f_row3_col5\" class=\"data row3 col5\" >5.1%</td>\n",
       "      <td id=\"T_d819f_row3_col6\" class=\"data row3 col6\" >7.1%</td>\n",
       "      <td id=\"T_d819f_row3_col7\" class=\"data row3 col7\" >4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d819f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d819f_row4_col0\" class=\"data row4 col0\" >38</td>\n",
       "      <td id=\"T_d819f_row4_col1\" class=\"data row4 col1\" >West</td>\n",
       "      <td id=\"T_d819f_row4_col2\" class=\"data row4 col2\" >2,350</td>\n",
       "      <td id=\"T_d819f_row4_col3\" class=\"data row4 col3\" >$5.72</td>\n",
       "      <td id=\"T_d819f_row4_col4\" class=\"data row4 col4\" >13.5%</td>\n",
       "      <td id=\"T_d819f_row4_col5\" class=\"data row4 col5\" >5.0%</td>\n",
       "      <td id=\"T_d819f_row4_col6\" class=\"data row4 col6\" >7.0%</td>\n",
       "      <td id=\"T_d819f_row4_col7\" class=\"data row4 col7\" >4.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7389e57610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## I think these are the vars we can likely use for modeling\n",
    "\n",
    "cols = [ 'CID', 'Region', 'Usales', 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "\n",
    "# grouping cols and aggregations\n",
    "grp = [ 'CID', 'Region' ]\n",
    "aggregations = { 'Usales':'sum', 'Pprice':'mean', 'Ddisc':'mean', 'Odisc':'mean',\n",
    "                 'Cdisc':'mean', 'Pdisc':'mean'}\n",
    "\n",
    "# Use groupby with agg function to aggregate\n",
    "tmp = dfMerged[ cols ].copy()\n",
    "dfAgg = tmp.groupby( grp ).agg( aggregations )\n",
    "\n",
    "# rename cols.  \n",
    "dfAgg.rename( columns = { \n",
    "        'Usales':'totalUsales', \n",
    "        'Pprice':'meanPprice', \n",
    "        'Ddisc':'meanDdisc',\n",
    "        'Odisc':'meanOdisc', \n",
    "        'Cdisc':'meanCdisc',\n",
    "        'Pdisc':'meanPdisc'}, inplace = True )\n",
    "dfAgg = dfAgg.reset_index()\n",
    "\n",
    "# this just makes it easier to read...similar to CAST in SQL\n",
    "formatting = { 'totalUsales':'{0:,.0f}', 'meanPprice':'${0:.2f}', 'meanDdisc':'{0:,.1%}', 'meanCdisc':'{0:,.1%}', 'meanOdisc':'{0:,.1%}', \n",
    "             'meanPdisc':'{0:,.1%}' }\n",
    "display( dfAgg.head().style.set_caption( 'dfAgg:  Aggregated Data' ).\\\n",
    "    set_table_styles( tbl_styles ).format( formatting ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9c78f_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9c78f_\">\n",
       "  <caption>DataFrame Dimensions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9c78f_level0_row0\" class=\"row_heading level0 row0\" >Number of Rows</th>\n",
       "      <td id=\"T_9c78f_row0_col0\" class=\"data row0 col0\" >779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c78f_level0_row1\" class=\"row_heading level0 row1\" >Number of Columns</th>\n",
       "      <td id=\"T_9c78f_row1_col0\" class=\"data row1 col0\" >8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7389e5b4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_size( dfAgg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "779 makes sense since that is our unique CIDs.  \n",
    "\n",
    "# Whew!  We made it!\n",
    "\n",
    "That took a while to do the _data engineering_ but we are finally ready to do our customer segmentation.  \n",
    "\n",
    "## Clustering to Get Customer Segments\n",
    "\n",
    "Let's assume we don't know _a priori_ how we should segment our customers.  \n",
    "\n",
    "> Can the data help us to form an opinion on how to segment our customer?  Yes!\n",
    "\n",
    "Our dataset is tiny and doesn't have a lot of columns.  But with the few columns we have we can experiment. \n",
    "\n",
    "Let's start by taking some guesses at how we might segment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f21c4_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f21c4_\">\n",
       "  <caption>Subset Data for Clustering</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Region</th>\n",
       "      <th class=\"col_heading level0 col1\" >totalUsales</th>\n",
       "      <th class=\"col_heading level0 col2\" >meanPprice</th>\n",
       "      <th class=\"col_heading level0 col3\" >meanDdisc</th>\n",
       "      <th class=\"col_heading level0 col4\" >meanCdisc</th>\n",
       "      <th class=\"col_heading level0 col5\" >meanPdisc</th>\n",
       "      <th class=\"col_heading level0 col6\" >meanOdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f21c4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f21c4_row0_col0\" class=\"data row0 col0\" >Northeast</td>\n",
       "      <td id=\"T_f21c4_row0_col1\" class=\"data row0 col1\" >3,461</td>\n",
       "      <td id=\"T_f21c4_row0_col2\" class=\"data row0 col2\" >$5.40</td>\n",
       "      <td id=\"T_f21c4_row0_col3\" class=\"data row0 col3\" >13.2%</td>\n",
       "      <td id=\"T_f21c4_row0_col4\" class=\"data row0 col4\" >6.8%</td>\n",
       "      <td id=\"T_f21c4_row0_col5\" class=\"data row0 col5\" >3.8%</td>\n",
       "      <td id=\"T_f21c4_row0_col6\" class=\"data row0 col6\" >5.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f21c4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f21c4_row1_col0\" class=\"data row1 col0\" >West</td>\n",
       "      <td id=\"T_f21c4_row1_col1\" class=\"data row1 col1\" >1,001</td>\n",
       "      <td id=\"T_f21c4_row1_col2\" class=\"data row1 col2\" >$5.80</td>\n",
       "      <td id=\"T_f21c4_row1_col3\" class=\"data row1 col3\" >12.9%</td>\n",
       "      <td id=\"T_f21c4_row1_col4\" class=\"data row1 col4\" >7.2%</td>\n",
       "      <td id=\"T_f21c4_row1_col5\" class=\"data row1 col5\" >3.7%</td>\n",
       "      <td id=\"T_f21c4_row1_col6\" class=\"data row1 col6\" >5.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f21c4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f21c4_row2_col0\" class=\"data row2 col0\" >West</td>\n",
       "      <td id=\"T_f21c4_row2_col1\" class=\"data row2 col1\" >787</td>\n",
       "      <td id=\"T_f21c4_row2_col2\" class=\"data row2 col2\" >$5.74</td>\n",
       "      <td id=\"T_f21c4_row2_col3\" class=\"data row2 col3\" >13.6%</td>\n",
       "      <td id=\"T_f21c4_row2_col4\" class=\"data row2 col4\" >7.1%</td>\n",
       "      <td id=\"T_f21c4_row2_col5\" class=\"data row2 col5\" >4.0%</td>\n",
       "      <td id=\"T_f21c4_row2_col6\" class=\"data row2 col6\" >4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f21c4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f21c4_row3_col0\" class=\"data row3 col0\" >West</td>\n",
       "      <td id=\"T_f21c4_row3_col1\" class=\"data row3 col1\" >1,873</td>\n",
       "      <td id=\"T_f21c4_row3_col2\" class=\"data row3 col2\" >$5.70</td>\n",
       "      <td id=\"T_f21c4_row3_col3\" class=\"data row3 col3\" >13.7%</td>\n",
       "      <td id=\"T_f21c4_row3_col4\" class=\"data row3 col4\" >7.1%</td>\n",
       "      <td id=\"T_f21c4_row3_col5\" class=\"data row3 col5\" >4.1%</td>\n",
       "      <td id=\"T_f21c4_row3_col6\" class=\"data row3 col6\" >5.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f21c4_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f21c4_row4_col0\" class=\"data row4 col0\" >West</td>\n",
       "      <td id=\"T_f21c4_row4_col1\" class=\"data row4 col1\" >2,350</td>\n",
       "      <td id=\"T_f21c4_row4_col2\" class=\"data row4 col2\" >$5.72</td>\n",
       "      <td id=\"T_f21c4_row4_col3\" class=\"data row4 col3\" >13.5%</td>\n",
       "      <td id=\"T_f21c4_row4_col4\" class=\"data row4 col4\" >7.0%</td>\n",
       "      <td id=\"T_f21c4_row4_col5\" class=\"data row4 col5\" >4.1%</td>\n",
       "      <td id=\"T_f21c4_row4_col6\" class=\"data row4 col6\" >5.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a006750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets just grab a subset of the columns/features\n",
    "# this is what our initial guess would be for the clusters/segments\n",
    "\n",
    "cols = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "dfClusters = dfAgg[ cols ]\n",
    "\n",
    "display( dfClusters.head().style.set_caption( 'Subset Data for Clustering' ).\\\n",
    "    set_table_styles( tbl_styles ).format( formatting ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fc1a6_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fc1a6_\">\n",
       "  <caption>dfClusters: Standardized Data for Clustering</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Region</th>\n",
       "      <th class=\"col_heading level0 col1\" >totalUsales</th>\n",
       "      <th class=\"col_heading level0 col2\" >meanPprice</th>\n",
       "      <th class=\"col_heading level0 col3\" >meanDdisc</th>\n",
       "      <th class=\"col_heading level0 col4\" >meanCdisc</th>\n",
       "      <th class=\"col_heading level0 col5\" >meanPdisc</th>\n",
       "      <th class=\"col_heading level0 col6\" >meanOdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fc1a6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fc1a6_row0_col0\" class=\"data row0 col0\" >Northeast</td>\n",
       "      <td id=\"T_fc1a6_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_fc1a6_row0_col2\" class=\"data row0 col2\" >$0.22</td>\n",
       "      <td id=\"T_fc1a6_row0_col3\" class=\"data row0 col3\" >54.4%</td>\n",
       "      <td id=\"T_fc1a6_row0_col4\" class=\"data row0 col4\" >-59.6%</td>\n",
       "      <td id=\"T_fc1a6_row0_col5\" class=\"data row0 col5\" >-108.7%</td>\n",
       "      <td id=\"T_fc1a6_row0_col6\" class=\"data row0 col6\" >52.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc1a6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fc1a6_row1_col0\" class=\"data row1 col0\" >West</td>\n",
       "      <td id=\"T_fc1a6_row1_col1\" class=\"data row1 col1\" >-1</td>\n",
       "      <td id=\"T_fc1a6_row1_col2\" class=\"data row1 col2\" >$0.74</td>\n",
       "      <td id=\"T_fc1a6_row1_col3\" class=\"data row1 col3\" >43.0%</td>\n",
       "      <td id=\"T_fc1a6_row1_col4\" class=\"data row1 col4\" >62.5%</td>\n",
       "      <td id=\"T_fc1a6_row1_col5\" class=\"data row1 col5\" >-194.3%</td>\n",
       "      <td id=\"T_fc1a6_row1_col6\" class=\"data row1 col6\" >2.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc1a6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fc1a6_row2_col0\" class=\"data row2 col0\" >West</td>\n",
       "      <td id=\"T_fc1a6_row2_col1\" class=\"data row2 col1\" >-1</td>\n",
       "      <td id=\"T_fc1a6_row2_col2\" class=\"data row2 col2\" >$0.67</td>\n",
       "      <td id=\"T_fc1a6_row2_col3\" class=\"data row2 col3\" >72.2%</td>\n",
       "      <td id=\"T_fc1a6_row2_col4\" class=\"data row2 col4\" >40.7%</td>\n",
       "      <td id=\"T_fc1a6_row2_col5\" class=\"data row2 col5\" >6.6%</td>\n",
       "      <td id=\"T_fc1a6_row2_col6\" class=\"data row2 col6\" >-162.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc1a6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fc1a6_row3_col0\" class=\"data row3 col0\" >West</td>\n",
       "      <td id=\"T_fc1a6_row3_col1\" class=\"data row3 col1\" >-0</td>\n",
       "      <td id=\"T_fc1a6_row3_col2\" class=\"data row3 col2\" >$0.61</td>\n",
       "      <td id=\"T_fc1a6_row3_col3\" class=\"data row3 col3\" >73.7%</td>\n",
       "      <td id=\"T_fc1a6_row3_col4\" class=\"data row3 col4\" >51.7%</td>\n",
       "      <td id=\"T_fc1a6_row3_col5\" class=\"data row3 col5\" >40.6%</td>\n",
       "      <td id=\"T_fc1a6_row3_col6\" class=\"data row3 col6\" >29.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc1a6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fc1a6_row4_col0\" class=\"data row4 col0\" >West</td>\n",
       "      <td id=\"T_fc1a6_row4_col1\" class=\"data row4 col1\" >-0</td>\n",
       "      <td id=\"T_fc1a6_row4_col2\" class=\"data row4 col2\" >$0.64</td>\n",
       "      <td id=\"T_fc1a6_row4_col3\" class=\"data row4 col3\" >68.3%</td>\n",
       "      <td id=\"T_fc1a6_row4_col4\" class=\"data row4 col4\" >6.6%</td>\n",
       "      <td id=\"T_fc1a6_row4_col5\" class=\"data row4 col5\" >96.7%</td>\n",
       "      <td id=\"T_fc1a6_row4_col6\" class=\"data row4 col6\" >-2.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a093510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f95f6_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f95f6_\">\n",
       "  <caption>Descriptive Statistics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >count</th>\n",
       "      <th class=\"col_heading level0 col1\" >mean</th>\n",
       "      <th class=\"col_heading level0 col2\" >std</th>\n",
       "      <th class=\"col_heading level0 col3\" >min</th>\n",
       "      <th class=\"col_heading level0 col4\" >25%</th>\n",
       "      <th class=\"col_heading level0 col5\" >50%</th>\n",
       "      <th class=\"col_heading level0 col6\" >75%</th>\n",
       "      <th class=\"col_heading level0 col7\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f95f6_level0_row0\" class=\"row_heading level0 row0\" >totalUsales</th>\n",
       "      <td id=\"T_f95f6_row0_col0\" class=\"data row0 col0\" >779.0000</td>\n",
       "      <td id=\"T_f95f6_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_f95f6_row0_col2\" class=\"data row0 col2\" >1.0006</td>\n",
       "      <td id=\"T_f95f6_row0_col3\" class=\"data row0 col3\" >-0.9286</td>\n",
       "      <td id=\"T_f95f6_row0_col4\" class=\"data row0 col4\" >-0.6245</td>\n",
       "      <td id=\"T_f95f6_row0_col5\" class=\"data row0 col5\" >-0.3216</td>\n",
       "      <td id=\"T_f95f6_row0_col6\" class=\"data row0 col6\" >0.3514</td>\n",
       "      <td id=\"T_f95f6_row0_col7\" class=\"data row0 col7\" >13.5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f95f6_level0_row1\" class=\"row_heading level0 row1\" >meanPprice</th>\n",
       "      <td id=\"T_f95f6_row1_col0\" class=\"data row1 col0\" >779.0000</td>\n",
       "      <td id=\"T_f95f6_row1_col1\" class=\"data row1 col1\" >-0.0000</td>\n",
       "      <td id=\"T_f95f6_row1_col2\" class=\"data row1 col2\" >1.0006</td>\n",
       "      <td id=\"T_f95f6_row1_col3\" class=\"data row1 col3\" >-1.9850</td>\n",
       "      <td id=\"T_f95f6_row1_col4\" class=\"data row1 col4\" >-0.6832</td>\n",
       "      <td id=\"T_f95f6_row1_col5\" class=\"data row1 col5\" >0.0513</td>\n",
       "      <td id=\"T_f95f6_row1_col6\" class=\"data row1 col6\" >0.7143</td>\n",
       "      <td id=\"T_f95f6_row1_col7\" class=\"data row1 col7\" >1.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f95f6_level0_row2\" class=\"row_heading level0 row2\" >meanDdisc</th>\n",
       "      <td id=\"T_f95f6_row2_col0\" class=\"data row2 col0\" >779.0000</td>\n",
       "      <td id=\"T_f95f6_row2_col1\" class=\"data row2 col1\" >-0.0000</td>\n",
       "      <td id=\"T_f95f6_row2_col2\" class=\"data row2 col2\" >1.0006</td>\n",
       "      <td id=\"T_f95f6_row2_col3\" class=\"data row2 col3\" >-1.6727</td>\n",
       "      <td id=\"T_f95f6_row2_col4\" class=\"data row2 col4\" >-1.4499</td>\n",
       "      <td id=\"T_f95f6_row2_col5\" class=\"data row2 col5\" >0.5596</td>\n",
       "      <td id=\"T_f95f6_row2_col6\" class=\"data row2 col6\" >0.7101</td>\n",
       "      <td id=\"T_f95f6_row2_col7\" class=\"data row2 col7\" >1.2897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f95f6_level0_row3\" class=\"row_heading level0 row3\" >meanCdisc</th>\n",
       "      <td id=\"T_f95f6_row3_col0\" class=\"data row3 col0\" >779.0000</td>\n",
       "      <td id=\"T_f95f6_row3_col1\" class=\"data row3 col1\" >-0.0000</td>\n",
       "      <td id=\"T_f95f6_row3_col2\" class=\"data row3 col2\" >1.0006</td>\n",
       "      <td id=\"T_f95f6_row3_col3\" class=\"data row3 col3\" >-4.4355</td>\n",
       "      <td id=\"T_f95f6_row3_col4\" class=\"data row3 col4\" >-0.5506</td>\n",
       "      <td id=\"T_f95f6_row3_col5\" class=\"data row3 col5\" >0.0273</td>\n",
       "      <td id=\"T_f95f6_row3_col6\" class=\"data row3 col6\" >0.5221</td>\n",
       "      <td id=\"T_f95f6_row3_col7\" class=\"data row3 col7\" >4.5294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f95f6_level0_row4\" class=\"row_heading level0 row4\" >meanPdisc</th>\n",
       "      <td id=\"T_f95f6_row4_col0\" class=\"data row4 col0\" >779.0000</td>\n",
       "      <td id=\"T_f95f6_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
       "      <td id=\"T_f95f6_row4_col2\" class=\"data row4 col2\" >1.0006</td>\n",
       "      <td id=\"T_f95f6_row4_col3\" class=\"data row4 col3\" >-4.3912</td>\n",
       "      <td id=\"T_f95f6_row4_col4\" class=\"data row4 col4\" >-0.5728</td>\n",
       "      <td id=\"T_f95f6_row4_col5\" class=\"data row4 col5\" >0.0574</td>\n",
       "      <td id=\"T_f95f6_row4_col6\" class=\"data row4 col6\" >0.5811</td>\n",
       "      <td id=\"T_f95f6_row4_col7\" class=\"data row4 col7\" >3.2547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f95f6_level0_row5\" class=\"row_heading level0 row5\" >meanOdisc</th>\n",
       "      <td id=\"T_f95f6_row5_col0\" class=\"data row5 col0\" >779.0000</td>\n",
       "      <td id=\"T_f95f6_row5_col1\" class=\"data row5 col1\" >-0.0000</td>\n",
       "      <td id=\"T_f95f6_row5_col2\" class=\"data row5 col2\" >1.0006</td>\n",
       "      <td id=\"T_f95f6_row5_col3\" class=\"data row5 col3\" >-4.5218</td>\n",
       "      <td id=\"T_f95f6_row5_col4\" class=\"data row5 col4\" >-0.5562</td>\n",
       "      <td id=\"T_f95f6_row5_col5\" class=\"data row5 col5\" >0.0334</td>\n",
       "      <td id=\"T_f95f6_row5_col6\" class=\"data row5 col6\" >0.5671</td>\n",
       "      <td id=\"T_f95f6_row5_col7\" class=\"data row5 col7\" >3.7860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f738a64b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## standardize the 6 numeric cols.  \n",
    "##\n",
    "## Do you know why?\n",
    "##\n",
    "##\n",
    "##\n",
    "\n",
    "cols = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "\n",
    "tmp = dfClusters[ cols ]\n",
    "\n",
    "## Standardize and horizontally concatenate with the Region variable; \n",
    "tmp_standard = pd.DataFrame( StandardScaler().fit_transform( tmp ), columns = cols )\n",
    "dfClusters = pd.concat( [ dfClusters[ 'Region'], tmp_standard ], axis = 1 )\n",
    "##\n",
    "display( dfClusters.head().style.set_caption( 'dfClusters: Standardized Data for Clustering' ).\\\n",
    "    set_table_styles( tbl_styles ).format( formatting ) )\n",
    "display( dfClusters.describe().T.style.set_caption( 'Descriptive Statistics' ).\\\n",
    "    set_table_styles( tbl_styles ).format( p_value ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "* The standardization is a `z scale` so the mean is always `ZERO`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_63821_ caption {\n",
       "  color: darkblue;\n",
       "  font-size: 18px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_63821_\">\n",
       "  <caption>dfClusters: Character Labels as Numerics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >Region</th>\n",
       "      <th class=\"col_heading level0 col1\" >totalUsales</th>\n",
       "      <th class=\"col_heading level0 col2\" >meanPprice</th>\n",
       "      <th class=\"col_heading level0 col3\" >meanDdisc</th>\n",
       "      <th class=\"col_heading level0 col4\" >meanCdisc</th>\n",
       "      <th class=\"col_heading level0 col5\" >meanPdisc</th>\n",
       "      <th class=\"col_heading level0 col6\" >meanOdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_63821_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_63821_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_63821_row0_col2\" class=\"data row0 col2\" >$0.22</td>\n",
       "      <td id=\"T_63821_row0_col3\" class=\"data row0 col3\" >54.4%</td>\n",
       "      <td id=\"T_63821_row0_col4\" class=\"data row0 col4\" >-59.6%</td>\n",
       "      <td id=\"T_63821_row0_col5\" class=\"data row0 col5\" >-108.7%</td>\n",
       "      <td id=\"T_63821_row0_col6\" class=\"data row0 col6\" >52.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63821_row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "      <td id=\"T_63821_row1_col1\" class=\"data row1 col1\" >-1</td>\n",
       "      <td id=\"T_63821_row1_col2\" class=\"data row1 col2\" >$0.74</td>\n",
       "      <td id=\"T_63821_row1_col3\" class=\"data row1 col3\" >43.0%</td>\n",
       "      <td id=\"T_63821_row1_col4\" class=\"data row1 col4\" >62.5%</td>\n",
       "      <td id=\"T_63821_row1_col5\" class=\"data row1 col5\" >-194.3%</td>\n",
       "      <td id=\"T_63821_row1_col6\" class=\"data row1 col6\" >2.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63821_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_63821_row2_col1\" class=\"data row2 col1\" >-1</td>\n",
       "      <td id=\"T_63821_row2_col2\" class=\"data row2 col2\" >$0.67</td>\n",
       "      <td id=\"T_63821_row2_col3\" class=\"data row2 col3\" >72.2%</td>\n",
       "      <td id=\"T_63821_row2_col4\" class=\"data row2 col4\" >40.7%</td>\n",
       "      <td id=\"T_63821_row2_col5\" class=\"data row2 col5\" >6.6%</td>\n",
       "      <td id=\"T_63821_row2_col6\" class=\"data row2 col6\" >-162.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63821_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_63821_row3_col1\" class=\"data row3 col1\" >-0</td>\n",
       "      <td id=\"T_63821_row3_col2\" class=\"data row3 col2\" >$0.61</td>\n",
       "      <td id=\"T_63821_row3_col3\" class=\"data row3 col3\" >73.7%</td>\n",
       "      <td id=\"T_63821_row3_col4\" class=\"data row3 col4\" >51.7%</td>\n",
       "      <td id=\"T_63821_row3_col5\" class=\"data row3 col5\" >40.6%</td>\n",
       "      <td id=\"T_63821_row3_col6\" class=\"data row3 col6\" >29.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63821_row4_col0\" class=\"data row4 col0\" >3</td>\n",
       "      <td id=\"T_63821_row4_col1\" class=\"data row4 col1\" >-0</td>\n",
       "      <td id=\"T_63821_row4_col2\" class=\"data row4 col2\" >$0.64</td>\n",
       "      <td id=\"T_63821_row4_col3\" class=\"data row4 col3\" >68.3%</td>\n",
       "      <td id=\"T_63821_row4_col4\" class=\"data row4 col4\" >6.6%</td>\n",
       "      <td id=\"T_63821_row4_col5\" class=\"data row4 col5\" >96.7%</td>\n",
       "      <td id=\"T_63821_row4_col6\" class=\"data row4 col6\" >-2.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7389ffb590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need Region, which is categorial, to be transformed into a numeric categorical value\n",
    "##\n",
    "## Do you know why?\n",
    "##\n",
    "##\n",
    "\n",
    "x = le.fit_transform( dfClusters.Region )\n",
    "dfClusters[ 'Region' ] = x\n",
    "\n",
    "display( dfClusters.head().style.set_caption( 'dfClusters: Character Labels as Numerics' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( formatting ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering Approach\n",
    "\n",
    "With hierarchical clustering each object (customer in this case) starts as its own cluster/group (singletons).  Then those clusters are \"joined\" based on how similar (close) they are:\n",
    "\n",
    "* \"closeness\" is defined by a distance metric (default: Euclidean Distance), but there are others.  \n",
    "  * \"Manhattan Distance\" is a particularly interesting one.  \n",
    "* the distance is measured based on how you want to link the clusters.  This is called Linkage\n",
    "  * the center (called the \"centroid\") of the clusters\n",
    "  * the \"average\" linkage distance for the cluster\n",
    "  * \"Ward's minimum variance linkage\" (default)\n",
    "  * maximum, weighted, median linkage, etc.  \n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "* Data Engineering\n",
    "  * rescale your data (we did that above)\n",
    "  * standardization is probably the best place to start\n",
    "* Select the Metric\n",
    "  * Start with Euclidean but consider experimenting\n",
    "* Select the Linkage\n",
    "  * Start with Ward's but experiment\n",
    "* Cluster\n",
    "  * do the clustering, analyze, and experiment\n",
    "\n",
    "> There is a lot of experimentation and _Design Thinking_ that goes into this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ward = shc.linkage( dfClusters, method = 'ward' )\n",
    "\n",
    "## Plot a dendogram (fancy word for upside-down tree)\n",
    "## WARNING: this will take a minute\n",
    "\n",
    "## this would be something to experiment with. In the interest of time \n",
    "## I've done this for you, but feel free to experiment, which is what you \n",
    "## would do in the real world.  \n",
    "max_dist = 23\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 10, 7  ) )  \n",
    "plt.title( 'CID Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Customer (CID)' )\n",
    "plt.ylabel( 'Distance' )\n",
    "plt.text( 2500, 23.5, 'Cut-off Line' )\n",
    "##\n",
    "shc.dendrogram( ward )\n",
    "plt.axhline( y = max_dist, c = 'black', ls = '-', lw = 1.5 );\n",
    "##\n",
    "## Document dendrogram\n",
    "##\n",
    "rec_lst = [ [ (20, 14 ), 500, 4 ], [ (800, 17 ), 1200, 3 ], [ (3600, 17 ), 1350, 4 ],\n",
    "          [ (5900, 18 ), 1600, 4 ]  ]\n",
    "txt_lst = [ 100, 1100, 4000, 6300 ]\n",
    "for i in range( len( rec_lst ) ):\n",
    "    x = rec_lst[ i ]\n",
    "    rectangle = plt.Rectangle( x[ 0 ], x[ 1 ], x[ 2 ], fill = None, ec = \"black\", lw = 3 )\n",
    "    plt.gca().add_patch(rectangle)\n",
    "    plt.text( txt_lst[ i ], 23.5, 'Cluster ' + str( i + 1 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this showing me?  \n",
    "\n",
    "A horizontal line is drawn at a distance of 23 (totally arbitrary...but it gives me 4 clusters...which feels about right to me).  Any cluster formed below this line is a group.  Also notice that there are four groups.    \n",
    "\n",
    ">>What is the right number of clusters?  Dunno.  It's _Prunes Analysis_.  But maybe 4-6 is a good place to start if you don't have _a priori_ knowledge.  \n",
    "\n",
    "Here we use _fcluster_ to _flatten the cluster_ so we get the number of clusters we want.  Now we can look at the individual customers in the clusters and do some _Design Thinking_ with our marketing folks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Identify the CIDs in each cluster\n",
    "## Consider any cluster grouping formed below 23\n",
    "##\n",
    "cluster_labels = fcluster( ward, max_dist, criterion = 'distance' )\n",
    "df_hclusters[ 'Cluster_Number' ] = cluster_labels\n",
    "\n",
    "display( df_hclusters.head().style.set_caption( 'DataFrame with Cluster Assignment Number' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_hclusters[ 'Cluster' ] = [ 'Cluster ' + str( x ) for x in df_hclusters.Cluster_Number ]\n",
    "display( df_hclusters.stb.freq( [ 'Cluster' ] ).style.set_caption( 'Cluster Distribution' ).\\\n",
    "    set_table_styles( tbl_styles ).\\\n",
    "    bar( subset = [ 'count' ], align='mid', color = 'red').hide_index().\\\n",
    "        format( {'percent':'{0:.4}%', 'cumulative_percent':'{0:.4}%'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a boxplot for each cluster for Order Discount\n",
    "##\n",
    "ax = sns.boxplot( x = 'Cluster_Number', y = 'meanOdisc', data = df_hclusters )\n",
    "ax.set_title( 'Order Discount\\nby Clusters\\nHierarchical Clustering', fontsize = font_title )\n",
    "ax.set( xlabel = 'Clusters', ylabel = 'Order Discount' )\n",
    "base = 'Base: All data; n = ' + str( df_hclusters.shape[ 0 ] )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>Exercises</center></h1></strong>\n",
    "    \n",
    "[Back to Contents](#Contents)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II.1\n",
    "this might be good for a second example...\n",
    "\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Import a *CSV* data set of bank customers.  The *CSV* file is named *BankChurners.csv*.  HINT: Use *pd.read_csv*.  Call the imported DataFrame *df_bank*.   \n",
    "\n",
    "| Variable  | Values  | Source  | Mnemonic |\n",
    "|-----------|---------|---------|----------|\n",
    "| Customer ID | Unique identifier | Bank | CID |\n",
    "| Attrition Flag | String: Existing Customer, Attrited Customer | Bank | Attrition_Flag | \n",
    "| Customer Age | Integer | Bank | Age | \n",
    "| Customer Gender | Single Character: F = Female, M = Male | Bank | Gender |\n",
    "| Number of Household Dependents | Interger: 0, 1, 2, ... | Bank | Dependent_count |\n",
    "| Education Level | String | Bank | Education_Level |\n",
    "| Marital Status | String | Bank | Marital Status |\n",
    "| Income Category | String | Bank | Income_Category |\n",
    "| Type of Bank Card | String | Bank | Card_Category |\n",
    "| Months as Customer | Integer | Bank | Months_on_Book |\n",
    "| Total Number of Products Held by Customer | Integer | Bank | Total_Relationship_Count |\n",
    "| No. of Months Inactive in Last 12 Months | Integer | Bank | Months_Inactive_12_mon |\n",
    "| No. of Contacts in Last 12 Months | Interger: 0, 1, 2, ... | Bank | Contacts_Count_12_mon |\n",
    "| Credit Limit on the Credit Card | Integer | Bank | Credit_Limit |\n",
    "| Total Revolving Balance on the Credit Card | Integer | Bank | Total_Revolving_Bal |\n",
    "| Open to Buy Credit Line (Average of last 12 months) | Integer | Bank | Avg_Open_To_Buy |\n",
    "| Change in Transaction Amount (Q4 over Q1) | Float | Bank | Total_Amt_Chng_Q4_Q1 |\n",
    "| Total Transaction Amount (Last 12 months) | Integer | Bank |\n",
    "| Total Transaction Count (Last 12 months) | Integer | Bank | Total_Trans_Amt |\n",
    "| Change in Transaction Count (Q4 over Q1) | Float | Bank | Total_Ct_Chng_Q4_Q1 |\n",
    "| Average Card Utilization Ratio | Float | Bank | Avg_Utilization_Ratio |\n",
    "\n",
    "Use the following demographic variables to create a Hierarchical Cluster of the bank customers:\n",
    "\n",
    ">- Age\n",
    ">- Gender\n",
    ">- Card_Category\n",
    "\n",
    "This problem is a little tricky.  First, I recommend subsetting the three variables into a temporary DataFrame: *tmp*.  Second, *Gender* and *Card_Category* are character strings so they have to be recoded.  You can use a list comprehension to recode both.  Suggestions:\n",
    ">- tmp[ 'Gender' ] = [ 1 if x == 'M' else 0 for x in tmp.Gender ]\n",
    ">- tmp[ 'Card_Category' ] = [ 1 if x == 'Blue' else 2 if x == 'Silver' else 3 if x == 'Gold' else 4 for x in tmp.Card_Category ]\n",
    "\n",
    "Finally, the DataFrame is very large, so I recommend taking a random sample of $n = 500$.  You can use:\n",
    "\n",
    ">- smpl = tmp.sample( n = 500, random_state = 42, replace = False )\n",
    "\n",
    "[See Solution](#Solution-II.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering Approach\n",
    "\n",
    "\n",
    "K-Means is another popular clustering method.  With K-means you have to declare the number of clusters, known as _k_.  `k=4` means you want 4 clusters based on your numeric features:  Sales, Pocket Price, and the four Discounts.\n",
    "\n",
    "Process:\n",
    "\n",
    "* declare k (the seed, or initial, clusters)\n",
    "* group objects based on their shortest distance from the seeds\n",
    "* create new seeds as the _mean_ (or _centroid_) of the groups.  \n",
    "* merge an object (customer) into each group based on the shortest distance to the centroid\n",
    "* repeat until all customers are assigned to the _k_ groups.  \n",
    "\n",
    ">>When would you use this?  When you think you know the number of groups you want _a priori_.  But again, you can always experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set up data \n",
    "##\n",
    "cols = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_kclusters = df_agg[ cols ].copy()\n",
    "##\n",
    "display( df_kclusters.head().style.set_caption( 'K-Means Clustering Data' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the data for all numerics.  We remove the region, it doesn't appear to be needed.  \n",
    "##\n",
    "cols = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "tmp = df_kclusters[ cols ]\n",
    "##\n",
    "## Do K-Means\n",
    "## why 42?  We need something to make it repeatable.  \n",
    "\n",
    "kmeans = KMeans( n_clusters = 4, random_state = 42 ).fit( tmp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The kmeans centers are retrieved using the method *cluster_centers_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Add cluster labels to main cluster DataFrame\n",
    "##\n",
    "df_kclusters[ 'Cluster_Number' ] = kmeans.labels_   ## Notice the ending underscore\n",
    "##\n",
    "display( df_kclusters.head().style.set_caption( 'DataFrame with K-Means Cluster Assignments' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The kmeans cluster numbers are retrieved using the method *labels_*.  Notice the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_kclusters[ 'Cluster' ] = [ 'Cluster ' + str( x ) for x in df_kclusters.Cluster_Number ]\n",
    "display( df_kclusters.stb.freq( [ 'Cluster' ] ).style.set_caption( 'Cluster Distribution' ).\\\n",
    "    set_table_styles( tbl_styles ).\\\n",
    "    bar( subset = [ 'count' ], align='mid', color = 'red').hide_index().format( {'percent':'{0:.4}%', 'cumulative_percent':'{0:.4}%'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a boxplot for each cluster for Order Discount\n",
    "##\n",
    "ax = sns.boxplot( x = 'Cluster_Number', y = 'meanOdisc', data = df_kclusters )\n",
    "ax.set_title( 'Order Discount\\nby Clusters\\nK-Means Clustering', fontsize = font_title )\n",
    "ax.set( xlabel = 'Clusters', ylabel = 'Order Discount' )\n",
    "base = 'Base: All data; n = ' + str( df_hclusters.shape[ 0 ] )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now do some _Design Thinking_ among our analysts and sales teams.  Are these the _right_ clusters for what we want to accomplish?  This is worth a discussion and exploration.   More on this next...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Use the bank DataFrame to do a K-Means clustering.  Use the following numeric variables:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Use a random sample of $n = 500$.\n",
    "\n",
    "[See Solution](#Solution-II.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with these clusters?\n",
    "\n",
    "_Design Thinking_ with your marketing people.  \n",
    "\n",
    "* _What do we do next_ is a key tenant of `Prescriptive Analytics` and this information is not something easily conveyed on a Power BI dashboard (imo).  \n",
    "* Do some _profiling_.  What are these clusters telling us?  Maybe look at some sample customers from a \"Customer 360\" perspective.  What are their common traits?  \n",
    "* Now you can name the clusters/profiles.  Without a good, descriptive, meaningful, generic name that we can agree on, your clusters are meaningless. \n",
    "\n",
    "There is an art to naming.  Imagine you are a clothing retailer.  Some clusters might be: \n",
    "   * Fashion Aware\n",
    "   * Balanced Buyer\n",
    "   * Indulgent Expressives\n",
    "   * Fashionista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Analytics\n",
    "\n",
    "_Churn_ is the loss of customers, their revenue, and higher CAC (customer acquisition costs) to replace them.  \n",
    "\n",
    "Why do customers churn?\n",
    "\n",
    "* price is too high\n",
    "* poor product quality\n",
    "* out-of-fashion products\n",
    "* poor customer support\n",
    "* poor online reviews\n",
    "* etc etc etc\n",
    "\n",
    "What we want to do is reduce churn by identifying/predicting potential churners and taking proactice steps to prevent or minimize churn.  \n",
    "\n",
    "_There may be times when we WANT a customer to churn_  \n",
    "\n",
    "Some churn is inevitable.  But we should be able to minimize it.  \n",
    "\n",
    "Most companies do basic _descriptive analytics_ on churn:\n",
    "* what is the rate-of-churn\n",
    "* track lost revenue\n",
    "* identify and track CAC\n",
    "\n",
    "Some companies do _predictive analytics_:\n",
    "* who are the most likely to churn?\n",
    "\n",
    "Very few companies do _prescriptive analytics_:\n",
    "* What do we do to minimize churn?\n",
    "* How can we measure if our interventions are working?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Case Study - Telecom\n",
    "\n",
    "> I worked for `Comcast Cellular One` out of college (1997-ish) doing churn analytics.  I wasn't good at it and only lasted a few months.  The reason was simple...I could do the math but I couldn't do the _Prescriptive Analytics_.  This is very conversational between business domain experts and data professionals.  \n",
    "\n",
    "The telecom industry was a monopoly until 1984 with the divestiture of AT&T.  Never before had customers \"churned\" from one telecom provider to another.  Suddenly, churn analytics was all the rage.  \n",
    "\n",
    "Let's assume you are a telecom company with a historical database of customer metrics including:\n",
    "* their telecom \"usage\"\n",
    "* subscription services (internet, bundled products, online backup, security products)\n",
    "* month-by-month subscriber vs annual contract\n",
    "* an indicator as to whether the customer churned\n",
    "\n",
    "The CEO was looking at a Power BI dashboard and noticed that 1 out of 4 customers leaves after some period of time.  (_Descriptive Analytics_).  This is a big problem when the numbers need to be reported to Wall Street.  The CAC in this industry is also VERY high.  \n",
    "\n",
    "ASK:  Build a model to predict likely churns (_Predictive Analytics_) so that we can work with marketing and the domain experts to determine what some good strategies would be to _minimize_ churn.  (_Prescriptive Analytics_ or _what do we do next?_)\n",
    "\n",
    "### General Process\n",
    "\n",
    "* We need a data set of past customers, with their histories, and a _label_ as to whether they churned.  **This isn't easy and will likely take some time**.  \n",
    "* Once we talk through the _features_ that are needed we'll have a trained _model_ that will tell us the probability that a customer will churn.  \n",
    "* We want to run that against _current_ customers to identify the potential churners so we can determine how to prevent it. (the _treatment_)\n",
    "* BONUS:  Really advanced companies will constantly \"score\" customers whenever something about those customers changes, in real-time.  This allows those companies to make proactive _treatments_ vs being reactive.  This takes time and requires a lot of Prescriptive Analysis.  \n",
    "\n",
    "\n",
    "But first..._why_ are they churning?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Data Dictionary\n",
    "\n",
    "\n",
    "| Variable | Values | Source | Column Name |\n",
    "|----------|--------|--------|----------|\n",
    "| Customer ID | AlphaNumeric | IT | CID |\n",
    "| Subscriber Gender | String: Male/Female | IT | gender |\n",
    "| Senior Citizen Status | String: Yes/No | IT | seniorCitizen |\n",
    "| Subscriber has Partner | String: Yes/No | IT | partner |\n",
    "| Dependents in Household | String: Yes/No | IT | dependents |\n",
    "| Tenure as Customer | Numeric: Months | IT | tenure |\n",
    "| Have Phone Service | String: Yes/No | IT | phoneService |\n",
    "| Have Multiple Phone Lines | String: Yes/No/No phone service | IT | multipleLines |\n",
    "| Subscribe to Internet Service | String: No/Fiber Opic/DSL | IT | internetService |\n",
    "| Subscribe to  Online Security | String: Yes/No/No Internet Service | IT | onlineSecurity |\n",
    "| Subscribe to  Online Computer Backup | String: Yes/No/No Internet Service | IT | onlineBackup |\n",
    "| Subscribe to  Device Protection Plan | String: Yes/No/No Internet Service | IT | deviceProtection |\n",
    "| Subscribe to Tech Support | String: Yes/No/No Internet Service | IT | techSupport |\n",
    "| Subscribe to Streaming TV | String: Yes/No/No Internet Service | IT | streamingTV |\n",
    "| Subscribe to Streaming Movies | String: Yes/No/No Internet Service | IT | streamingMovies |\n",
    "| Contract Type | String: Month-to-Month/One year/Two year | IT |contractType |\n",
    "| Subscribe to Paperless Bill | String: Yes/No | IT | paperlessBilling |\n",
    "| Payment Method | String: Bank transfer (automatic)/Credit card (automatic)/Electronic check/Mailed check | Billing | paymentMethod |\n",
    "| Average Monthly Charges | Numeric: Dollars and Cents | Billing | monthlyCharges |\n",
    "| Total Charges Over Tenure Time | Numeric: Dollars and Cents | Billing | totalCharges |\n",
    "| Churned at any Time | String: Yes/No | Marketing | churn |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, Examine, and Process the Churn Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set file name\n",
    "##\n",
    "file = 'churn.xlsx'\n",
    "df_churn = pd.read_excel( path + file )\n",
    "##\n",
    "## Record churn from Yes/No to 1/0\n",
    "##\n",
    "df_churn[ 'Churn' ] = [ 1 if x == 'Yes' else 0 for x in df_churn.churn ]\n",
    "##\n",
    "## Display\n",
    "##\n",
    "display( df_churn.head().style.set_caption( 'Churn Data' ).set_table_styles( tbl_styles ).hide_index().\\\n",
    "       format( {'monthlyCharges':'${0:.2f}', 'totalCharges':'${0:,.2f}'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size( df_churn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_check( df_churn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvReport( df_churn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "There are 11 cases with missing values for Total Charges.  There are several options to handle these cases:\n",
    "\n",
    ">1. Drop the 11 cases.\n",
    ">2. Replace with the mean Total Charges.\n",
    ">3. Replace with the mean Total Charges of groups (e.g., Male, Senior Citizens, No Multiple Lines).\n",
    ">4. Interpolate.\n",
    ">5. Estimate an *OLS* model for Total Charges and predict the missing values.\n",
    "\n",
    "See <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#filling-missing-values-fillna\" target=\"_parent\">here</a> for methods in Pandas for handling missing values.\n",
    "\n",
    "I will delete the 11 cases for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop records with NA values.\n",
    "##\n",
    "df_churn.dropna( axis = 0, inplace = True )\n",
    "df_size( df_churn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset data\n",
    "##\n",
    "data = df_churn[ 'churn' ].value_counts( normalize = True ).round( 3 )\n",
    "data = pd.DataFrame( data )\n",
    "base = 'Base: n = ' + str( df_churn.shape[ 0 ] )\n",
    "display( data.style.set_caption( 'Churn Distribution' ).\\\n",
    "    bar( align = 'mid', color = 'red' ).format( '{:.1%}' ).\\\n",
    "    set_table_styles( tbl_styles ) )\n",
    "print( base )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot tenure histogram\n",
    "##\n",
    "base = 'Base: All records; n = ' + str( df_churn.shape[ 0 ] )\n",
    "ax = sns.distplot( df_churn.tenure )\n",
    "ax.set_title( 'Tenure Distribution', fontsize = font_title )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The distribution is bimodal.  Recommendation: recode.\n",
    "\n",
    "ie, we have a lot of new customers and a lot of old customers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot monthly charges histogram\n",
    "##\n",
    "ax = sns.distplot( df_churn.monthlyCharges )\n",
    "ax.set_title( 'Monthly Charges Distribution', fontsize = font_title )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The distribution is trimodal.  Recommendation: recode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Recode the two variables: tenure and monthly charges\n",
    "##\n",
    "df_churn[ 'tenureRecoded' ] = [ '<30' if x < 30 else '>=30' for x in df_churn.tenure ]\n",
    "df_churn[ 'monthlyChargesRecoded' ] = [ '<30' if x < 30 else '>= 70' if x >= 70 \n",
    "                                       else '30 - 70' for x in df_churn.monthlyCharges ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split the Churn Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use 75% train\n",
    "##\n",
    "churn_train, churn_test = train_test_split( df_churn, train_size = 0.75, random_state = 42 )\n",
    "display( df_size( churn_train ) )\n",
    "display( df_size( churn_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>Exercises</center></h1></strong>\n",
    "    \n",
    "[Back to Contents](#Contents)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Split the Bank DataFrame into train and testing data sets using $\\frac{3}{4}$ for training.\n",
    "\n",
    "[See Solution](#Solution-III.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>End Exercises</center></h1></strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Churn Prediction Model\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Select columns for training\n",
    "##\n",
    "cols = [ 'Churn', 'gender', 'tenureRecoded', 'contractType', 'monthlyChargesRecoded' ]\n",
    "display( churn_train[ cols ].head().style.set_caption( 'Subsetted Training Data' ).set_table_styles( tbl_styles ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we code a Churn Prediction Model\n",
    "\n",
    "This is merely ONE way to do it.  We use _logistic regression_ by defining a formula for our churn.  \n",
    "\n",
    "Here's the formula.  This notation is often called a `patsy model`:\n",
    "\n",
    "```\n",
    "Churn ~ C( gender, Sum ) + C( tenureRecoded, Sum ) + C( contractType, Sum ) + C( monthlyChargesRecoded, Sum )\n",
    "```\n",
    "\n",
    "Huh?  \n",
    "\n",
    "Let's decode it:\n",
    "\n",
    "* `Churn ~` :  think of it as \"churn equals...\"\n",
    "* `C( gender, Sum )` :  gender has 2 possible values:  `Male` and `Female`.  Algorithms don't like to work on text and we know that gender is `categorical`.  `C` means `convert to categorical`.  `Sum` is a complicated way in econometrics to say, \"every coefficient must sum to zero\",.  In this case it just means `-1` is `female` and `1` is `male`.  We are _categorically-encoding a string value_\n",
    "* do the same thing for `tenureRecoded`, `contratType`, and `monthlyChargesRecoded`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'Churn ~ C( gender, Sum ) + C( tenureRecoded, Sum ) + C( contractType, Sum ) + C( monthlyChargesRecoded, Sum )'\n",
    "\n",
    "\n",
    "##  \"instantiate\" a model\n",
    "##  This is logistic regression\n",
    "mod = smf.logit( formula, data = churn_train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "display( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "* We are trying to maximize the `Log-Likelihood`.  \n",
    "* In the process it generates a \"pseudo\" `R-squ`.  This is too much to explain here but `0.2314` simply can be interpreted as _23% of the variation in my target is accounted for in my model_.  That's not great, but it's a start.  \n",
    "* `C(gender,Sum)[S.Female]` simply means that the coefficient for females is `0.0197`.  Since we used `Sum` we know that for males it is `-0.0197`.  \n",
    "* the same can be said for `tenureRecoded` (below or above 30 months)\n",
    "* `ContractType` has 3 categories but we only see two in the output.  To calculate the \"two year\" contract we simply add the two categories displayed and multiply by -1.  (So, `-1.26`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>Exercises</center></h1></strong>\n",
    "    \n",
    "[Back to Contents](#Contents)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Train a bank attrition (i.e., churn) model.  The dependent variable is *Attrition_Flag*.  Recode this as 1 if the account is closed; else 0.  Use a list comprehension for this.  Use the following numeric variables for the independent variable or *features*:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Call the fitted model *bank_logit01*. \n",
    "\n",
    "[See Solution](#Solution-III.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>End Exercises</center></h1></strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 154:00\n",
    "\n",
    "Now, what can I do with all of that information that the model is giving me?  \n",
    "\n",
    "## Odds Analysis\n",
    "\n",
    "_Warning...this is a little math-y...skip to the next section if you are ok just trusting me on the math:_\n",
    "\n",
    "\n",
    "You can calculate the odds of churning by _exponentiating the estimated coefficients_ given to us above.\n",
    "\n",
    "The odds of something happening are calculated as \n",
    "\n",
    "$\\frac{p}{1 - p}$  \n",
    "\n",
    "So, the odds for the binary case of Gender:   \n",
    "\n",
    "$Females = e^{\\beta_1}$   \n",
    "\n",
    "after some algebra and recognizing that the $\\beta_0$ term cancels in the numerator and denominator.  The odds for $Males = e^{-\\beta_1}$.  Therefore, the odds ratio for Females to Males is $\\frac{e^{\\beta_1}}{e^{-\\beta_1}} = e^{2 \\times \\beta_1}$.  Conversely, the odds ratio for Males to Females is $\\frac{e^{-\\beta_1}}{e^{\\beta_1}} = e^{-2 \\times \\beta_1}$, or just the inverse.\n",
    "\n",
    "For the trinary case, the odds are calculated the same way, but recognize that the odds for the base are given by $e^{-(\\beta_1 + \\beta_2)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the odds and odds ratio for Females-Males\n",
    "\n",
    "\n",
    "boldprt( 'Gender Odds Ratios\\n' )\n",
    "## exp = exponentiation\n",
    "odds_female = math.exp( logit01.params[ 1 ] )\n",
    "print( f'Female Odds: {odds_female:.3f}' )\n",
    "##\n",
    "odds_male = math.exp( -logit01.params[ 1 ] )\n",
    "print( f'Male Odds: {odds_male:.3f}' )\n",
    "##\n",
    "odds_ratio = odds_female/odds_male\n",
    "boldprt( '='*40 )\n",
    "print( f'Odds Ratio Females to Males: {odds_ratio:.3f}' )\n",
    "print( f'Odds Ratio Males to Females: {1/odds_ratio:.3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The odds are about even for males and females.  They are about equally likely to churn so gender is **not** a factor in churn.  \n",
    "\n",
    "See?  It's pretty simple.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the odds and odds ratio for contract\n",
    "##    Contract is #3 and #4 in the parameter list\n",
    "\n",
    "boldprt( 'Contract Odds Ratios\\n' )\n",
    "odds_month = math.exp( logit01.params[ 3 ] )\n",
    "print( f'Monthly Odds: {odds_month:.3f}' )\n",
    "\n",
    "odds_1Yr = math.exp( logit01.params[ 4 ] )\n",
    "print( f'1 Yr Odds: {odds_1Yr:.3f}' )\n",
    "\n",
    "odds_2Yr = math.exp( -( logit01.params[ 3 ] + logit01.params[ 4 ] ) )\n",
    "print( f'2 Yr Odds: {odds_2Yr:.3f}' )\n",
    "\n",
    "odds_ratio_Month_1Yr = odds_month/odds_1Yr\n",
    "odds_ratio_Month_2Yr = odds_month/odds_2Yr\n",
    "odds_ratio_1Yr_2Yr = odds_1Yr/odds_2Yr\n",
    "print( '='*40 )\n",
    "print( f'Odds Ratio Month to 1 Yr: {odds_ratio_Month_1Yr:.3f}' )\n",
    "print( f'Odds Ratio Month to 2 Yr: {odds_ratio_Month_2Yr:.3f}' )\n",
    "print( f'Odds Ratio 1 Yr to 2 Yr: {odds_ratio_1Yr_2Yr:.3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "`Odds Ratio Month to 1 Yr: 4.312` :  this means that the month-to-month subscribers are **4x** more likely to churn than the 1-Year-Contract Subscribers.  \n",
    "\n",
    "The odds or \"likelihood\" of someone churning if they were a month-to-month subscriber is almost **14x higher** than if they were on a two-year contract.  The two-year contract subscribers are more loyal and committed. This makes sense. \n",
    "\n",
    "**What do we do next?**\n",
    "\n",
    "* If we assume CAC is high and churn minimization is desired then we might want to develop retention programs that **target the conversion of month-to-month users to longer term contracts.**\n",
    "* it is less urgent to develop programs to retain longer-term contract customers.  \n",
    "* we should likely develop programs to target customers near the end of their contract term.  \n",
    "\n",
    "_This is all just starting to scratch the surface of what we can do._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics Outcome\n",
    "\n",
    "This PROCESS is a great way to confirm hypotheses that our marketing teams and execs may _think_ is the cause for churn.  VERY INTERESTING conversations and _Design Thinking_ sessions occur during this time.  \n",
    "\n",
    "Let's quickly generate all of the odds ratios:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a DataFrame of estimates and drop first row (the intercept).\n",
    "## we print this out solely to see what is happening\n",
    "\n",
    "df_odds = pd.DataFrame( logit01.params, columns = [ 'Estimates' ] ).reset_index().iloc[1: , :]\n",
    "df_odds[ 'Group' ] = df_odds[ 'index' ]\n",
    "df_odds.Group = df_odds.Group.apply( lambda st: st[ st.find( \"C(\" ) + 2:st.find( \",\" ) ] )\n",
    "aggregation = {'Estimates':'sum'}\n",
    "grp = df_odds.groupby( by = [ 'Group' ] ).agg( aggregation )*( -1 )\n",
    "print(grp)\n",
    "display( df_odds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's print out the dataframe again in a manner that's a little easier to understand\n",
    "\n",
    "df_odds = df_odds.merge( grp, left_on = 'Group', right_on = 'Group' )\n",
    "df_odds.rename( columns = { \"Estimates_y\": \"Base\" }, inplace = True )\n",
    "df_odds[ 'Odds_Ratio' ] = np.exp(df_odds.Estimates_x)/np.exp(df_odds.Base)\n",
    "##\n",
    "display( df_odds.style.set_caption( 'Odds Ratio Data -- Relative to Base' ).set_table_styles( tbl_styles ).\\\n",
    "        format( { 'Odds_Ratio':'{:,.1f}'} ).hide_index() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "* `Base` is the base value of the group vs its comparison (the `Index`)\n",
    "  * the `Base` for contractType is `2 year` vs the `Index` which is either `Month-to-month` or `One year`\n",
    "  * Example from above:  `Versus the Base of Two Year contract, a month-to-month contract is 13.8x more likely to churn`\n",
    "* this can be a little confusing at first.  There's probably a better way to display this data but I'm not sure what it is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot odds ratios\n",
    "##\n",
    "ax = df_odds.plot( x = 'index', y = 'Odds_Ratio',  kind = 'barh' )\n",
    "ax.set_title( 'Odds Ratios', fontsize = font_title )\n",
    "ax.set_xlabel( 'Odds Ratio' )\n",
    "ax.set_ylabel( '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Churn with the Model\n",
    "\n",
    "At some point we'll have a trained model that is predicting churn that we can understand.  We now want to do `inferencing` against live data (or in this case we want to do `validation` of the model using the hold-out dataset from above: `churn-test`).  \n",
    "\n",
    "The prediction process is simple: use the `predict` function instead of `fit`.  \n",
    "\n",
    "Note:  the model will give us _probability_ of churn, so we specify a cut-off threshold: $\\theta$ (theta).  A probability > $\\theta$ is coded as `1` or `True` or `Churn`; `0` or `False` or `Not Churn`, otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Make predictions\n",
    "## Use churn_test for this example\n",
    "\n",
    "theta = 0.5\n",
    "\n",
    "prediction = logit01.predict( churn_test )\n",
    "\n",
    "## let's make this easier to read\n",
    "\n",
    "classification = [ 1 if x > theta else 0 for x in prediction ]\n",
    "data = { 'probability':prediction, 'classification':classification }\n",
    "tmp = pd.DataFrame( data )\n",
    "tmp[ 'Churn' ] = [ 'Churn' if x == 1 else 'Not Churn' for x in tmp.classification ]\n",
    "display( tmp.head().style.set_caption( 'Predictions from Logit Model' ).set_table_styles( tbl_styles ).format( {'probability':p_value } ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "* the first col is the `Customer ID`\n",
    "* the `probability` is coming out of the model and is sometimes useful when you are exploring whethere someone is _close_ to churning \n",
    "* We might want to change `theta` and explore what happens\n",
    "\n",
    "### What-If Analysis\n",
    "\n",
    "Predict churn for different settings of the variables.  This is similar to `what-if` analysis in Excel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Specify scenario values to use for prediction\n",
    "## we can do a lot of experimentation by adjusting the variables\n",
    "## Let's just look at this case:  Men with new 1 year contracts and at least $70 of monthly spend\n",
    "\n",
    "data = {\n",
    "         'gender': [ 'Male' ],\n",
    "         'tenureRecoded': [ '<30' ],\n",
    "         'contractType': [ 'One year' ],\n",
    "         'monthlyChargesRecoded': [ '>= 70' ]\n",
    "        }\n",
    "\n",
    "## Create a DataFrame using the dictionary\n",
    "churn_scenario = pd.DataFrame.from_dict( data )\n",
    "\n",
    "## Display the settings and the predicted unit sales\n",
    "display( churn_scenario.style.set_caption( 'Scenario Settings' ).set_table_styles( tbl_styles ).hide_index() )\n",
    "##\n",
    "## Create a prediction\n",
    "##\n",
    "theta = 0.5\n",
    "##\n",
    "prediction = logit01.predict( churn_scenario )\n",
    "classification = [ 'Churn' if x > theta else 'Not Churn' for x in prediction ]\n",
    "data = { 'probability':prediction, 'classification':classification }\n",
    "tmp = pd.DataFrame( data )\n",
    "display( tmp.style.set_caption( 'Scenario Prediction' ).set_table_styles( tbl_styles ).hide_index().format( {'probability':p_value } ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Men with **new** 1 year contracts and at least $70 of monthly spend likely will NOT churn.  \n",
    "\n",
    "Let's look at this in more detail:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make predictions\n",
    "\n",
    "theta = 0.5\n",
    "\n",
    "y_test = churn_test[ 'Churn' ]\n",
    "prediction = logit01.predict( churn_test )\n",
    "classification = [ 1 if x > theta else 0 for x in prediction ]\n",
    "x = classification_report( y_test, classification, digits = 3 )\n",
    "boldprt( f'Logit Model Classification Report:\\n{x}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "* The `accuracy` is 78%.  This means the model accurately predicted churn 78% of the time.  \n",
    "\n",
    "\n",
    "Some definitions:  \n",
    "\n",
    "* `precision` is the ratio $tp/(tp + fp)$ where $tp$ is the number of true positives and $fp$ the number of false positives. _The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative._\n",
    "* `recall` is the ratio $tp/(tp + fn)$ where $tp$ is the number of true positives and $fn$ the number of false negatives. _The recall is intuitively the ability of the classifier to find all the positive samples._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*.\n",
    "\n",
    "Sometimes it is **CRITICAL** that we optimize for ONE of these metrics.  \n",
    "\n",
    "Here's an example:  If I am trying to diagnose cancer it _might_ be ok to have some **false positives** but it is absolutely UNACCEPTABLE if our model allows **false negatives** (ie, the patient had cancer and we didn't detect it).  \n",
    "\n",
    "Sometimes it's interesting to have conversations around _why_ we see certain results in our models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "x = confusion_matrix(y_test, classification).ravel()\n",
    "##\n",
    "lbl = [ 'True Negative', 'False Positive', 'False Negative', 'True Positive' ]\n",
    "##\n",
    "## Display the confusion matrix in a DataFrame\n",
    "##\n",
    "df_confusion = pd.DataFrame( x, columns = [ 'Value' ], index = lbl )\n",
    "df_confusion[ 'Percent (%)' ] = df_confusion.Value/df_confusion.Value.sum()\n",
    "##\n",
    "display( df_confusion.style.format( { 'Percent (%)': '{:.1%}' } ).highlight_max( color = 'yellow' ).\\\n",
    "    set_caption( 'Confusion Matrix Summary' ).set_table_styles( tbl_styles ).format( {'Value':'{0:,.0f}' } ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There was 1149 true negatives, 151 false positives, 238 false negatives, and 220 true positives.  The total is 1758 which is the size of the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the confusion values\n",
    "##\n",
    "ax = sns.barplot( y = df_confusion.index, x = df_confusion[ 'Percent (%)' ] )\n",
    "ax.set_title( 'Percent of Sample\\nby Confusion Labels', fontsize = font_title )\n",
    "ax.set( xlabel = 'Percent Confusion', ylabel = '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative plot of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['Not Likley to Churn', 'Likely to Churn']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, classification )\n",
    "df_cm = pd.DataFrame( data = cm/cm.sum(), index = lbl, columns = lbl )\n",
    "##\n",
    "display( df_cm.style.format( { 'Not Likley to Churn': '{:.1%}', 'Likely to Churn': '{:.1%}' } ).highlight_max( color = 'yellow' ).\\\n",
    "    set_caption( 'Churn Confusion Matrix' ).set_table_styles( tbl_styles ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "78% (= 65.9% + 11.9% $\\approx$ 78% ) of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a Churn Database\n",
    "\n",
    "You can score the entire database with the model results.  If you are satisfied with the model (i.e., passes statistical checks and test analysis), then apply the model to the entire database.\n",
    "\n",
    "We can also think about `real-time inferencing` so we can apply a treatment proactively vs reactively.  \n",
    "\n",
    "\n",
    "### What do we do next?  \n",
    "\n",
    "We don't just want a scored database of possible churners.  If we know what the `win back cost` is and we can build a formula for the `expected revenue loss` then we can quickly determine which churners we need to focus on RIGHT NOW.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the model to the database\n",
    "score = logit01.predict( df_churn )\n",
    "score = pd.DataFrame( logit01.predict( df_churn ), columns = [ 'Score'] )\n",
    "df_scored = pd.concat( [ df_churn, score ], axis = 1 ) ##, ignore_index = True )\n",
    "\n",
    "# this isn't a great way to calculated Expected Revenue Loss, but it's a good starting point\n",
    "# and leads to a great discussion\n",
    "# expectedLoss = total charges to date\n",
    "df_scored[ 'expectedLoss' ] = df_scored.totalCharges * df_scored.Score\n",
    "\n",
    "# Specify win-back cost\n",
    "# again, this is hard-coded but is a great conversation starter.  Perhaps CAC for this customer segment should be used here?\n",
    "win_back_cost = 350\n",
    "\n",
    "## Determine who to target: expected loss > win back cost\n",
    "\n",
    "df_scored[ 'Target' ] = [ 'Yes' if x > win_back_cost else 'No' for x in df_scored.expectedLoss ]\n",
    "df_scored.sort_values( by = [ 'expectedLoss', 'Target' ], ascending = False, inplace = True )\n",
    "cols = [ 'CID', 'expectedLoss', 'Target' ]\n",
    "display( df_scored[ cols ].head().style.set_caption( 'Win-Back Target' ).set_table_styles( tbl_styles ).\\\n",
    "       format( {'expectedLoss':'${0:,.2f}'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Distribution of Target variable\n",
    "##\n",
    "x = df_scored.Target.value_counts( normalize = True )\n",
    "pd.DataFrame( x ).style.set_caption( 'Distribution of Target' ).set_table_styles( tbl_styles ).format( {'Target':'{0:.1%}' } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "These are the customers where, if they churn, we lose more revenue than what it would cost to win them back or acquire a similar, new customer.  **About 40% of my customers should be targeted given the scenario above**.  \n",
    "\n",
    "\n",
    "## Here's what I hope you learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson IV: Introduction to Customer Lifetime Value\n",
    "\n",
    "\n",
    "This is based on https://github.com/hariharan2305/DailyKnowledge/blob/master/Customer%20Lifetime%20Value/Customer%20Lifetime%20Value%20(CLV).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '47' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '49' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '50' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '51' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '53' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '54' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '55' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '56' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLV Case Study\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '58' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLV Data Dictionary\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "**Retail Sales Data**\n",
    "\n",
    "| Variable | Values | Source | Mnemonic |\n",
    "|----------|--------|--------|----------|\n",
    "| Invoice Number | Nominal, a 6-digit integral number | UCI | InvoiceNo |\n",
    "| Product (item) code | Nominal, a 5-digit integral number | UCI | StockCode |\n",
    "| Product (item) name | String | UCI | Description |\n",
    "| Quantities of each product (item) per transaction | Numeric | UCI | Quantity |\n",
    "| Invoice Date and time | Numeric, day and time | UCI | InvoiceDate |\n",
    "| Unit price | Numeric, price per unit in sterling | UCI | UnitPrice |\n",
    "| Customer ID | Nominal, 5-digit integral number | UCI | CID |\n",
    "| Country name | Nominal, name of the country of customer | UCI | Country |\n",
    "\n",
    "Source: University of California Irvine Machine Learning Repository (*UCI*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, Examine, and Process the CLV Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the data\n",
    "##\n",
    "file = 'onlineRetail.csv'\n",
    "##\n",
    "format_dict = {'UnitPrice':'${0:.2f}', 'CID':'{0:.0f}' }\n",
    "df_retail = pd.read_csv( path + file, parse_dates = [ 'InvoiceDate' ] )\n",
    "df_retail.rename( columns = { 'CustomerID':'CID' }, inplace = True )\n",
    "display( df_retail.head().style.set_caption( 'Initial Data Download' ).set_table_styles( tbl_styles ).hide_index().\\\n",
    "       format( format_dict ) )\n",
    "display( df_size( df_retail ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Get country distribution\n",
    "##\n",
    "df_country = pd.DataFrame( df_retail.Country.value_counts( normalize = True ) )\n",
    "df_country.rename( columns = {'Country':'Proportion'}, inplace = True ) \n",
    "df_country.head().style.set_caption( 'Country Representation' ).set_table_styles( tbl_styles ).\\\n",
    "    format( { 'Proportion':'{:,.1%}'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset UK\n",
    "##\n",
    "df_uk = df_retail.query( \"Country == 'United Kingdom'\" ).drop( labels = [ 'Country' ], axis = 1 )\n",
    "display( df_uk.head().style.set_caption( 'Preprocessed Data' ).set_table_styles( tbl_styles ).\n",
    "        hide_index().format( format_dict ) )\n",
    "display( df_size( df_uk ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display summary statistics\n",
    "##\n",
    "display( df_uk.describe().T.style.set_caption( 'Summary Statistics' ).set_table_styles( tbl_styles ).format( p_value ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice the negative quantity and price points.  These represent returns and are not positive business.  I'll delete all records with negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop the records with negative data\n",
    "##\n",
    "df_uk = df_uk[ ( df_uk.Quantity > 0 ) & ( df_uk.UnitPrice > 0 ) ]\n",
    "display( df_uk.describe().T.style.set_caption( 'Summary Statistics' ).set_table_styles( tbl_styles ).format( p_value ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Check for missing values\n",
    "##\n",
    "mvReport( df_uk )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop all records with missing CIDs.  We need to know who the customers are so missing values are a problem.\n",
    "##\n",
    "df_uk.dropna( inplace = True )\n",
    "mvReport( df_uk )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate total revenue\n",
    "##\n",
    "format_dict.update( {'Revenue':'${0:.2f}'})\n",
    "df_uk[ 'Revenue' ] = df_uk[ 'Quantity' ] * df_uk[ 'UnitPrice' ]\n",
    "display( df_uk.head().style.set_caption( 'Retail Data' ).set_table_styles( tbl_styles ).\\\n",
    "        hide_index().format( format_dict ) )\n",
    "display( df_uk.describe().T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Print some DataFrame details\n",
    "##\n",
    "format_dict = {'total revenue':'${0:,.0f}', 'total quantity':'{0:,.0f}', 'unique customers':'{0:,.0f}'}\n",
    "data = { 'min Date':df_uk.InvoiceDate.dt.date.min(), 'max Date':df_uk.InvoiceDate.dt.date.max(),\n",
    "        'unique customers':df_uk.CID.nunique(), 'total quantity':df_uk.Quantity.sum(),\n",
    "        'total revenue':df_uk.Revenue.sum() }\n",
    "display( pd.DataFrame( data, index = [0] ).style.set_caption( 'Variable Summaries' ).\\\n",
    "        set_table_styles( tbl_styles ).hide_index().format( format_dict ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice that there are 3,920 unique customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CLV\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *lifetimes* package.\n",
    "\n",
    "Create a *Recency, Frequency and Monetary Value* (*RFM*) summary table from the transactions data.\n",
    "\n",
    "The *summary_data_from_transactions_data* function in *lifetimes* package aggregates transaction level data and calculates for each customer:\n",
    "\n",
    ">- **frequency**: the number of repeat purchases (more than 1 purchases).\n",
    ">- **recency**: the time between the first and the last transaction.\n",
    ">- **T**: the time between the first purchase and the end of the transaction period.\n",
    ">- **monetary_value**: it is the mean of a given customer's sales value (i.e., Revenue).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create summary data using summary_data_from_transaction_data function.\n",
    "##\n",
    "format_dict = { 'CID':'{0:.0f}', 'T':'{0:.0f}', 'frequency':'{0:.0f}', 'recency':'{0:.0f}', 'monetary_value':'${0:,.2f}'}\n",
    "summary = lifetimes.utils.summary_data_from_transaction_data( df_uk, 'CID', 'InvoiceDate', 'Revenue' )\n",
    "summary = summary.reset_index()\n",
    "base = summary.shape[ 0 ]\n",
    "display( summary.head().style.set_caption( 'RFM Summary Data' ).set_table_styles( tbl_styles ).format( format_dict ).hide_index() )\n",
    "boldprt( f'Base: {base} customers' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice that the monetary value is based on the calculated Revenue (unit sales $\\times$ price).  This is not net profit.\n",
    "\n",
    "The value 0 for frequency and recency means these are one-time buyers. Let's check how many such one-time buyers there are in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot distribution of frequency\n",
    "##\n",
    "ax = summary[ 'frequency' ].plot( kind = 'hist', bins = 50 )\n",
    "ax.set_title( 'Frequency Distribution', fontsize = font_title )\n",
    "boldprt( 'Summary Statistics\\n')\n",
    "print( summary[ 'frequency' ].describe() )\n",
    "print( \"-\"*60 )\n",
    "one_time_buyers = round( sum( summary[ 'frequency' ] == 0)/float( len( summary ) )*( 100 ), 2 )\n",
    "print( f\"Percentage of customers purchase the item once: {one_time_buyers}%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*BG/NBD* model is available as *BetaGeoFitter* class in *lifetimes* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Fit the BG/NBD model\n",
    "##\n",
    "## ===> Step 1: Instantiate the model <===\n",
    "##\n",
    "bgf = lifetimes.BetaGeoFitter( penalizer_coef = 0.0 )\n",
    "##\n",
    "## ===> Step 2: Fit the model <===\n",
    "##\n",
    "bgf01 = bgf.fit( summary[ 'frequency' ], summary[ 'recency' ], summary[ 'T' ] )\n",
    "##\n",
    "## ===> Step 3: Summarize the model <===\n",
    "##\n",
    "display( bgf.summary.style.set_caption( 'BG/NBD Model Summary' ).set_table_styles( tbl_styles ).format( p_value ) )\n",
    "boldprt( 'Base: Model bgf01' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to know whether a customer is alive or not (i.e., predict customer churn) based on the historical data. Use *model.conditional_probability_alive()* in *lifetimes* to compute the probability that a customer with the 3-tuple  history (frequency, recency, T) is currently alive.  You can then plot this using *plot_probabilty_alive_matrix(model)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Compute the customer alive probability\n",
    "##\n",
    "format_dict.update( {'probability_alive':'{0:.3f}'})\n",
    "summary['probability_alive'] = bgf.conditional_probability_alive( summary[ 'frequency' ],\\\n",
    "                                        summary[ 'recency' ], summary[ 'T' ] )\n",
    "display( summary.head(10).style.set_caption( \"Customer 'Alive' Probability\" ).set_table_styles( tbl_styles ).\\\n",
    "        hide_index().format( format_dict ) )\n",
    "boldprt( 'Base: Model bgf01' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set threshold for classifying customers as alive or dead:\n",
    "##   probability_alive > theta, then Alive; else, Churned \n",
    "##\n",
    "theta = 0.75\n",
    "##\n",
    "## Score customers\n",
    "##\n",
    "base = summary.shape[ 0 ]\n",
    "summary[ 'Alive' ] = [ 'Alive' if x > theta else 'Churned' for x in summary.probability_alive ]\n",
    "display( summary.head(10).style.set_caption( \"Customer 'Alive' Status\" ).set_table_styles( tbl_styles ).hide_index().format( format_dict ) )\n",
    "boldprt( f'Base: {base} customers' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The probabilty of being alive is calculated based on the recency and frequency of a customer. So,\n",
    "\n",
    ">1. If a customer has bought multiple times (frequency) and the time between first & last transaction is high (recency), then his/her probability being alive is high.\n",
    ">2. If a customer has less frequency (bought once or twice) and the time between first & last transaction is low (recency), then his/her probability being alive is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine Alive/Churn status\n",
    "##\n",
    "status = summary.Alive.value_counts( normalize = True )\n",
    "tmp = pd.DataFrame( status )\n",
    "tmp.rename( columns = { \"Alive\": \"Status\" }, inplace = True )\n",
    "display( tmp.style.set_caption( \"Alive/Churn Status\" ).set_table_styles( tbl_styles ).format( format ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the CLV Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Use the trained model to predict the likely future transactions of each customer.  Use the *conditional_expected_number_of_purchases_up_to_time* method in *lifetimes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Predict transactions for the next 30 days based on historical data\n",
    "##\n",
    "## Set steps-ahead parameter\n",
    "##\n",
    "t = 30\n",
    "##\n",
    "## Predict\n",
    "##\n",
    "summary[ 'predicted_trans' ] = round( bgf.conditional_expected_number_of_purchases_up_to_time\\\n",
    "                                ( t, summary[ 'frequency' ], summary[ 'recency' ], summary[ 'T' ] ), 2 )\n",
    "##\n",
    "format_dict.update( {'predicted_trans':'{0:.2f}'})\n",
    "summary_sorted = summary.sort_values( by = 'predicted_trans', ascending = False )\n",
    "display( summary_sorted.head().\\\n",
    "       style.set_caption( 'Predicted Future Transactions: 30 Days Ahead' ).set_table_styles( tbl_styles ).\\\n",
    "       hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CLV Monetary Value\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the relationship between frequency and monetary_value\n",
    "##\n",
    "return_customers_summary = summary[ summary[ 'frequency' ] > 0 ]\n",
    "base = 'Base: ' + str( return_customers_summary.shape[ 0 ] ) + ' customers'\n",
    "display( return_customers_summary.head().style.set_caption( 'Predicted Transactions' ).set_table_styles( tbl_styles ).\\\n",
    "        hide_index().format( format_dict ) )\n",
    "boldprt( base )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check correlation between frequency and monetary_value\n",
    "##\n",
    "cols = ['frequency', 'monetary_value']\n",
    "display( return_customers_summary[ cols ].corr().style.set_caption( 'Correlation Between Frequency & Value' ).\\\n",
    "        set_table_styles( tbl_styles ).format( '{:0.2f}' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The correlation are very weak. Hence, the assumption is satisfied and we can fit the model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Model the monetary value using the Gamma-Gamma Model\n",
    "##\n",
    "## ===> Step 1: Instantiate the model <===\n",
    "##\n",
    "ggf = lifetimes.GammaGammaFitter( penalizer_coef = 0.001 )\n",
    "##\n",
    "## ===> Step 2: Fit the model <===\n",
    "##\n",
    "ggf01 = ggf.fit( return_customers_summary[ 'frequency' ], return_customers_summary[ 'monetary_value' ] )\n",
    "##\n",
    "## ===> Step 3: Summarize the model <===\n",
    "##\n",
    "display( ggf.summary.style.set_caption( 'GGF Model Summary' ).set_table_styles( tbl_styles ).format( p_value ) )\n",
    "boldprt( 'Base: Model ggf01' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the expected average profit for each transaction and the *CLV* using the model. Use:\n",
    "\n",
    ">1. *model.conditional_expected_average_profit()*: This method computes the conditional expectation of the average profit per transaction for a group of one or more customers.\n",
    ">2. *model.customer_lifetime_value()*: This method computes the average lifetime value of a group of one or more customers. This method takes the *BG/NBD* model and the prediction horizon as a parameter to calculate the *CLV*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate the conditional expected average profit for each customer per transaction\n",
    "##\n",
    "format_dict.update( {'exp_avg_sales':'${0:,.2f}'})\n",
    "summary = summary[summary['monetary_value'] >0]\n",
    "summary['exp_avg_sales'] = ggf.conditional_expected_average_profit(summary['frequency'],\n",
    "                                       summary['monetary_value'])\n",
    "display( summary.head().style.set_caption( 'Summary Measures' ).set_table_styles( tbl_styles).\\\n",
    "        hide_index().format( format_dict ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The expected average sales is based on the actual sales value, not profit. We can use the model to get *predicted CLV* and then multiply that by a profit margin to get a profit value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Predict CLV for the next 30 days; set discount rate to 1% (0.01)\n",
    "##\n",
    "format_dict.update( {'predicted_clv':'${0:,.2f}'})\n",
    "summary[ 'predicted_clv' ] = ggf.customer_lifetime_value( bgf, summary[ 'frequency' ], summary[ 'recency' ], summary[ 'T' ],\\\n",
    "                                                       summary[ 'monetary_value' ], \n",
    "                                                       time = 1,             # lifetime in months\n",
    "                                                       freq = 'D',           # frequency in which the data is present(T)      \n",
    "                                                       discount_rate = 0.01  # discount rate\n",
    "                                                    )\n",
    "display( summary.head().style.set_caption( 'Summary' ).set_table_styles( tbl_styles).hide_index().format( format_dict ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The predicted *CLV* is sales volume.  Need to calculate net profit using the profit margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate CLV in terms of net profit for each customer (profit margin = 5%)\n",
    "## Net profit for each customer is sales value times profit margin.\n",
    "##\n",
    "profit_margin = 0.05\n",
    "##\n",
    "format_dict.update( {'CLV':'${0:,.2f}'})\n",
    "summary[ 'CLV' ] = summary[ 'predicted_clv' ] * profit_margin\n",
    "display( summary.head().style.set_caption( 'Summary' ).set_table_styles( tbl_styles).\\\n",
    "        hide_index().format( format_dict ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display the distribution of CLV for the next 30 days\n",
    "##\n",
    "display( summary[ 'CLV' ].describe() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>Summary and Wrap-up\n",
    "---------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '61' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>Contact Information\n",
    "---------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n",
    "\n",
    "If you have any questions after this course, please do not hesitate to contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '63' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>Exercise Solutions\n",
    "--------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.1\n",
    "\n",
    "[Return to Exercise II.1](#Exercise-II.1)\n",
    "\n",
    "Import a *CSV* data set of bank customers.  The *CSV* file is named *BankChurners.csv*.  HINT: Use *pd.read_csv*.  Call the imported DataFrame *df_bank*.  \n",
    "\n",
    "| Variable  | Values  | Source  | Mnemonic |\n",
    "|-----------|---------|---------|----------|\n",
    "| Customer ID | Unique identifier | Bank | CID |\n",
    "| Attrition Flag | String: Existing Customer, Attrited Customer | Bank | Attrition_Flag | \n",
    "| Customer Age | Integer | Bank | Age | \n",
    "| Customer Gender | Single Character: F = Female, M = Male | Bank | Gender |\n",
    "| Number of Household Dependents | Interger: 0, 1, 2, ... | Bank | Dependent_count |\n",
    "| Education Level | String | Bank | Education_Level |\n",
    "| Marital Status | String | Bank | Marital Status |\n",
    "| Income Category | String | Bank | Income_Category |\n",
    "| Type of Bank Card | String | Bank | Card_Category |\n",
    "| Months as Customer | Integer | Bank | Months_on_Book |\n",
    "| Total Number of Products Held by Customer | Integer | Bank | Total_Relationship_Count |\n",
    "| No. of Months Inactive in Last 12 Months | Integer | Bank | Months_Inactive_12_mon |\n",
    "| No. of Contacts in Last 12 Months | Interger: 0, 1, 2, ... | Bank | Contacts_Count_12_mon |\n",
    "| Credit Limit on the Credit Card | Integer | Bank | Credit_Limit |\n",
    "| Total Revolving Balance on the Credit Card | Integer | Bank | Total_Revolving_Bal |\n",
    "| Open to Buy Credit Line (Average of last 12 months) | Integer | Bank | Avg_Open_To_Buy |\n",
    "| Change in Transaction Amount (Q4 over Q1) | Float | Bank | Total_Amt_Chng_Q4_Q1 |\n",
    "| Total Transaction Amount (Last 12 months) | Integer | Bank |\n",
    "| Total Transaction Count (Last 12 months) | Integer | Bank | Total_Trans_Amt |\n",
    "| Change in Transaction Count (Q4 over Q1) | Float | Bank | Total_Ct_Chng_Q4_Q1 |\n",
    "| Average Card Utilization Ratio | Float | Bank | Avg_Utilization_Ratio |\n",
    "\n",
    "Use the following demographic variables to create a Hierarchical Cluster of the bank customers:\n",
    "\n",
    ">- Age\n",
    ">- Gender\n",
    ">- Card_Category\n",
    "\n",
    "This problem is a little tricky.  First, I recommend subsetting the three variables into a temporary DataFrame: *tmp*.  Second, *Gender* and *Card_Category* are character strings so they have to be recoded.  You can use a list comprehension to recode both.  Suggestions:\n",
    ">- tmp[ 'Gender' ] = [ 1 if x == 'M' else 0 for x in tmp.Gender ]\n",
    ">- tmp[ 'Card_Category' ] = [ 1 if x == 'Blue' else 2 if x == 'Silver' else 3 if x == 'Gold' else 4 for x in tmp.Card_Category ]\n",
    "\n",
    "Finally, the DataFrame is very large, so I recommend taking a random sample of $n = 500$.  You can use:\n",
    "\n",
    ">- smpl = tmp.sample( n = 500, random_state = 42, replace = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n",
    "file = 'BankChurners.csv'\n",
    "df_bank = pd.read_csv( path + file )\n",
    "df_bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n",
    "cols = [ 'Age', 'Gender', 'Card_Category' ]\n",
    "tmp = df_bank[ cols ].copy()\n",
    "##\n",
    "## Recode data\n",
    "tmp[ 'Gender' ] = [ 1 if x == 'M' else 0 for x in tmp.Gender ]\n",
    "tmp[ 'Card_Category' ] = [ 1 if x == 'Blue' else 2 if x == 'Silver' else 3 if x == 'Gold' else 4 for x in tmp.Card_Category ]\n",
    "##\n",
    "display( tmp.head() )\n",
    "##\n",
    "## Draw a random sample of size n = 500\n",
    "## Put the sample in a new DataFrame.\n",
    "##\n",
    "smpl = tmp.sample( n = 500, random_state = 42, replace = False )\n",
    "##\n",
    "ward = shc.linkage( smpl, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram\n",
    "## WARNING: this will take a few minutes\n",
    "##\n",
    "max_dist = 50\n",
    "##\n",
    "plt.figure( figsize = ( 10, 7  ) )  \n",
    "plt.title( 'CID Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Customer (CID)' )\n",
    "plt.ylabel( 'Distance' )\n",
    "plt.text( 2500, max_dist + 0.5, 'Cut-off Line' )\n",
    "##\n",
    "shc.dendrogram( ward )\n",
    "plt.axhline( y = max_dist, c = 'black', ls = '-', lw = 1.5 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.2\n",
    "\n",
    "[Return to Exercise II.2](#Exercise-II.2)\n",
    "\n",
    "Use the bank DataFrame to do a K-Means clustering.  Use the following numeric variables:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Use a random sample of $n = 500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'Months_on_Book', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit' ]\n",
    "tmp = df_bank[ cols ].copy()\n",
    "display( tmp.head() )\n",
    "##\n",
    "## Do K-Means\n",
    "##\n",
    "kmeans = KMeans( n_clusters = 4, random_state = 42 ).fit( tmp )\n",
    "## \n",
    "## Add cluster labels to main cluster DataFrame\n",
    "##\n",
    "tmp[ 'Cluster_Number' ] = kmeans.labels_   ## Notice the ending underscore\n",
    "##\n",
    "display( tmp.head() )\n",
    "display( tmp.Cluster_Number.value_counts( normalize = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.1\n",
    "\n",
    "[Return to Exercise III.1](#Exercise-III.1)\n",
    "\n",
    "Split the Bank DataFrame into train and testing data sets using $\\frac{3}{4}$ for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use 75% train\n",
    "##\n",
    "bank_train, bank_test = train_test_split( df_bank, train_size = 0.75, random_state = 42 )\n",
    "display( df_size( bank_train ) )\n",
    "display( df_size( bank_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.2\n",
    "\n",
    "[Return to Exercise III.2](#Exercise-III.2)\n",
    "\n",
    "Train a bank attrition (i.e., churn) model.  The dependent variable is *Attrition_Flag*.  Recode this as 1 if the account is closed; else 0.  Use a list comprehension for this.  Use the following numeric variables for the independent variable or *features*:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Call the fitted model *bank_logit01*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the data\n",
    "##\n",
    "cols = [ 'Attrition_Flag', 'Months_on_Book', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n",
    "        'Contacts_Count_12_mon', 'Credit_Limit' ]\n",
    "tmp = bank_train[ cols ].copy()\n",
    "tmp[ 'Attrition_Flag' ] = [ 1 if x == 'Attrited Customer' else 0 for x in tmp.Attrition_Flag ]\n",
    "display( tmp.head() )\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'Attrition_Flag ~ Months_on_Book + Total_Relationship_Count + Months_Inactive_12_mon + \\\n",
    "    Contacts_Count_12_mon + Credit_Limit' \n",
    "##\n",
    "## ===> Step 2: Instantiate the logit model <===\n",
    "##\n",
    "mod = smf.logit( formula, data = tmp )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##\n",
    "bank_logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "display( bank_logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>End Exercise Solutions\n",
    "--------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
