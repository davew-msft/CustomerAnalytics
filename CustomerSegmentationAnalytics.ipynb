{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this is part of a larger Customer Analytics workshop)\n",
    "\n",
    "# Customer Segmentation Analytics\n",
    "\n",
    "Customers are not all the same.  If we can figure out what _classes_ of customers we have, we may be able to better meet their needs. \n",
    "\n",
    "## What is Segmentation\n",
    "\n",
    "* segmentation involves dividing the market (your prospects and customers) into homogenous subgroups\n",
    "* using these segments we can drive other business activities:\n",
    "  * different, targeted marketing campaigns\n",
    "  * unique pricing strategies and price points.  (Understanding basic economics/econometrics can help you here)\n",
    "\n",
    "## How do we do Segmentation\n",
    "\n",
    "3 basic methods:\n",
    "\n",
    "* **_a priori_ segmentation**\n",
    "  * we know the segments ahead of time.  Based on experience and history we know how we want to classify our customers.  \n",
    "  * example:  electric utilities divide their market into residential, commercial, and industry segments\n",
    "* **supervised (machine) learning** -- a model-based approach\n",
    "  * we know what _features_ we want to use as part of the segmentation and the _label_ is the segments\n",
    "  * we have to have _labeled_ historical segmentation data already\n",
    "  * When is this useful?  If we know the segments for existing customers but we want to classify new customers.  \n",
    "* **unsupervised learning** -- clustering \n",
    "  * when we don't know the segments _a priori_ but want the machine to teach us interesting things about our data.  \n",
    "  * this is a great way to facilitate _Design Thinking_ about our customers.  \n",
    "  * Examples\n",
    "    * hierarchical clustering \n",
    "      * agglomerative:  all rows start as their own cluster and the hierarchy is built bottom-up.  More compute-efficient.\n",
    "      * divisive:  all rows start in one big initial cluster (the root) and then we pull apart the differences (top-down approach).  More compute-intensive.    \n",
    "    * k-means clustering\n",
    "\n",
    "Let's do some clustering using a real world use case\n",
    "\n",
    "## Case Study\n",
    "\n",
    "You are the data analyst for a furniture wholesaler.  Your company manufactures and sells to locally-owned retailers throughout the US and you divide your marketing regions consistently with the [US Census regions](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf).  \n",
    "* midwest\n",
    "* northeast\n",
    "* south\n",
    "* west\n",
    "\n",
    "Each region has a salesforce and a VP of Sales and they have autonomy to set discounts and terms.  \n",
    "\n",
    "You have 43 products in 6 product lines, further divided into product classes (they aren't all listed here):\n",
    "\n",
    "|Product Line|Product Class|Product|\n",
    "|---|---|---|\n",
    "|Den|Chairs|Wingback chair|\n",
    "|Den|Tables|Side table|\n",
    "|Den|Sofas|Craftsman sofa|\n",
    "|Dining Room|Chairs|dining room armchair|\n",
    "|Dining Room|Tables|dining room table|\n",
    "|Dining Room|Tables|Baking Racks|\n",
    "|Kids Rooms|...|...|\n",
    "|Kitchen|...|...|\n",
    "|Living Room|...|...|\n",
    "|Bedroom|...|...|\n",
    "\n",
    "4 types of discounts are offered at the discretion of the regional sales teams.\n",
    "\n",
    "**The CMO wants customers segmented specifically for living room blinds so that we can develop a targeted marketing campaign.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup Template\n",
    "\n",
    "These are tools and scripts I always use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done running imports.py\n",
      "done running utils.py\n"
     ]
    }
   ],
   "source": [
    "## set various paths\n",
    "#datapath = '../Data/'\n",
    "datapath = 'https://davewdemodata.blob.core.windows.net/lake/CustomerAnalytics/orders.csv?sv=2020-10-02&st=2022-02-04T18%3A40%3A49Z&se=2030-02-05T18%3A40%3A00Z&sr=b&sp=r&sig=ROOMQEX14ZhvpPnPy9T%2BJ8kcllW3FxS8wADT5LwdcOE%3D'\n",
    "\n",
    "# you might have to run this block first to install the packages\n",
    "# if using the devcontainer, this was already done for you\n",
    "#!pip install -r {scriptspath + 'requirements.txt'}\n",
    "\n",
    "# this will update the requirements.txt file later, if needed\n",
    "#!pip freeze > requirements.txt\n",
    "\n",
    "%run ./scripts/imports.py\n",
    "%run ./scripts/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analytics\n",
    "\n",
    "Now that we've done all that setup above, let's take a look at the data in our datalake using Synapse SQL Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Data Dictionary\n",
    "\n",
    "This is the data we are going to use.  You can use this as a reference.\n",
    "\n",
    "| Variable                  | Values                                 | Source       | Column Name |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | INT                     | Order Sys    | Onum         |\n",
    "| Customer ID               | INT                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                             | Order Sys    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "| Total Discount            | \\% discount                            | Calculated: Sum of discounts | Tdisc         |\n",
    "| Pocket Price              | \\$US                                   | Calculated: LPrice $\\times$ (1 - TDisc) | Pprice  | \n",
    "| Log of Unit Sales         | Log sales                              | Calculated: log(Usales)  | log_Usales  |\n",
    "| Log of Pocket Price       | \\$US                                   | Calculated: log(Pprice)  | log_Pprice  |\n",
    "| Revenue                   | \\$US                                   | Calculated: Usales $\\times$ Pprice | Rev          |\n",
    "| Contribution              | \\$US                                   | Calculated: Rev - Mcost | Con  |\n",
    "| Contribution Margin       | \\%                                     | Calculated: Con/Rev | CM |\n",
    "| Net Revenue               | \\$US                                   | Calculated: (Usales - returnAmount) $\\times$  Pprice  | netRev  |\n",
    "| Lost Revenue         |  \\$US   | Calculated: Rev - netRev  | lostRev  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's just look at the data we were given and try to make some sense out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Onum</th>\n",
       "      <th>CID</th>\n",
       "      <th>Tdate</th>\n",
       "      <th>Pline</th>\n",
       "      <th>...</th>\n",
       "      <th>Ddisc</th>\n",
       "      <th>Cdisc</th>\n",
       "      <th>Odisc</th>\n",
       "      <th>Pdisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>586</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>587</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>588</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>589</td>\n",
       "      <td>1015</td>\n",
       "      <td>2004-01-25</td>\n",
       "      <td>Living Room</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Onum   CID      Tdate        Pline  ...  Ddisc  Cdisc  Odisc  Pdisc\n",
       "0   585  1015 2004-01-25  Living Room  ...    NaN    NaN  0.043  0.042\n",
       "1   586  1015 2004-01-25  Living Room  ...  0.157  0.075  0.041  0.031\n",
       "2   587  1015 2004-01-25  Living Room  ...    NaN  0.048  0.053  0.021\n",
       "3   588  1015 2004-01-25  Living Room  ...    NaN  0.072    NaN  0.033\n",
       "4   589  1015 2004-01-25  Living Room  ...  0.140  0.056  0.041  0.055\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "df_orders = pd.read_csv( datapath, parse_dates = [ 'Tdate' ] )\n",
    "df_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "df_orders = pd.read_csv( datapath, parse_dates = [ 'Tdate' ] )\n",
    "##\n",
    "## Initial Calculations on first DataFrame\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )  ## Total discounts\n",
    "##\n",
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )  ## Pocket prices: what the wholesaler receives as revenue on the item\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice  ## Gross revenue\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice  ## Net revenue\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev  ## Lost revenue due to returns\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost  ## Contribution (i.e., profit)\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev  ## Contribution margin\n",
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = 'customers.csv'\n",
    "df_cust = pd.read_csv( path + file )\n",
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "format_dict = { 'Mcost':'${0:.2f}', 'Lprice':'${0:.2f}', 'Pprice':'${0:.2f}', 'Rev':'${0:,.2f}', 'netRev':'${0:,.2f}',\n",
    "             'lostRev':'${0:,.2f}', 'Con':'${0:,.2f}', 'CM':'{0:,.2%}', 'Ddisc':'{0:,.1%}', 'Cdisc':'{0:,.1%}', 'Odisc':'{0:,.1%}', \n",
    "             'Pdisc':'{0:,.1%}', 'Tdisc':'{0:,.1%}' }\n",
    "display( df.head().style.set_caption( 'Base DataFrame' ).\\\n",
    "    set_table_styles( tbl_styles ).\\\n",
    "    hide_index().format( format_dict ) )\n",
    "print( 'Base: All data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "df_orders = pd.read_csv( datapath, parse_dates = [ 'Tdate' ] )\n",
    "##\n",
    "## Initial Calculations on first DataFrame\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )  ## Total discounts\n",
    "##\n",
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )  ## Pocket prices: what the wholesaler receives as revenue on the item\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice  ## Gross revenue\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice  ## Net revenue\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev  ## Lost revenue due to returns\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost  ## Contribution (i.e., profit)\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev  ## Contribution margin\n",
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = 'customers.csv'\n",
    "df_cust = pd.read_csv( path + file )\n",
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "format_dict = { 'Mcost':'${0:.2f}', 'Lprice':'${0:.2f}', 'Pprice':'${0:.2f}', 'Rev':'${0:,.2f}', 'netRev':'${0:,.2f}',\n",
    "             'lostRev':'${0:,.2f}', 'Con':'${0:,.2f}', 'CM':'{0:,.2%}', 'Ddisc':'{0:,.1%}', 'Cdisc':'{0:,.1%}', 'Odisc':'{0:,.1%}', \n",
    "             'Pdisc':'{0:,.1%}', 'Tdisc':'{0:,.1%}' }\n",
    "display( df.head().style.set_caption( 'Base DataFrame' ).\\\n",
    "    set_table_styles( tbl_styles ).\\\n",
    "    hide_index().format( format_dict ) )\n",
    "print( 'Base: All data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the DataFrame size\n",
    "##\n",
    "df_size( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_check( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvReport( df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, count the number of unique *CID*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## How many unique CIDs are available?\n",
    "##\n",
    "x = df.CID.nunique()\n",
    "boldprt( 'Number of Unique CIDs:' )\n",
    "print( f'{x}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Aggregate panel data to CID level for modeling\n",
    "##\n",
    "## Identify variables for modeling and aggregation\n",
    "##\n",
    "cols = [ 'CID', 'Region', 'Usales', 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "##\n",
    "## Identify variables for grouping\n",
    "##\n",
    "grp = [ 'CID', 'Region' ]\n",
    "##\n",
    "## Specify aggregations\n",
    "##\n",
    "aggregations = { 'Usales':'sum', 'Pprice':'mean', 'Ddisc':'mean', 'Odisc':'mean',\n",
    "                 'Cdisc':'mean', 'Pdisc':'mean'}\n",
    "##\n",
    "## Use groupby with agg function to aggregate\n",
    "##\n",
    "tmp = df[ cols ].copy()\n",
    "df_agg = tmp.groupby( grp ).agg( aggregations )\n",
    "##\n",
    "## Rename columns and reset index\n",
    "##\n",
    "df_agg.rename( columns = { 'Usales':'totalUsales', 'Pprice':'meanPprice', 'Ddisc':'meanDdisc',\n",
    "                      'Odisc':'meanOdisc', 'Cdisc':'meanCdisc',\n",
    "                      'Pdisc':'meanPdisc'}, inplace = True )\n",
    "df_agg = df_agg.reset_index()\n",
    "##\n",
    "## Print head\n",
    "##\n",
    "format_dict = { 'totalUsales':'{0:,.0f}', 'meanPprice':'${0:.2f}', 'meanDdisc':'{0:,.1%}', 'meanCdisc':'{0:,.1%}', 'meanOdisc':'{0:,.1%}', \n",
    "             'meanPdisc':'{0:,.1%}' }\n",
    "display( df_agg.head().style.set_caption( 'Aggregated Data' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the DataFrame size\n",
    "##\n",
    "df_size( df_agg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the df_agg data\n",
    "##\n",
    "cols = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_hclusters = df_agg[ cols ]\n",
    "##\n",
    "display( df_hclusters.head().style.set_caption( 'Subset Data for Clustering' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize the six numeric variables.\n",
    "##\n",
    "cols = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "##\n",
    "## Extract the variables as a temporary DataFrame\n",
    "##\n",
    "tmp = df_hclusters[ cols ]\n",
    "##\n",
    "## Standardize and horizontally concatenate with the Region variable; \n",
    "## concatenate on axis = 1\n",
    "##\n",
    "## Note that the mean is 0 because the mean is exactly in the middle.  z scale\n",
    "##\n",
    "##\n",
    "tmp_standard = pd.DataFrame( StandardScaler().fit_transform( tmp ), columns = cols )\n",
    "df_hclusters = pd.concat( [ df_hclusters[ 'Region'], tmp_standard ], axis = 1 )\n",
    "##\n",
    "display( df_hclusters.head().style.set_caption( 'Standardized Data for Clustering' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )\n",
    "display( df_hclusters.describe().T.style.set_caption( 'Descriptive Statistics' ).\\\n",
    "    set_table_styles( tbl_styles ).format( p_value ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Convert character labels to numerics\n",
    "##\n",
    "x = le.fit_transform( df_hclusters.Region )\n",
    "df_hclusters[ 'Region' ] = x\n",
    "##\n",
    "display( df_hclusters.head().style.set_caption( 'Character Labels as Numerics' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering Approach\n",
    "\n",
    "With hierarchical clustering each object (customer in this case) starts as its own cluster/group (singletons).  Then those clusters are \"joined\" based on how similar (close) they are:\n",
    "\n",
    "* \"closeness\" is defined by a distance metric (default: Euclidean Distance), but there are others.  \n",
    "  * \"Manhattan Distance\" is a particularly interesting one.  \n",
    "* the distance is measured based on how you want to link the clusters.  This is called Linkage\n",
    "  * the center (called the \"centroid\") of the clusters\n",
    "  * the \"average\" linkage distance for the cluster\n",
    "  * \"Ward's minimum variance linkage\" (default)\n",
    "  * maximum, weighted, median linkage, etc.  \n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "* Data Engineering\n",
    "  * rescale your data (we did that above)\n",
    "  * standardization is probably the best place to start\n",
    "* Select the Metric\n",
    "  * Start with Euclidean but consider experimenting\n",
    "* Select the Linkage\n",
    "  * Start with Ward's but experiment\n",
    "* Cluster\n",
    "  * do the clustering, analyze, and experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE 73 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use Ward's Minimum Variance Linkage Method with a Euclidean distance measure.\n",
    "##\n",
    "ward = shc.linkage( df_hclusters, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram (fancy word for upside-down tree)\n",
    "## WARNING: this will take a minute\n",
    "\n",
    "## this would be something to experiment with. In the interest of time \n",
    "## I've done this for you, but feel free to experiment, which is what you \n",
    "## would do in the real world.  \n",
    "max_dist = 23\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 10, 7  ) )  \n",
    "plt.title( 'CID Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Customer (CID)' )\n",
    "plt.ylabel( 'Distance' )\n",
    "plt.text( 2500, 23.5, 'Cut-off Line' )\n",
    "##\n",
    "shc.dendrogram( ward )\n",
    "plt.axhline( y = max_dist, c = 'black', ls = '-', lw = 1.5 );\n",
    "##\n",
    "## Document dendrogram\n",
    "##\n",
    "rec_lst = [ [ (20, 14 ), 500, 4 ], [ (800, 17 ), 1200, 3 ], [ (3600, 17 ), 1350, 4 ],\n",
    "          [ (5900, 18 ), 1600, 4 ]  ]\n",
    "txt_lst = [ 100, 1100, 4000, 6300 ]\n",
    "for i in range( len( rec_lst ) ):\n",
    "    x = rec_lst[ i ]\n",
    "    rectangle = plt.Rectangle( x[ 0 ], x[ 1 ], x[ 2 ], fill = None, ec = \"black\", lw = 3 )\n",
    "    plt.gca().add_patch(rectangle)\n",
    "    plt.text( txt_lst[ i ], 23.5, 'Cluster ' + str( i + 1 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this showing me?  \n",
    "\n",
    "A horizontal line is drawn at a distance of 23 (totally arbitrary...but it gives me 4 clusters...which feels about right to me).  Any cluster formed below this line is a group.  Also notice that there are four groups.    \n",
    "\n",
    ">>What is the right number of clusters?  Dunno.  It's _Prunes Analysis_.  But maybe 4-6 is a good place to start if you don't have _a priori_ knowledge.  \n",
    "\n",
    "Here we use _fcluster_ to _flatten the cluster_ so we get the number of clusters we want.  Now we can look at the individual customers in the clusters and do some _Design Thinking_ with our marketing folks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Identify the CIDs in each cluster\n",
    "## Consider any cluster grouping formed below 23\n",
    "##\n",
    "cluster_labels = fcluster( ward, max_dist, criterion = 'distance' )\n",
    "df_hclusters[ 'Cluster_Number' ] = cluster_labels\n",
    "\n",
    "display( df_hclusters.head().style.set_caption( 'DataFrame with Cluster Assignment Number' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_hclusters[ 'Cluster' ] = [ 'Cluster ' + str( x ) for x in df_hclusters.Cluster_Number ]\n",
    "display( df_hclusters.stb.freq( [ 'Cluster' ] ).style.set_caption( 'Cluster Distribution' ).\\\n",
    "    set_table_styles( tbl_styles ).\\\n",
    "    bar( subset = [ 'count' ], align='mid', color = 'red').hide_index().\\\n",
    "        format( {'percent':'{0:.4}%', 'cumulative_percent':'{0:.4}%'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a boxplot for each cluster for Order Discount\n",
    "##\n",
    "ax = sns.boxplot( x = 'Cluster_Number', y = 'meanOdisc', data = df_hclusters )\n",
    "ax.set_title( 'Order Discount\\nby Clusters\\nHierarchical Clustering', fontsize = font_title )\n",
    "ax.set( xlabel = 'Clusters', ylabel = 'Order Discount' )\n",
    "base = 'Base: All data; n = ' + str( df_hclusters.shape[ 0 ] )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>Exercises</center></h1></strong>\n",
    "    \n",
    "[Back to Contents](#Contents)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II.1\n",
    "this might be good for a second example...\n",
    "\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Import a *CSV* data set of bank customers.  The *CSV* file is named *BankChurners.csv*.  HINT: Use *pd.read_csv*.  Call the imported DataFrame *df_bank*.   \n",
    "\n",
    "| Variable  | Values  | Source  | Mnemonic |\n",
    "|-----------|---------|---------|----------|\n",
    "| Customer ID | Unique identifier | Bank | CID |\n",
    "| Attrition Flag | String: Existing Customer, Attrited Customer | Bank | Attrition_Flag | \n",
    "| Customer Age | Integer | Bank | Age | \n",
    "| Customer Gender | Single Character: F = Female, M = Male | Bank | Gender |\n",
    "| Number of Household Dependents | Interger: 0, 1, 2, ... | Bank | Dependent_count |\n",
    "| Education Level | String | Bank | Education_Level |\n",
    "| Marital Status | String | Bank | Marital Status |\n",
    "| Income Category | String | Bank | Income_Category |\n",
    "| Type of Bank Card | String | Bank | Card_Category |\n",
    "| Months as Customer | Integer | Bank | Months_on_Book |\n",
    "| Total Number of Products Held by Customer | Integer | Bank | Total_Relationship_Count |\n",
    "| No. of Months Inactive in Last 12 Months | Integer | Bank | Months_Inactive_12_mon |\n",
    "| No. of Contacts in Last 12 Months | Interger: 0, 1, 2, ... | Bank | Contacts_Count_12_mon |\n",
    "| Credit Limit on the Credit Card | Integer | Bank | Credit_Limit |\n",
    "| Total Revolving Balance on the Credit Card | Integer | Bank | Total_Revolving_Bal |\n",
    "| Open to Buy Credit Line (Average of last 12 months) | Integer | Bank | Avg_Open_To_Buy |\n",
    "| Change in Transaction Amount (Q4 over Q1) | Float | Bank | Total_Amt_Chng_Q4_Q1 |\n",
    "| Total Transaction Amount (Last 12 months) | Integer | Bank |\n",
    "| Total Transaction Count (Last 12 months) | Integer | Bank | Total_Trans_Amt |\n",
    "| Change in Transaction Count (Q4 over Q1) | Float | Bank | Total_Ct_Chng_Q4_Q1 |\n",
    "| Average Card Utilization Ratio | Float | Bank | Avg_Utilization_Ratio |\n",
    "\n",
    "Use the following demographic variables to create a Hierarchical Cluster of the bank customers:\n",
    "\n",
    ">- Age\n",
    ">- Gender\n",
    ">- Card_Category\n",
    "\n",
    "This problem is a little tricky.  First, I recommend subsetting the three variables into a temporary DataFrame: *tmp*.  Second, *Gender* and *Card_Category* are character strings so they have to be recoded.  You can use a list comprehension to recode both.  Suggestions:\n",
    ">- tmp[ 'Gender' ] = [ 1 if x == 'M' else 0 for x in tmp.Gender ]\n",
    ">- tmp[ 'Card_Category' ] = [ 1 if x == 'Blue' else 2 if x == 'Silver' else 3 if x == 'Gold' else 4 for x in tmp.Card_Category ]\n",
    "\n",
    "Finally, the DataFrame is very large, so I recommend taking a random sample of $n = 500$.  You can use:\n",
    "\n",
    ">- smpl = tmp.sample( n = 500, random_state = 42, replace = False )\n",
    "\n",
    "[See Solution](#Solution-II.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering Approach\n",
    "\n",
    "\n",
    "K-Means is another popular clustering method.  With K-means you have to declare the number of clusters, known as _k_.  `k=4` means you want 4 clusters based on your numeric features:  Sales, Pocket Price, and the four Discounts.\n",
    "\n",
    "Process:\n",
    "\n",
    "* declare k (the seed, or initial, clusters)\n",
    "* group objects based on their shortest distance from the seeds\n",
    "* create new seeds as the _mean_ (or _centroid_) of the groups.  \n",
    "* merge an object (customer) into each group based on the shortest distance to the centroid\n",
    "* repeat until all customers are assigned to the _k_ groups.  \n",
    "\n",
    ">>When would you use this?  When you think you know the number of groups you want _a priori_.  But again, you can always experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set up data \n",
    "##\n",
    "cols = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_kclusters = df_agg[ cols ].copy()\n",
    "##\n",
    "display( df_kclusters.head().style.set_caption( 'K-Means Clustering Data' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the data for all numerics.  We remove the region, it doesn't appear to be needed.  \n",
    "##\n",
    "cols = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "tmp = df_kclusters[ cols ]\n",
    "##\n",
    "## Do K-Means\n",
    "## why 42?  We need something to make it repeatable.  \n",
    "\n",
    "kmeans = KMeans( n_clusters = 4, random_state = 42 ).fit( tmp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The kmeans centers are retrieved using the method *cluster_centers_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Add cluster labels to main cluster DataFrame\n",
    "##\n",
    "df_kclusters[ 'Cluster_Number' ] = kmeans.labels_   ## Notice the ending underscore\n",
    "##\n",
    "display( df_kclusters.head().style.set_caption( 'DataFrame with K-Means Cluster Assignments' ).\\\n",
    "    set_table_styles( tbl_styles ).hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The kmeans cluster numbers are retrieved using the method *labels_*.  Notice the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_kclusters[ 'Cluster' ] = [ 'Cluster ' + str( x ) for x in df_kclusters.Cluster_Number ]\n",
    "display( df_kclusters.stb.freq( [ 'Cluster' ] ).style.set_caption( 'Cluster Distribution' ).\\\n",
    "    set_table_styles( tbl_styles ).\\\n",
    "    bar( subset = [ 'count' ], align='mid', color = 'red').hide_index().format( {'percent':'{0:.4}%', 'cumulative_percent':'{0:.4}%'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a boxplot for each cluster for Order Discount\n",
    "##\n",
    "ax = sns.boxplot( x = 'Cluster_Number', y = 'meanOdisc', data = df_kclusters )\n",
    "ax.set_title( 'Order Discount\\nby Clusters\\nK-Means Clustering', fontsize = font_title )\n",
    "ax.set( xlabel = 'Clusters', ylabel = 'Order Discount' )\n",
    "base = 'Base: All data; n = ' + str( df_hclusters.shape[ 0 ] )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now do some _Design Thinking_ among our analysts and sales teams.  Are these the _right_ clusters for what we want to accomplish?  This is worth a discussion and exploration.   More on this next...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Use the bank DataFrame to do a K-Means clustering.  Use the following numeric variables:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Use a random sample of $n = 500$.\n",
    "\n",
    "[See Solution](#Solution-II.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with these clusters?\n",
    "\n",
    "_Design Thinking_ with your marketing people.  \n",
    "\n",
    "* _What do we do next_ is a key tenant of `Prescriptive Analytics` and this information is not something easily conveyed on a Power BI dashboard (imo).  \n",
    "* Do some _profiling_.  What are these clusters telling us?  Maybe look at some sample customers from a \"Customer 360\" perspective.  What are their common traits?  \n",
    "* Now you can name the clusters/profiles.  Without a good, descriptive, meaningful, generic name that we can agree on, your clusters are meaningless. \n",
    "\n",
    "There is an art to naming.  Imagine you are a clothing retailer.  Some clusters might be: \n",
    "   * Fashion Aware\n",
    "   * Balanced Buyer\n",
    "   * Indulgent Expressives\n",
    "   * Fashionista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Analytics\n",
    "\n",
    "_Churn_ is the loss of customers, their revenue, and higher CAC (customer acquisition costs) to replace them.  \n",
    "\n",
    "Why do customers churn?\n",
    "\n",
    "* price is too high\n",
    "* poor product quality\n",
    "* out-of-fashion products\n",
    "* poor customer support\n",
    "* poor online reviews\n",
    "* etc etc etc\n",
    "\n",
    "What we want to do is reduce churn by identifying/predicting potential churners and taking proactice steps to prevent or minimize churn.  \n",
    "\n",
    "_There may be times when we WANT a customer to churn_  \n",
    "\n",
    "Some churn is inevitable.  But we should be able to minimize it.  \n",
    "\n",
    "Most companies do basic _descriptive analytics_ on churn:\n",
    "* what is the rate-of-churn\n",
    "* track lost revenue\n",
    "* identify and track CAC\n",
    "\n",
    "Some companies do _predictive analytics_:\n",
    "* who are the most likely to churn?\n",
    "\n",
    "Very few companies do _prescriptive analytics_:\n",
    "* What do we do to minimize churn?\n",
    "* How can we measure if our interventions are working?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Case Study - Telecom\n",
    "\n",
    "> I worked for `Comcast Cellular One` out of college (1997-ish) doing churn analytics.  I wasn't good at it and only lasted a few months.  The reason was simple...I could do the math but I couldn't do the _Prescriptive Analytics_.  This is very conversational between business domain experts and data professionals.  \n",
    "\n",
    "The telecom industry was a monopoly until 1984 with the divestiture of AT&T.  Never before had customers \"churned\" from one telecom provider to another.  Suddenly, churn analytics was all the rage.  \n",
    "\n",
    "Let's assume you are a telecom company with a historical database of customer metrics including:\n",
    "* their telecom \"usage\"\n",
    "* subscription services (internet, bundled products, online backup, security products)\n",
    "* month-by-month subscriber vs annual contract\n",
    "* an indicator as to whether the customer churned\n",
    "\n",
    "The CEO was looking at a Power BI dashboard and noticed that 1 out of 4 customers leaves after some period of time.  (_Descriptive Analytics_).  This is a big problem when the numbers need to be reported to Wall Street.  The CAC in this industry is also VERY high.  \n",
    "\n",
    "ASK:  Build a model to predict likely churns (_Predictive Analytics_) so that we can work with marketing and the domain experts to determine what some good strategies would be to _minimize_ churn.  (_Prescriptive Analytics_ or _what do we do next?_)\n",
    "\n",
    "### General Process\n",
    "\n",
    "* We need a data set of past customers, with their histories, and a _label_ as to whether they churned.  **This isn't easy and will likely take some time**.  \n",
    "* Once we talk through the _features_ that are needed we'll have a trained _model_ that will tell us the probability that a customer will churn.  \n",
    "* We want to run that against _current_ customers to identify the potential churners so we can determine how to prevent it. (the _treatment_)\n",
    "* BONUS:  Really advanced companies will constantly \"score\" customers whenever something about those customers changes, in real-time.  This allows those companies to make proactive _treatments_ vs being reactive.  This takes time and requires a lot of Prescriptive Analysis.  \n",
    "\n",
    "\n",
    "But first..._why_ are they churning?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Data Dictionary\n",
    "\n",
    "\n",
    "| Variable | Values | Source | Column Name |\n",
    "|----------|--------|--------|----------|\n",
    "| Customer ID | AlphaNumeric | IT | CID |\n",
    "| Subscriber Gender | String: Male/Female | IT | gender |\n",
    "| Senior Citizen Status | String: Yes/No | IT | seniorCitizen |\n",
    "| Subscriber has Partner | String: Yes/No | IT | partner |\n",
    "| Dependents in Household | String: Yes/No | IT | dependents |\n",
    "| Tenure as Customer | Numeric: Months | IT | tenure |\n",
    "| Have Phone Service | String: Yes/No | IT | phoneService |\n",
    "| Have Multiple Phone Lines | String: Yes/No/No phone service | IT | multipleLines |\n",
    "| Subscribe to Internet Service | String: No/Fiber Opic/DSL | IT | internetService |\n",
    "| Subscribe to  Online Security | String: Yes/No/No Internet Service | IT | onlineSecurity |\n",
    "| Subscribe to  Online Computer Backup | String: Yes/No/No Internet Service | IT | onlineBackup |\n",
    "| Subscribe to  Device Protection Plan | String: Yes/No/No Internet Service | IT | deviceProtection |\n",
    "| Subscribe to Tech Support | String: Yes/No/No Internet Service | IT | techSupport |\n",
    "| Subscribe to Streaming TV | String: Yes/No/No Internet Service | IT | streamingTV |\n",
    "| Subscribe to Streaming Movies | String: Yes/No/No Internet Service | IT | streamingMovies |\n",
    "| Contract Type | String: Month-to-Month/One year/Two year | IT |contractType |\n",
    "| Subscribe to Paperless Bill | String: Yes/No | IT | paperlessBilling |\n",
    "| Payment Method | String: Bank transfer (automatic)/Credit card (automatic)/Electronic check/Mailed check | Billing | paymentMethod |\n",
    "| Average Monthly Charges | Numeric: Dollars and Cents | Billing | monthlyCharges |\n",
    "| Total Charges Over Tenure Time | Numeric: Dollars and Cents | Billing | totalCharges |\n",
    "| Churned at any Time | String: Yes/No | Marketing | churn |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, Examine, and Process the Churn Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set file name\n",
    "##\n",
    "file = 'churn.xlsx'\n",
    "df_churn = pd.read_excel( path + file )\n",
    "##\n",
    "## Record churn from Yes/No to 1/0\n",
    "##\n",
    "df_churn[ 'Churn' ] = [ 1 if x == 'Yes' else 0 for x in df_churn.churn ]\n",
    "##\n",
    "## Display\n",
    "##\n",
    "display( df_churn.head().style.set_caption( 'Churn Data' ).set_table_styles( tbl_styles ).hide_index().\\\n",
    "       format( {'monthlyCharges':'${0:.2f}', 'totalCharges':'${0:,.2f}'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size( df_churn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_check( df_churn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvReport( df_churn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "There are 11 cases with missing values for Total Charges.  There are several options to handle these cases:\n",
    "\n",
    ">1. Drop the 11 cases.\n",
    ">2. Replace with the mean Total Charges.\n",
    ">3. Replace with the mean Total Charges of groups (e.g., Male, Senior Citizens, No Multiple Lines).\n",
    ">4. Interpolate.\n",
    ">5. Estimate an *OLS* model for Total Charges and predict the missing values.\n",
    "\n",
    "See <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#filling-missing-values-fillna\" target=\"_parent\">here</a> for methods in Pandas for handling missing values.\n",
    "\n",
    "I will delete the 11 cases for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop records with NA values.\n",
    "##\n",
    "df_churn.dropna( axis = 0, inplace = True )\n",
    "df_size( df_churn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset data\n",
    "##\n",
    "data = df_churn[ 'churn' ].value_counts( normalize = True ).round( 3 )\n",
    "data = pd.DataFrame( data )\n",
    "base = 'Base: n = ' + str( df_churn.shape[ 0 ] )\n",
    "display( data.style.set_caption( 'Churn Distribution' ).\\\n",
    "    bar( align = 'mid', color = 'red' ).format( '{:.1%}' ).\\\n",
    "    set_table_styles( tbl_styles ) )\n",
    "print( base )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot tenure histogram\n",
    "##\n",
    "base = 'Base: All records; n = ' + str( df_churn.shape[ 0 ] )\n",
    "ax = sns.distplot( df_churn.tenure )\n",
    "ax.set_title( 'Tenure Distribution', fontsize = font_title )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The distribution is bimodal.  Recommendation: recode.\n",
    "\n",
    "ie, we have a lot of new customers and a lot of old customers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot monthly charges histogram\n",
    "##\n",
    "ax = sns.distplot( df_churn.monthlyCharges )\n",
    "ax.set_title( 'Monthly Charges Distribution', fontsize = font_title )\n",
    "footer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The distribution is trimodal.  Recommendation: recode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Recode the two variables: tenure and monthly charges\n",
    "##\n",
    "df_churn[ 'tenureRecoded' ] = [ '<30' if x < 30 else '>=30' for x in df_churn.tenure ]\n",
    "df_churn[ 'monthlyChargesRecoded' ] = [ '<30' if x < 30 else '>= 70' if x >= 70 \n",
    "                                       else '30 - 70' for x in df_churn.monthlyCharges ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split the Churn Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use 75% train\n",
    "##\n",
    "churn_train, churn_test = train_test_split( df_churn, train_size = 0.75, random_state = 42 )\n",
    "display( df_size( churn_train ) )\n",
    "display( df_size( churn_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>Exercises</center></h1></strong>\n",
    "    \n",
    "[Back to Contents](#Contents)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Split the Bank DataFrame into train and testing data sets using $\\frac{3}{4}$ for training.\n",
    "\n",
    "[See Solution](#Solution-III.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>End Exercises</center></h1></strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Churn Prediction Model\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Select columns for training\n",
    "##\n",
    "cols = [ 'Churn', 'gender', 'tenureRecoded', 'contractType', 'monthlyChargesRecoded' ]\n",
    "display( churn_train[ cols ].head().style.set_caption( 'Subsetted Training Data' ).set_table_styles( tbl_styles ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we code a Churn Prediction Model\n",
    "\n",
    "This is merely ONE way to do it.  We use _logistic regression_ by defining a formula for our churn.  \n",
    "\n",
    "Here's the formula.  This notation is often called a `patsy model`:\n",
    "\n",
    "```\n",
    "Churn ~ C( gender, Sum ) + C( tenureRecoded, Sum ) + C( contractType, Sum ) + C( monthlyChargesRecoded, Sum )\n",
    "```\n",
    "\n",
    "Huh?  \n",
    "\n",
    "Let's decode it:\n",
    "\n",
    "* `Churn ~` :  think of it as \"churn equals...\"\n",
    "* `C( gender, Sum )` :  gender has 2 possible values:  `Male` and `Female`.  Algorithms don't like to work on text and we know that gender is `categorical`.  `C` means `convert to categorical`.  `Sum` is a complicated way in econometrics to say, \"every coefficient must sum to zero\",.  In this case it just means `-1` is `female` and `1` is `male`.  We are _categorically-encoding a string value_\n",
    "* do the same thing for `tenureRecoded`, `contratType`, and `monthlyChargesRecoded`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'Churn ~ C( gender, Sum ) + C( tenureRecoded, Sum ) + C( contractType, Sum ) + C( monthlyChargesRecoded, Sum )'\n",
    "\n",
    "\n",
    "##  \"instantiate\" a model\n",
    "##  This is logistic regression\n",
    "mod = smf.logit( formula, data = churn_train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "display( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "* We are trying to maximize the `Log-Likelihood`.  \n",
    "* In the process it generates a \"pseudo\" `R-squ`.  This is too much to explain here but `0.2314` simply can be interpreted as _23% of the variation in my target is accounted for in my model_.  That's not great, but it's a start.  \n",
    "* `C(gender,Sum)[S.Female]` simply means that the coefficient for females is `0.0197`.  Since we used `Sum` we know that for males it is `-0.0197`.  \n",
    "* the same can be said for `tenureRecoded` (below or above 30 months)\n",
    "* `ContractType` has 3 categories but we only see two in the output.  To calculate the \"two year\" contract we simply add the two categories displayed and multiply by -1.  (So, `-1.26`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>Exercises</center></h1></strong>\n",
    "    \n",
    "[Back to Contents](#Contents)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Train a bank attrition (i.e., churn) model.  The dependent variable is *Attrition_Flag*.  Recode this as 1 if the account is closed; else 0.  Use a list comprehension for this.  Use the following numeric variables for the independent variable or *features*:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Call the fitted model *bank_logit01*. \n",
    "\n",
    "[See Solution](#Solution-III.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <center><h1><strong><font color = black>End Exercises</center></h1></strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 154:00\n",
    "\n",
    "Now, what can I do with all of that information that the model is giving me?  \n",
    "\n",
    "## Odds Analysis\n",
    "\n",
    "_Warning...this is a little math-y...skip to the next section if you are ok just trusting me on the math:_\n",
    "\n",
    "\n",
    "You can calculate the odds of churning by _exponentiating the estimated coefficients_ given to us above.\n",
    "\n",
    "The odds of something happening are calculated as \n",
    "\n",
    "$\\frac{p}{1 - p}$  \n",
    "\n",
    "So, the odds for the binary case of Gender:   \n",
    "\n",
    "$Females = e^{\\beta_1}$   \n",
    "\n",
    "after some algebra and recognizing that the $\\beta_0$ term cancels in the numerator and denominator.  The odds for $Males = e^{-\\beta_1}$.  Therefore, the odds ratio for Females to Males is $\\frac{e^{\\beta_1}}{e^{-\\beta_1}} = e^{2 \\times \\beta_1}$.  Conversely, the odds ratio for Males to Females is $\\frac{e^{-\\beta_1}}{e^{\\beta_1}} = e^{-2 \\times \\beta_1}$, or just the inverse.\n",
    "\n",
    "For the trinary case, the odds are calculated the same way, but recognize that the odds for the base are given by $e^{-(\\beta_1 + \\beta_2)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the odds and odds ratio for Females-Males\n",
    "\n",
    "\n",
    "boldprt( 'Gender Odds Ratios\\n' )\n",
    "## exp = exponentiation\n",
    "odds_female = math.exp( logit01.params[ 1 ] )\n",
    "print( f'Female Odds: {odds_female:.3f}' )\n",
    "##\n",
    "odds_male = math.exp( -logit01.params[ 1 ] )\n",
    "print( f'Male Odds: {odds_male:.3f}' )\n",
    "##\n",
    "odds_ratio = odds_female/odds_male\n",
    "boldprt( '='*40 )\n",
    "print( f'Odds Ratio Females to Males: {odds_ratio:.3f}' )\n",
    "print( f'Odds Ratio Males to Females: {1/odds_ratio:.3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The odds are about even for males and females.  They are about equally likely to churn so gender is **not** a factor in churn.  \n",
    "\n",
    "See?  It's pretty simple.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the odds and odds ratio for contract\n",
    "##    Contract is #3 and #4 in the parameter list\n",
    "\n",
    "boldprt( 'Contract Odds Ratios\\n' )\n",
    "odds_month = math.exp( logit01.params[ 3 ] )\n",
    "print( f'Monthly Odds: {odds_month:.3f}' )\n",
    "\n",
    "odds_1Yr = math.exp( logit01.params[ 4 ] )\n",
    "print( f'1 Yr Odds: {odds_1Yr:.3f}' )\n",
    "\n",
    "odds_2Yr = math.exp( -( logit01.params[ 3 ] + logit01.params[ 4 ] ) )\n",
    "print( f'2 Yr Odds: {odds_2Yr:.3f}' )\n",
    "\n",
    "odds_ratio_Month_1Yr = odds_month/odds_1Yr\n",
    "odds_ratio_Month_2Yr = odds_month/odds_2Yr\n",
    "odds_ratio_1Yr_2Yr = odds_1Yr/odds_2Yr\n",
    "print( '='*40 )\n",
    "print( f'Odds Ratio Month to 1 Yr: {odds_ratio_Month_1Yr:.3f}' )\n",
    "print( f'Odds Ratio Month to 2 Yr: {odds_ratio_Month_2Yr:.3f}' )\n",
    "print( f'Odds Ratio 1 Yr to 2 Yr: {odds_ratio_1Yr_2Yr:.3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "`Odds Ratio Month to 1 Yr: 4.312` :  this means that the month-to-month subscribers are **4x** more likely to churn than the 1-Year-Contract Subscribers.  \n",
    "\n",
    "The odds or \"likelihood\" of someone churning if they were a month-to-month subscriber is almost **14x higher** than if they were on a two-year contract.  The two-year contract subscribers are more loyal and committed. This makes sense. \n",
    "\n",
    "**What do we do next?**\n",
    "\n",
    "* If we assume CAC is high and churn minimization is desired then we might want to develop retention programs that **target the conversion of month-to-month users to longer term contracts.**\n",
    "* it is less urgent to develop programs to retain longer-term contract customers.  \n",
    "* we should likely develop programs to target customers near the end of their contract term.  \n",
    "\n",
    "_This is all just starting to scratch the surface of what we can do._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics Outcome\n",
    "\n",
    "This PROCESS is a great way to confirm hypotheses that our marketing teams and execs may _think_ is the cause for churn.  VERY INTERESTING conversations and _Design Thinking_ sessions occur during this time.  \n",
    "\n",
    "Let's quickly generate all of the odds ratios:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a DataFrame of estimates and drop first row (the intercept).\n",
    "## we print this out solely to see what is happening\n",
    "\n",
    "df_odds = pd.DataFrame( logit01.params, columns = [ 'Estimates' ] ).reset_index().iloc[1: , :]\n",
    "df_odds[ 'Group' ] = df_odds[ 'index' ]\n",
    "df_odds.Group = df_odds.Group.apply( lambda st: st[ st.find( \"C(\" ) + 2:st.find( \",\" ) ] )\n",
    "aggregation = {'Estimates':'sum'}\n",
    "grp = df_odds.groupby( by = [ 'Group' ] ).agg( aggregation )*( -1 )\n",
    "print(grp)\n",
    "display( df_odds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's print out the dataframe again in a manner that's a little easier to understand\n",
    "\n",
    "df_odds = df_odds.merge( grp, left_on = 'Group', right_on = 'Group' )\n",
    "df_odds.rename( columns = { \"Estimates_y\": \"Base\" }, inplace = True )\n",
    "df_odds[ 'Odds_Ratio' ] = np.exp(df_odds.Estimates_x)/np.exp(df_odds.Base)\n",
    "##\n",
    "display( df_odds.style.set_caption( 'Odds Ratio Data -- Relative to Base' ).set_table_styles( tbl_styles ).\\\n",
    "        format( { 'Odds_Ratio':'{:,.1f}'} ).hide_index() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "* `Base` is the base value of the group vs its comparison (the `Index`)\n",
    "  * the `Base` for contractType is `2 year` vs the `Index` which is either `Month-to-month` or `One year`\n",
    "  * Example from above:  `Versus the Base of Two Year contract, a month-to-month contract is 13.8x more likely to churn`\n",
    "* this can be a little confusing at first.  There's probably a better way to display this data but I'm not sure what it is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot odds ratios\n",
    "##\n",
    "ax = df_odds.plot( x = 'index', y = 'Odds_Ratio',  kind = 'barh' )\n",
    "ax.set_title( 'Odds Ratios', fontsize = font_title )\n",
    "ax.set_xlabel( 'Odds Ratio' )\n",
    "ax.set_ylabel( '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Churn with the Model\n",
    "\n",
    "At some point we'll have a trained model that is predicting churn that we can understand.  We now want to do `inferencing` against live data (or in this case we want to do `validation` of the model using the hold-out dataset from above: `churn-test`).  \n",
    "\n",
    "The prediction process is simple: use the `predict` function instead of `fit`.  \n",
    "\n",
    "Note:  the model will give us _probability_ of churn, so we specify a cut-off threshold: $\\theta$ (theta).  A probability > $\\theta$ is coded as `1` or `True` or `Churn`; `0` or `False` or `Not Churn`, otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Make predictions\n",
    "## Use churn_test for this example\n",
    "\n",
    "theta = 0.5\n",
    "\n",
    "prediction = logit01.predict( churn_test )\n",
    "\n",
    "## let's make this easier to read\n",
    "\n",
    "classification = [ 1 if x > theta else 0 for x in prediction ]\n",
    "data = { 'probability':prediction, 'classification':classification }\n",
    "tmp = pd.DataFrame( data )\n",
    "tmp[ 'Churn' ] = [ 'Churn' if x == 1 else 'Not Churn' for x in tmp.classification ]\n",
    "display( tmp.head().style.set_caption( 'Predictions from Logit Model' ).set_table_styles( tbl_styles ).format( {'probability':p_value } ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "* the first col is the `Customer ID`\n",
    "* the `probability` is coming out of the model and is sometimes useful when you are exploring whethere someone is _close_ to churning \n",
    "* We might want to change `theta` and explore what happens\n",
    "\n",
    "### What-If Analysis\n",
    "\n",
    "Predict churn for different settings of the variables.  This is similar to `what-if` analysis in Excel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Specify scenario values to use for prediction\n",
    "## we can do a lot of experimentation by adjusting the variables\n",
    "## Let's just look at this case:  Men with new 1 year contracts and at least $70 of monthly spend\n",
    "\n",
    "data = {\n",
    "         'gender': [ 'Male' ],\n",
    "         'tenureRecoded': [ '<30' ],\n",
    "         'contractType': [ 'One year' ],\n",
    "         'monthlyChargesRecoded': [ '>= 70' ]\n",
    "        }\n",
    "\n",
    "## Create a DataFrame using the dictionary\n",
    "churn_scenario = pd.DataFrame.from_dict( data )\n",
    "\n",
    "## Display the settings and the predicted unit sales\n",
    "display( churn_scenario.style.set_caption( 'Scenario Settings' ).set_table_styles( tbl_styles ).hide_index() )\n",
    "##\n",
    "## Create a prediction\n",
    "##\n",
    "theta = 0.5\n",
    "##\n",
    "prediction = logit01.predict( churn_scenario )\n",
    "classification = [ 'Churn' if x > theta else 'Not Churn' for x in prediction ]\n",
    "data = { 'probability':prediction, 'classification':classification }\n",
    "tmp = pd.DataFrame( data )\n",
    "display( tmp.style.set_caption( 'Scenario Prediction' ).set_table_styles( tbl_styles ).hide_index().format( {'probability':p_value } ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Men with **new** 1 year contracts and at least $70 of monthly spend likely will NOT churn.  \n",
    "\n",
    "Let's look at this in more detail:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make predictions\n",
    "\n",
    "theta = 0.5\n",
    "\n",
    "y_test = churn_test[ 'Churn' ]\n",
    "prediction = logit01.predict( churn_test )\n",
    "classification = [ 1 if x > theta else 0 for x in prediction ]\n",
    "x = classification_report( y_test, classification, digits = 3 )\n",
    "boldprt( f'Logit Model Classification Report:\\n{x}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "* The `accuracy` is 78%.  This means the model accurately predicted churn 78% of the time.  \n",
    "\n",
    "\n",
    "Some definitions:  \n",
    "\n",
    "* `precision` is the ratio $tp/(tp + fp)$ where $tp$ is the number of true positives and $fp$ the number of false positives. _The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative._\n",
    "* `recall` is the ratio $tp/(tp + fn)$ where $tp$ is the number of true positives and $fn$ the number of false negatives. _The recall is intuitively the ability of the classifier to find all the positive samples._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*.\n",
    "\n",
    "Sometimes it is **CRITICAL** that we optimize for ONE of these metrics.  \n",
    "\n",
    "Here's an example:  If I am trying to diagnose cancer it _might_ be ok to have some **false positives** but it is absolutely UNACCEPTABLE if our model allows **false negatives** (ie, the patient had cancer and we didn't detect it).  \n",
    "\n",
    "Sometimes it's interesting to have conversations around _why_ we see certain results in our models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "x = confusion_matrix(y_test, classification).ravel()\n",
    "##\n",
    "lbl = [ 'True Negative', 'False Positive', 'False Negative', 'True Positive' ]\n",
    "##\n",
    "## Display the confusion matrix in a DataFrame\n",
    "##\n",
    "df_confusion = pd.DataFrame( x, columns = [ 'Value' ], index = lbl )\n",
    "df_confusion[ 'Percent (%)' ] = df_confusion.Value/df_confusion.Value.sum()\n",
    "##\n",
    "display( df_confusion.style.format( { 'Percent (%)': '{:.1%}' } ).highlight_max( color = 'yellow' ).\\\n",
    "    set_caption( 'Confusion Matrix Summary' ).set_table_styles( tbl_styles ).format( {'Value':'{0:,.0f}' } ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There was 1149 true negatives, 151 false positives, 238 false negatives, and 220 true positives.  The total is 1758 which is the size of the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the confusion values\n",
    "##\n",
    "ax = sns.barplot( y = df_confusion.index, x = df_confusion[ 'Percent (%)' ] )\n",
    "ax.set_title( 'Percent of Sample\\nby Confusion Labels', fontsize = font_title )\n",
    "ax.set( xlabel = 'Percent Confusion', ylabel = '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative plot of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['Not Likley to Churn', 'Likely to Churn']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, classification )\n",
    "df_cm = pd.DataFrame( data = cm/cm.sum(), index = lbl, columns = lbl )\n",
    "##\n",
    "display( df_cm.style.format( { 'Not Likley to Churn': '{:.1%}', 'Likely to Churn': '{:.1%}' } ).highlight_max( color = 'yellow' ).\\\n",
    "    set_caption( 'Churn Confusion Matrix' ).set_table_styles( tbl_styles ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "78% (= 65.9% + 11.9% $\\approx$ 78% ) of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a Churn Database\n",
    "\n",
    "You can score the entire database with the model results.  If you are satisfied with the model (i.e., passes statistical checks and test analysis), then apply the model to the entire database.\n",
    "\n",
    "We can also think about `real-time inferencing` so we can apply a treatment proactively vs reactively.  \n",
    "\n",
    "\n",
    "### What do we do next?  \n",
    "\n",
    "We don't just want a scored database of possible churners.  If we know what the `win back cost` is and we can build a formula for the `expected revenue loss` then we can quickly determine which churners we need to focus on RIGHT NOW.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the model to the database\n",
    "score = logit01.predict( df_churn )\n",
    "score = pd.DataFrame( logit01.predict( df_churn ), columns = [ 'Score'] )\n",
    "df_scored = pd.concat( [ df_churn, score ], axis = 1 ) ##, ignore_index = True )\n",
    "\n",
    "# this isn't a great way to calculated Expected Revenue Loss, but it's a good starting point\n",
    "# and leads to a great discussion\n",
    "# expectedLoss = total charges to date\n",
    "df_scored[ 'expectedLoss' ] = df_scored.totalCharges * df_scored.Score\n",
    "\n",
    "# Specify win-back cost\n",
    "# again, this is hard-coded but is a great conversation starter.  Perhaps CAC for this customer segment should be used here?\n",
    "win_back_cost = 350\n",
    "\n",
    "## Determine who to target: expected loss > win back cost\n",
    "\n",
    "df_scored[ 'Target' ] = [ 'Yes' if x > win_back_cost else 'No' for x in df_scored.expectedLoss ]\n",
    "df_scored.sort_values( by = [ 'expectedLoss', 'Target' ], ascending = False, inplace = True )\n",
    "cols = [ 'CID', 'expectedLoss', 'Target' ]\n",
    "display( df_scored[ cols ].head().style.set_caption( 'Win-Back Target' ).set_table_styles( tbl_styles ).\\\n",
    "       format( {'expectedLoss':'${0:,.2f}'} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Distribution of Target variable\n",
    "##\n",
    "x = df_scored.Target.value_counts( normalize = True )\n",
    "pd.DataFrame( x ).style.set_caption( 'Distribution of Target' ).set_table_styles( tbl_styles ).format( {'Target':'{0:.1%}' } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "These are the customers where, if they churn, we lose more revenue than what it would cost to win them back or acquire a similar, new customer.  **About 40% of my customers should be targeted given the scenario above**.  \n",
    "\n",
    "\n",
    "## Here's what I hope you learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson IV: Introduction to Customer Lifetime Value\n",
    "\n",
    "\n",
    "This is based on https://github.com/hariharan2305/DailyKnowledge/blob/master/Customer%20Lifetime%20Value/Customer%20Lifetime%20Value%20(CLV).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '47' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '49' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '50' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '51' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '53' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '54' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '55' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '56' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLV Case Study\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '58' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLV Data Dictionary\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "**Retail Sales Data**\n",
    "\n",
    "| Variable | Values | Source | Mnemonic |\n",
    "|----------|--------|--------|----------|\n",
    "| Invoice Number | Nominal, a 6-digit integral number | UCI | InvoiceNo |\n",
    "| Product (item) code | Nominal, a 5-digit integral number | UCI | StockCode |\n",
    "| Product (item) name | String | UCI | Description |\n",
    "| Quantities of each product (item) per transaction | Numeric | UCI | Quantity |\n",
    "| Invoice Date and time | Numeric, day and time | UCI | InvoiceDate |\n",
    "| Unit price | Numeric, price per unit in sterling | UCI | UnitPrice |\n",
    "| Customer ID | Nominal, 5-digit integral number | UCI | CID |\n",
    "| Country name | Nominal, name of the country of customer | UCI | Country |\n",
    "\n",
    "Source: University of California Irvine Machine Learning Repository (*UCI*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, Examine, and Process the CLV Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the data\n",
    "##\n",
    "file = 'onlineRetail.csv'\n",
    "##\n",
    "format_dict = {'UnitPrice':'${0:.2f}', 'CID':'{0:.0f}' }\n",
    "df_retail = pd.read_csv( path + file, parse_dates = [ 'InvoiceDate' ] )\n",
    "df_retail.rename( columns = { 'CustomerID':'CID' }, inplace = True )\n",
    "display( df_retail.head().style.set_caption( 'Initial Data Download' ).set_table_styles( tbl_styles ).hide_index().\\\n",
    "       format( format_dict ) )\n",
    "display( df_size( df_retail ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Get country distribution\n",
    "##\n",
    "df_country = pd.DataFrame( df_retail.Country.value_counts( normalize = True ) )\n",
    "df_country.rename( columns = {'Country':'Proportion'}, inplace = True ) \n",
    "df_country.head().style.set_caption( 'Country Representation' ).set_table_styles( tbl_styles ).\\\n",
    "    format( { 'Proportion':'{:,.1%}'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset UK\n",
    "##\n",
    "df_uk = df_retail.query( \"Country == 'United Kingdom'\" ).drop( labels = [ 'Country' ], axis = 1 )\n",
    "display( df_uk.head().style.set_caption( 'Preprocessed Data' ).set_table_styles( tbl_styles ).\n",
    "        hide_index().format( format_dict ) )\n",
    "display( df_size( df_uk ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display summary statistics\n",
    "##\n",
    "display( df_uk.describe().T.style.set_caption( 'Summary Statistics' ).set_table_styles( tbl_styles ).format( p_value ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice the negative quantity and price points.  These represent returns and are not positive business.  I'll delete all records with negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop the records with negative data\n",
    "##\n",
    "df_uk = df_uk[ ( df_uk.Quantity > 0 ) & ( df_uk.UnitPrice > 0 ) ]\n",
    "display( df_uk.describe().T.style.set_caption( 'Summary Statistics' ).set_table_styles( tbl_styles ).format( p_value ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Check for missing values\n",
    "##\n",
    "mvReport( df_uk )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop all records with missing CIDs.  We need to know who the customers are so missing values are a problem.\n",
    "##\n",
    "df_uk.dropna( inplace = True )\n",
    "mvReport( df_uk )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate total revenue\n",
    "##\n",
    "format_dict.update( {'Revenue':'${0:.2f}'})\n",
    "df_uk[ 'Revenue' ] = df_uk[ 'Quantity' ] * df_uk[ 'UnitPrice' ]\n",
    "display( df_uk.head().style.set_caption( 'Retail Data' ).set_table_styles( tbl_styles ).\\\n",
    "        hide_index().format( format_dict ) )\n",
    "display( df_uk.describe().T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Print some DataFrame details\n",
    "##\n",
    "format_dict = {'total revenue':'${0:,.0f}', 'total quantity':'{0:,.0f}', 'unique customers':'{0:,.0f}'}\n",
    "data = { 'min Date':df_uk.InvoiceDate.dt.date.min(), 'max Date':df_uk.InvoiceDate.dt.date.max(),\n",
    "        'unique customers':df_uk.CID.nunique(), 'total quantity':df_uk.Quantity.sum(),\n",
    "        'total revenue':df_uk.Revenue.sum() }\n",
    "display( pd.DataFrame( data, index = [0] ).style.set_caption( 'Variable Summaries' ).\\\n",
    "        set_table_styles( tbl_styles ).hide_index().format( format_dict ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice that there are 3,920 unique customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CLV\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *lifetimes* package.\n",
    "\n",
    "Create a *Recency, Frequency and Monetary Value* (*RFM*) summary table from the transactions data.\n",
    "\n",
    "The *summary_data_from_transactions_data* function in *lifetimes* package aggregates transaction level data and calculates for each customer:\n",
    "\n",
    ">- **frequency**: the number of repeat purchases (more than 1 purchases).\n",
    ">- **recency**: the time between the first and the last transaction.\n",
    ">- **T**: the time between the first purchase and the end of the transaction period.\n",
    ">- **monetary_value**: it is the mean of a given customer's sales value (i.e., Revenue).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create summary data using summary_data_from_transaction_data function.\n",
    "##\n",
    "format_dict = { 'CID':'{0:.0f}', 'T':'{0:.0f}', 'frequency':'{0:.0f}', 'recency':'{0:.0f}', 'monetary_value':'${0:,.2f}'}\n",
    "summary = lifetimes.utils.summary_data_from_transaction_data( df_uk, 'CID', 'InvoiceDate', 'Revenue' )\n",
    "summary = summary.reset_index()\n",
    "base = summary.shape[ 0 ]\n",
    "display( summary.head().style.set_caption( 'RFM Summary Data' ).set_table_styles( tbl_styles ).format( format_dict ).hide_index() )\n",
    "boldprt( f'Base: {base} customers' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice that the monetary value is based on the calculated Revenue (unit sales $\\times$ price).  This is not net profit.\n",
    "\n",
    "The value 0 for frequency and recency means these are one-time buyers. Let's check how many such one-time buyers there are in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot distribution of frequency\n",
    "##\n",
    "ax = summary[ 'frequency' ].plot( kind = 'hist', bins = 50 )\n",
    "ax.set_title( 'Frequency Distribution', fontsize = font_title )\n",
    "boldprt( 'Summary Statistics\\n')\n",
    "print( summary[ 'frequency' ].describe() )\n",
    "print( \"-\"*60 )\n",
    "one_time_buyers = round( sum( summary[ 'frequency' ] == 0)/float( len( summary ) )*( 100 ), 2 )\n",
    "print( f\"Percentage of customers purchase the item once: {one_time_buyers}%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*BG/NBD* model is available as *BetaGeoFitter* class in *lifetimes* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Fit the BG/NBD model\n",
    "##\n",
    "## ===> Step 1: Instantiate the model <===\n",
    "##\n",
    "bgf = lifetimes.BetaGeoFitter( penalizer_coef = 0.0 )\n",
    "##\n",
    "## ===> Step 2: Fit the model <===\n",
    "##\n",
    "bgf01 = bgf.fit( summary[ 'frequency' ], summary[ 'recency' ], summary[ 'T' ] )\n",
    "##\n",
    "## ===> Step 3: Summarize the model <===\n",
    "##\n",
    "display( bgf.summary.style.set_caption( 'BG/NBD Model Summary' ).set_table_styles( tbl_styles ).format( p_value ) )\n",
    "boldprt( 'Base: Model bgf01' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to know whether a customer is alive or not (i.e., predict customer churn) based on the historical data. Use *model.conditional_probability_alive()* in *lifetimes* to compute the probability that a customer with the 3-tuple  history (frequency, recency, T) is currently alive.  You can then plot this using *plot_probabilty_alive_matrix(model)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Compute the customer alive probability\n",
    "##\n",
    "format_dict.update( {'probability_alive':'{0:.3f}'})\n",
    "summary['probability_alive'] = bgf.conditional_probability_alive( summary[ 'frequency' ],\\\n",
    "                                        summary[ 'recency' ], summary[ 'T' ] )\n",
    "display( summary.head(10).style.set_caption( \"Customer 'Alive' Probability\" ).set_table_styles( tbl_styles ).\\\n",
    "        hide_index().format( format_dict ) )\n",
    "boldprt( 'Base: Model bgf01' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set threshold for classifying customers as alive or dead:\n",
    "##   probability_alive > theta, then Alive; else, Churned \n",
    "##\n",
    "theta = 0.75\n",
    "##\n",
    "## Score customers\n",
    "##\n",
    "base = summary.shape[ 0 ]\n",
    "summary[ 'Alive' ] = [ 'Alive' if x > theta else 'Churned' for x in summary.probability_alive ]\n",
    "display( summary.head(10).style.set_caption( \"Customer 'Alive' Status\" ).set_table_styles( tbl_styles ).hide_index().format( format_dict ) )\n",
    "boldprt( f'Base: {base} customers' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The probabilty of being alive is calculated based on the recency and frequency of a customer. So,\n",
    "\n",
    ">1. If a customer has bought multiple times (frequency) and the time between first & last transaction is high (recency), then his/her probability being alive is high.\n",
    ">2. If a customer has less frequency (bought once or twice) and the time between first & last transaction is low (recency), then his/her probability being alive is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine Alive/Churn status\n",
    "##\n",
    "status = summary.Alive.value_counts( normalize = True )\n",
    "tmp = pd.DataFrame( status )\n",
    "tmp.rename( columns = { \"Alive\": \"Status\" }, inplace = True )\n",
    "display( tmp.style.set_caption( \"Alive/Churn Status\" ).set_table_styles( tbl_styles ).format( format ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the CLV Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Use the trained model to predict the likely future transactions of each customer.  Use the *conditional_expected_number_of_purchases_up_to_time* method in *lifetimes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Predict transactions for the next 30 days based on historical data\n",
    "##\n",
    "## Set steps-ahead parameter\n",
    "##\n",
    "t = 30\n",
    "##\n",
    "## Predict\n",
    "##\n",
    "summary[ 'predicted_trans' ] = round( bgf.conditional_expected_number_of_purchases_up_to_time\\\n",
    "                                ( t, summary[ 'frequency' ], summary[ 'recency' ], summary[ 'T' ] ), 2 )\n",
    "##\n",
    "format_dict.update( {'predicted_trans':'{0:.2f}'})\n",
    "summary_sorted = summary.sort_values( by = 'predicted_trans', ascending = False )\n",
    "display( summary_sorted.head().\\\n",
    "       style.set_caption( 'Predicted Future Transactions: 30 Days Ahead' ).set_table_styles( tbl_styles ).\\\n",
    "       hide_index().format( format_dict ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CLV Monetary Value\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the relationship between frequency and monetary_value\n",
    "##\n",
    "return_customers_summary = summary[ summary[ 'frequency' ] > 0 ]\n",
    "base = 'Base: ' + str( return_customers_summary.shape[ 0 ] ) + ' customers'\n",
    "display( return_customers_summary.head().style.set_caption( 'Predicted Transactions' ).set_table_styles( tbl_styles ).\\\n",
    "        hide_index().format( format_dict ) )\n",
    "boldprt( base )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check correlation between frequency and monetary_value\n",
    "##\n",
    "cols = ['frequency', 'monetary_value']\n",
    "display( return_customers_summary[ cols ].corr().style.set_caption( 'Correlation Between Frequency & Value' ).\\\n",
    "        set_table_styles( tbl_styles ).format( '{:0.2f}' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The correlation are very weak. Hence, the assumption is satisfied and we can fit the model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Model the monetary value using the Gamma-Gamma Model\n",
    "##\n",
    "## ===> Step 1: Instantiate the model <===\n",
    "##\n",
    "ggf = lifetimes.GammaGammaFitter( penalizer_coef = 0.001 )\n",
    "##\n",
    "## ===> Step 2: Fit the model <===\n",
    "##\n",
    "ggf01 = ggf.fit( return_customers_summary[ 'frequency' ], return_customers_summary[ 'monetary_value' ] )\n",
    "##\n",
    "## ===> Step 3: Summarize the model <===\n",
    "##\n",
    "display( ggf.summary.style.set_caption( 'GGF Model Summary' ).set_table_styles( tbl_styles ).format( p_value ) )\n",
    "boldprt( 'Base: Model ggf01' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the expected average profit for each transaction and the *CLV* using the model. Use:\n",
    "\n",
    ">1. *model.conditional_expected_average_profit()*: This method computes the conditional expectation of the average profit per transaction for a group of one or more customers.\n",
    ">2. *model.customer_lifetime_value()*: This method computes the average lifetime value of a group of one or more customers. This method takes the *BG/NBD* model and the prediction horizon as a parameter to calculate the *CLV*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate the conditional expected average profit for each customer per transaction\n",
    "##\n",
    "format_dict.update( {'exp_avg_sales':'${0:,.2f}'})\n",
    "summary = summary[summary['monetary_value'] >0]\n",
    "summary['exp_avg_sales'] = ggf.conditional_expected_average_profit(summary['frequency'],\n",
    "                                       summary['monetary_value'])\n",
    "display( summary.head().style.set_caption( 'Summary Measures' ).set_table_styles( tbl_styles).\\\n",
    "        hide_index().format( format_dict ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The expected average sales is based on the actual sales value, not profit. We can use the model to get *predicted CLV* and then multiply that by a profit margin to get a profit value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Predict CLV for the next 30 days; set discount rate to 1% (0.01)\n",
    "##\n",
    "format_dict.update( {'predicted_clv':'${0:,.2f}'})\n",
    "summary[ 'predicted_clv' ] = ggf.customer_lifetime_value( bgf, summary[ 'frequency' ], summary[ 'recency' ], summary[ 'T' ],\\\n",
    "                                                       summary[ 'monetary_value' ], \n",
    "                                                       time = 1,             # lifetime in months\n",
    "                                                       freq = 'D',           # frequency in which the data is present(T)      \n",
    "                                                       discount_rate = 0.01  # discount rate\n",
    "                                                    )\n",
    "display( summary.head().style.set_caption( 'Summary' ).set_table_styles( tbl_styles).hide_index().format( format_dict ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The predicted *CLV* is sales volume.  Need to calculate net profit using the profit margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate CLV in terms of net profit for each customer (profit margin = 5%)\n",
    "## Net profit for each customer is sales value times profit margin.\n",
    "##\n",
    "profit_margin = 0.05\n",
    "##\n",
    "format_dict.update( {'CLV':'${0:,.2f}'})\n",
    "summary[ 'CLV' ] = summary[ 'predicted_clv' ] * profit_margin\n",
    "display( summary.head().style.set_caption( 'Summary' ).set_table_styles( tbl_styles).\\\n",
    "        hide_index().format( format_dict ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display the distribution of CLV for the next 30 days\n",
    "##\n",
    "display( summary[ 'CLV' ].describe() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>Summary and Wrap-up\n",
    "---------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '61' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>Contact Information\n",
    "---------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n",
    "\n",
    "If you have any questions after this course, please do not hesitate to contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '63' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>Exercise Solutions\n",
    "--------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.1\n",
    "\n",
    "[Return to Exercise II.1](#Exercise-II.1)\n",
    "\n",
    "Import a *CSV* data set of bank customers.  The *CSV* file is named *BankChurners.csv*.  HINT: Use *pd.read_csv*.  Call the imported DataFrame *df_bank*.  \n",
    "\n",
    "| Variable  | Values  | Source  | Mnemonic |\n",
    "|-----------|---------|---------|----------|\n",
    "| Customer ID | Unique identifier | Bank | CID |\n",
    "| Attrition Flag | String: Existing Customer, Attrited Customer | Bank | Attrition_Flag | \n",
    "| Customer Age | Integer | Bank | Age | \n",
    "| Customer Gender | Single Character: F = Female, M = Male | Bank | Gender |\n",
    "| Number of Household Dependents | Interger: 0, 1, 2, ... | Bank | Dependent_count |\n",
    "| Education Level | String | Bank | Education_Level |\n",
    "| Marital Status | String | Bank | Marital Status |\n",
    "| Income Category | String | Bank | Income_Category |\n",
    "| Type of Bank Card | String | Bank | Card_Category |\n",
    "| Months as Customer | Integer | Bank | Months_on_Book |\n",
    "| Total Number of Products Held by Customer | Integer | Bank | Total_Relationship_Count |\n",
    "| No. of Months Inactive in Last 12 Months | Integer | Bank | Months_Inactive_12_mon |\n",
    "| No. of Contacts in Last 12 Months | Interger: 0, 1, 2, ... | Bank | Contacts_Count_12_mon |\n",
    "| Credit Limit on the Credit Card | Integer | Bank | Credit_Limit |\n",
    "| Total Revolving Balance on the Credit Card | Integer | Bank | Total_Revolving_Bal |\n",
    "| Open to Buy Credit Line (Average of last 12 months) | Integer | Bank | Avg_Open_To_Buy |\n",
    "| Change in Transaction Amount (Q4 over Q1) | Float | Bank | Total_Amt_Chng_Q4_Q1 |\n",
    "| Total Transaction Amount (Last 12 months) | Integer | Bank |\n",
    "| Total Transaction Count (Last 12 months) | Integer | Bank | Total_Trans_Amt |\n",
    "| Change in Transaction Count (Q4 over Q1) | Float | Bank | Total_Ct_Chng_Q4_Q1 |\n",
    "| Average Card Utilization Ratio | Float | Bank | Avg_Utilization_Ratio |\n",
    "\n",
    "Use the following demographic variables to create a Hierarchical Cluster of the bank customers:\n",
    "\n",
    ">- Age\n",
    ">- Gender\n",
    ">- Card_Category\n",
    "\n",
    "This problem is a little tricky.  First, I recommend subsetting the three variables into a temporary DataFrame: *tmp*.  Second, *Gender* and *Card_Category* are character strings so they have to be recoded.  You can use a list comprehension to recode both.  Suggestions:\n",
    ">- tmp[ 'Gender' ] = [ 1 if x == 'M' else 0 for x in tmp.Gender ]\n",
    ">- tmp[ 'Card_Category' ] = [ 1 if x == 'Blue' else 2 if x == 'Silver' else 3 if x == 'Gold' else 4 for x in tmp.Card_Category ]\n",
    "\n",
    "Finally, the DataFrame is very large, so I recommend taking a random sample of $n = 500$.  You can use:\n",
    "\n",
    ">- smpl = tmp.sample( n = 500, random_state = 42, replace = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n",
    "file = 'BankChurners.csv'\n",
    "df_bank = pd.read_csv( path + file )\n",
    "df_bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n",
    "cols = [ 'Age', 'Gender', 'Card_Category' ]\n",
    "tmp = df_bank[ cols ].copy()\n",
    "##\n",
    "## Recode data\n",
    "tmp[ 'Gender' ] = [ 1 if x == 'M' else 0 for x in tmp.Gender ]\n",
    "tmp[ 'Card_Category' ] = [ 1 if x == 'Blue' else 2 if x == 'Silver' else 3 if x == 'Gold' else 4 for x in tmp.Card_Category ]\n",
    "##\n",
    "display( tmp.head() )\n",
    "##\n",
    "## Draw a random sample of size n = 500\n",
    "## Put the sample in a new DataFrame.\n",
    "##\n",
    "smpl = tmp.sample( n = 500, random_state = 42, replace = False )\n",
    "##\n",
    "ward = shc.linkage( smpl, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram\n",
    "## WARNING: this will take a few minutes\n",
    "##\n",
    "max_dist = 50\n",
    "##\n",
    "plt.figure( figsize = ( 10, 7  ) )  \n",
    "plt.title( 'CID Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Customer (CID)' )\n",
    "plt.ylabel( 'Distance' )\n",
    "plt.text( 2500, max_dist + 0.5, 'Cut-off Line' )\n",
    "##\n",
    "shc.dendrogram( ward )\n",
    "plt.axhline( y = max_dist, c = 'black', ls = '-', lw = 1.5 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.2\n",
    "\n",
    "[Return to Exercise II.2](#Exercise-II.2)\n",
    "\n",
    "Use the bank DataFrame to do a K-Means clustering.  Use the following numeric variables:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Use a random sample of $n = 500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'Months_on_Book', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit' ]\n",
    "tmp = df_bank[ cols ].copy()\n",
    "display( tmp.head() )\n",
    "##\n",
    "## Do K-Means\n",
    "##\n",
    "kmeans = KMeans( n_clusters = 4, random_state = 42 ).fit( tmp )\n",
    "## \n",
    "## Add cluster labels to main cluster DataFrame\n",
    "##\n",
    "tmp[ 'Cluster_Number' ] = kmeans.labels_   ## Notice the ending underscore\n",
    "##\n",
    "display( tmp.head() )\n",
    "display( tmp.Cluster_Number.value_counts( normalize = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.1\n",
    "\n",
    "[Return to Exercise III.1](#Exercise-III.1)\n",
    "\n",
    "Split the Bank DataFrame into train and testing data sets using $\\frac{3}{4}$ for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use 75% train\n",
    "##\n",
    "bank_train, bank_test = train_test_split( df_bank, train_size = 0.75, random_state = 42 )\n",
    "display( df_size( bank_train ) )\n",
    "display( df_size( bank_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.2\n",
    "\n",
    "[Return to Exercise III.2](#Exercise-III.2)\n",
    "\n",
    "Train a bank attrition (i.e., churn) model.  The dependent variable is *Attrition_Flag*.  Recode this as 1 if the account is closed; else 0.  Use a list comprehension for this.  Use the following numeric variables for the independent variable or *features*:\n",
    "\n",
    ">- Months_on_Book\n",
    ">- Total_Relationship_Count\n",
    ">- Months_Inactive_12_mon\n",
    ">- Contacts_Count_12_mon\n",
    ">- Credit_Limit\n",
    "\n",
    "Call the fitted model *bank_logit01*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the data\n",
    "##\n",
    "cols = [ 'Attrition_Flag', 'Months_on_Book', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n",
    "        'Contacts_Count_12_mon', 'Credit_Limit' ]\n",
    "tmp = bank_train[ cols ].copy()\n",
    "tmp[ 'Attrition_Flag' ] = [ 1 if x == 'Attrited Customer' else 0 for x in tmp.Attrition_Flag ]\n",
    "display( tmp.head() )\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'Attrition_Flag ~ Months_on_Book + Total_Relationship_Count + Months_Inactive_12_mon + \\\n",
    "    Contacts_Count_12_mon + Credit_Limit' \n",
    "##\n",
    "## ===> Step 2: Instantiate the logit model <===\n",
    "##\n",
    "mod = smf.logit( formula, data = tmp )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##\n",
    "bank_logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "display( bank_logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<font color = black>End Exercise Solutions\n",
    "--------------------------------------\n",
    "    \n",
    "[Back to Contents](#Contents)  \n",
    "    \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
